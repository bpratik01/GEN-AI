{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b8c8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = os.getenv('GROQ_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d315c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need', 'title': 'Attention Is All You Need - Wikipedia', 'language': 'en'}, page_content='\\n\\n\\n\\nAttention Is All You Need - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDonate\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\nDonate Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1\\nAuthors\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\nMethods discussed and introduced\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\nHistorical context\\n\\n\\n\\n\\nToggle Historical context subsection\\n\\n\\n\\n\\n\\n3.1\\nPredecessors\\n\\n\\n\\n\\n\\n\\n\\n\\n3.2\\nAttention with seq2seq\\n\\n\\n\\n\\n\\n\\n\\n\\n3.3\\nParallelizing attention\\n\\n\\n\\n\\n\\n\\n\\n\\n3.4\\nAI boom era\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\nTraining\\n\\n\\n\\n\\n\\n\\n\\n\\n5\\nNotes\\n\\n\\n\\n\\n\\n\\n\\n\\n6\\nReferences\\n\\n\\n\\n\\n\\n\\n\\n\\n7\\nExternal links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nAttention Is All You Need\\n\\n\\n\\n13 languages\\n\\n\\n\\n\\nالعربيةCatalàEspañolفارسی한국어עבריתမြန်မာဘာသာ日本語PortuguêsРусскийTürkçeУкраїнська中文\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\nWikidata item\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\n2017 research paper by Google\\n\\n\\nAn illustration of main components of the transformer model from the paper\\n\"Attention Is All You Need\"[1] is a 2017 landmark[2][3] research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al.[4] It is considered a foundational[5] paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models.[6][7] At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as multimodal generative AI.[1]\\nThe paper\\'s title is a reference to the song \"All You Need Is Love\" by the Beatles.[8] The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper\\'s authors, liked the sound of that word.[9]\\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers franchise. The team was named Team Transformer.[8]\\nSome early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.[9]\\nAs of 2025,[update] the paper has been cited more than 173,000 times,[10] placing it among top ten most-cited papers of the 21st century.[11]\\n\\n\\nAuthors[edit]\\nThe authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones\\xa0[wikidata], Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" to the paper; the listed order was randomized. The Wired article highlights the group\\'s diversity:[8]\\nSix of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.\\n\\nAfter the paper, each of the authors left Google to join other companies or to found startups. Several of them expressed feelings of being unable to innovate and expand the Transformer in a direction they want, if they had stayed at Google.[12]\\nMethods discussed and introduced[edit]\\nThe paper is most well known for the introduction of the Transformer architecture, which forms the underlying architecture for most forms of modern Large Language Models (LLMs). A key reason for why the architecture is preferred by most modern LLMs is the parallelizability of the architecture over its predecessors. This ensures that the operations necessary for training can be accelerated on a GPU allowing both faster training times and models of bigger sizes to be trained.\\nThe following mechanisms were introduced by the paper as part of the development of the transformer architecture.\\nScaled dot-product attention & self-attention\\nThe use of the scaled dot-product attention and self-attention mechanism instead of a Recurrent neural network or Long short-term memory (which rely on recurrence instead) allow for better performance as described in the following paragraph. The paper described the scaled dot-product attention as follows:\\n\\n\\n\\n\\n\\n\\nA\\nt\\nt\\ne\\nn\\nt\\ni\\no\\nn\\n\\n\\n(\\nQ\\n,\\nK\\n,\\nV\\n)\\n:=\\n\\n\\ns\\no\\nf\\nt\\nm\\na\\nx\\n\\n\\n\\n(\\n\\n\\n\\nQ\\n×\\n\\nK\\n\\nT\\n\\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n\\n)\\n\\n×\\nV\\n\\n\\n{\\\\displaystyle {\\\\rm {Attention}}(Q,K,V):={\\\\rm {softmax}}\\\\left({\\\\frac {Q\\\\times K^{T}}{\\\\sqrt {d_{k}}}}\\\\right)\\\\times V}\\n\\n\\nwhere \\n\\n\\n\\nQ\\n\\n\\n{\\\\displaystyle Q}\\n\\n, \\n\\n\\n\\nK\\n\\n\\n{\\\\displaystyle K}\\n\\n, \\n\\n\\n\\nV\\n\\n\\n{\\\\displaystyle V}\\n\\n are respectively the query, key, value matrices, and  \\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n{\\\\displaystyle d_{k}}\\n\\n is the dimension of the values.\\nSince the model relies on Query (Q), Key (K) and Value (V) matrices that come from the same source itself (i.e. the input sequence / context window), this eliminates the need for RNNs completely ensuring parallelizability for the architecture. This differs from the original form of the Attention mechanism introduced in 2014. Additionally, the paper also discusses the use of an additional scaling factor that was found to be most effective with respect to the dimension of the key vectors (represented as \\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n{\\\\displaystyle d_{k}}\\n\\n and initially set to 64 within the paper) in the manner shown above.\\nIn the specific context of translation which the paper focused on, the Query and Key matrices are usually represented in embeddings corresponding to the source language while the Value matrix corresponds to the target language.\\nMulti-head attention\\nIn the self-attention mechanism, queries (Q), keys (K), and values (V) are dynamically generated for each input sequence (limited typically by the size of the context window), allowing the model to focus on different parts of the input sequence at different steps. Multi-head attention enhances this process by introducing multiple parallel attention heads. Each attention head learns different linear projections of the Q, K, and V matrices. This allows the model to capture different aspects of the relationships between words in the sequence simultaneously, rather than focusing on a single aspect.\\nBy doing this, multi-head attention ensures that the input embeddings are updated from a more varied and diverse set of perspectives. After the attention outputs from all heads are calculated, they are concatenated and passed through a final linear transformation to generate the output.\\nPositional encoding\\nSince the Transformer model is not a seq2seq model and does not rely on the sequence of the text in order to perform encoding and decoding, the paper relied on the use of sine and cosine wave functions to encode the position of the token into the embedding. The methods introduced in the paper are discussed below:\\n\\n\\n\\n\\nP\\n\\nE\\n\\n(\\n\\n\\np\\no\\ns\\n\\n\\n,\\n2\\ni\\n)\\n\\n\\n=\\nsin\\n\\u2061\\n(\\n\\n\\np\\no\\ns\\n\\n\\n\\n/\\n\\n\\n\\n10000\\n\\n\\n2\\ni\\n\\n/\\n\\n\\nd\\n\\n\\nm\\no\\nd\\ne\\nl\\n\\n\\n\\n\\n\\n)\\n\\n\\n{\\\\displaystyle PE_{({\\\\rm {pos}},2i)}=\\\\sin({\\\\rm {pos}}/{10000}^{2i/d_{\\\\rm {model}}})}\\n\\n\\n\\n\\n\\n\\nP\\n\\nE\\n\\n(\\n\\n\\np\\no\\ns\\n\\n\\n,\\n2\\ni\\n+\\n1\\n)\\n\\n\\n=\\ncos\\n\\u2061\\n(\\n\\n\\np\\no\\ns\\n\\n\\n\\n/\\n\\n\\n\\n10000\\n\\n\\n2\\ni\\n\\n/\\n\\n\\nd\\n\\n\\nm\\no\\nd\\ne\\nl\\n\\n\\n\\n\\n\\n)\\n\\n\\n{\\\\displaystyle PE_{({\\\\rm {pos}},2i+1)}=\\\\cos({\\\\rm {pos}}/{10000}^{2i/d_{\\\\rm {model}}})}\\n\\n\\nwherein \\n\\n\\n\\n\\n\\np\\no\\ns\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\rm {pos}}}\\n\\n, \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n, \\n\\n\\n\\n\\n\\nd\\n\\n\\nm\\no\\nd\\ne\\nl\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle {d_{\\\\rm {model}}}}\\n\\n correspond to the position of the word, the current dimension index and the dimension of the model respectively. The sine function is used for even indices of the embedding while the cosine function is used for odd indices. The resultant \\n\\n\\n\\nP\\nE\\n\\n\\n{\\\\displaystyle PE}\\n\\n embedding is then added to the word at that corresponding position with respect to the current context window. The paper specifically comments on why this method was chosen describing:\\n\"We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\"[1]\\n\\nHistorical context[edit]\\nMain articles: Transformer (deep learning architecture) §\\xa0History, and Seq2seq §\\xa0History\\nSee also: Timeline of machine learning\\n Predecessors[edit]\\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model\\'s state at the end of a long sentence without precise, extractable information about preceding tokens.\\nA key breakthrough was LSTM (1995),[note 1] a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units.[13] Neural networks using multiplicative units were later called sigma-pi networks[14] or higher-order networks.[15] LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\\nHowever, LSTM still used sequential processing, like most other RNNs.[note 2] Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.\\nModern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input.[16] One of its two networks has \"fast weights\" or \"dynamic links\" (1981).[17][18][19] A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries.[16] This was later shown to be equivalent to the unnormalized linear Transformer.[20][21]\\n\\n Attention with seq2seq[edit]\\nMain article: Seq2seq §\\xa0History\\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.[22][23]\\nA 380M-parameter model for machine translation uses two long short-term memories (LSTM).[23] Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM.[22] Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.[24][25]\\nThese early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.[26]\\nThe RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\".[4]\\nThe relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.[27]\\nIn 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM.[28] It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.[29]\\n\\n Parallelizing attention[edit]\\nMain article: Attention (machine learning) §\\xa0History\\nSeq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs.[30] One of its authors, Jakob Uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title \"attention is all you need\".[31] That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical.[31] In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.[32]\\nIn 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance.[1] This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.[33]\\n\\n\\n AI boom era[edit]\\nAlready in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles.[34] Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.\\nIn language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model.[35] In 2019 October, Google started using BERT to process search queries.[36] In 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model.[37]\\nStarting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly[38] popular, triggering a boom around large language models.[39][40]\\nSince 2020, Transformers have been applied in modalities beyond text, including the vision transformer,[41] speech recognition,[42] robotics,[43] and multimodal.[44] The vision transformer, in turn, stimulated new developments in convolutional neural networks.[45] Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024),[46] and Sora (2024), use Transformers to analyse input data (like text prompts) by breaking it down into \"tokens\" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data.\\n\\nTraining[edit]\\nWhile the primary focus of the paper at the time was to improve machine translation, the paper also discussed the use of the architecture on English Constituency Parsing, both with limited and large-sized datasets, achieving a high-score without specific tuning for the task indicating the promising nature of the model for use in a wide-variety of general purpose of seq2seq tasks.\\nDataset\\nThe English-to-German translation model was trained on the 2014 WMT (Workshop on Statistical Machine Translation) English-German dataset, consisting of nearly 4.5 million sentences derived from TED Talks and high-quality news articles. A separate translation model was trained on the much larger 2014 WMT English-French dataset, consisting of 36 million sentences. Both datasets were encoded with byte-pair encoding.\\nHardware\\nThe models were trained using 8 NVIDIA P100 GPUs. The base models were trained for 100,000 steps and the big models were trained for 300,000 steps - each step taking about 0.4 seconds to complete for the base models and 1.0 seconds for the big models. The base model trained for a total of 12 hours, and the big model trained for a total of 3.5 days. Both the base and big models outperforms the 2017 state-of-the-art in both English-German and English-French while achieving the comparatively lowest training cost.[1] The estimated computing cost was 0.089 petaFLOP-days.[47]\\nHyperparameters and regularization\\nFor their 100M-parameter Transformer model, the authors increased the learning rate linearly for the first 4000 (warmup) steps and decreased it proportionally to inverse square root of the current step number. Dropout layers were applied to the output of each sub-layer before normalization, the sums of the embeddings, and the positional encodings. The dropout rate was set to 0.1. Label smoothing was applied with a value of 0.1 which \"improves accuracy and BLEU score\".[1]\\n\\nNotes[edit]\\n\\n\\n^ Gated recurrent units (2014) further reduced its complexity.\\n\\n^ Some architectures, such as RWKV or state space models, avoid the issue.\\n\\n\\nReferences[edit]\\n\\n\\n^ a b c d e f Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (December 2017). \"Attention is All you Need\". In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). 31st Conference on  Neural Information Processing Systems (NIPS). Advances in Neural Information Processing Systems. Vol.\\xa030. Curran Associates, Inc. arXiv:1706.03762.\\n\\n^ Love, Julia (10 July 2023). \"AI Researcher Who Helped Write Landmark Paper Is Leaving Google\". Bloomberg News. Retrieved 1 April 2024.\\n\\n^ Goldman, Sharon (20 March 2024). \"\\'Attention is All You Need\\' creators look beyond Transformers for AI at Nvidia GTC: \\'The world needs something better\\'\". VentureBeat. Retrieved 1 April 2024.\\n\\n^ a b Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (19 May 2016). \"Neural Machine Translation by Jointly Learning to Align and Translate\". arXiv:1409.0473 [cs.CL].\\n\\n^ Shinde, Gitanjali; Wasatkar, Namrata; Mahalle, Parikshit (6 June 2024). Data-Centric Artificial Intelligence for Multidisciplinary Applications. CRC Press. p.\\xa075. ISBN\\xa09781040031131.\\n\\n^ Toews, Rob (3 September 2023). \"Transformers Revolutionized AI. What Will Replace Them?\". Forbes. Archived from the original on 26 September 2023. Retrieved 3 December 2023.\\n\\n^ Murgia, Madhumita (23 July 2023). \"Transformers: the Google scientists who pioneered an AI revolution\". Financial Times. Archived from the original on 28 December 2023. Retrieved 22 March 2024.\\n\\n^ a b c Levy, Steven. \"8 Google Employees Invented Modern AI. Here\\'s the Inside Story\". Wired. ISSN\\xa01059-1028. Retrieved 20 March 2024.\\n\\n^ a b Marche, Stephen (23 August 2024). \"Was Linguistic A.I. Created by Accident?\". The New Yorker. ISSN\\xa00028-792X. Retrieved 24 August 2024.\\n\\n^ \"Meet the $4 Billion AI Superstars That Google Lost\". Bloomberg. 13 July 2023 – via www.bloomberg.com.\\n\\n^ \"Exclusive: the most-cited papers of the twenty-first century\". Nature. 15 April 2025. Retrieved 18 April 2025.\\n\\n^ Murgia, Madhumita (23 July 2023). \"Transformers: the Google scientists who pioneered an AI revolution\". Financial Times. Retrieved 22 March 2025.\\n\\n^ Feldman, J. A.; Ballard, D. H. (1 July 1982). \"Connectionist models and their properties\". Cognitive Science. 6 (3): 205–254. doi:10.1016/S0364-0213(82)80001-3. ISSN\\xa00364-0213.\\n\\n^ Rumelhart, David E.; McClelland, James L.; Hinton, Geoffrey E. (29 July 1987). Parallel Distributed Processing, Volume 1: Explorations in the Microstructure of Cognition: Foundations, Chapter 2 (PDF). Cambridge, Mass: Bradford Books. ISBN\\xa0978-0-262-68053-0.\\n\\n^ Giles, C. Lee; Maxwell, Tom (1 December 1987). \"Learning, invariance, and generalization in high-order neural networks\". Applied Optics. 26 (23): 4972–4978. doi:10.1364/AO.26.004972. ISSN\\xa00003-6935. PMID\\xa020523475.\\n\\n^ a b Schmidhuber, Jürgen (1992). \"Learning to control fast-weight memories: an alternative to recurrent nets\" (PDF). Neural Computation. 4 (1): 131–139. doi:10.1162/neco.1992.4.1.131. S2CID\\xa016683347.\\n\\n^ Christoph von der Malsburg: The correlation theory of brain function. Internal Report 81-2, MPI Biophysical Chemistry, 1981. http://cogprints.org/1380/1/vdM_correlation.pdf See Reprint in Models of Neural Networks II, chapter 2, pages 95–119. Springer, Berlin, 1994.\\n\\n^ Jerome A. Feldman, \"Dynamic connections in neural networks,\" Biological Cybernetics, vol. 46, no. 1, pp. 27–39, Dec. 1982.\\n\\n^ Hinton, Geoffrey E.; Plaut, David C. (1987). \"Using Fast Weights to Deblur Old Memories\". Proceedings of the Annual Meeting of the Cognitive Science Society. 9.\\n\\n^ Katharopoulos, Angelos; Vyas, Apoorv; Pappas, Nikolaos; Fleuret, François (2020). \"Transformers are RNNs: Fast autoregressive Transformers with linear attention\". ICML 2020. PMLR. pp.\\xa05156–5165.\\n\\n^ Schlag, Imanol; Irie, Kazuki; Schmidhuber, Jürgen (2021). \"Linear Transformers Are Secretly Fast Weight Programmers\". ICML 2021. Springer. pp.\\xa09355–9366.\\n\\n^ a b Cho, Kyunghyun; van Merriënboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (October 2014). \"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation\". In Moschitti, Alessandro; Pang, Bo; Daelemans, Walter (eds.). Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar: Association for Computational Linguistics. pp.\\xa01724–1734. arXiv:1406.1078. doi:10.3115/v1/D14-1179.\\n\\n^ a b Sutskever, Ilya; Vinyals, Oriol; Le, Quoc Viet (14 December 2014). \"Sequence to sequence learning with neural networks\". arXiv:1409.3215 [cs.CL]. [first version posted to arXiv on 10 Sep 2014]\\n\\n^ Chung, Junyoung; Gulcehre, Caglar; Cho, KyungHyun; Bengio, Yoshua (2014). \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\". arXiv:1412.3555 [cs.NE].\\n\\n^ Gruber, N.; Jockisch, A. (2020), \"Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?\", Frontiers in Artificial Intelligence, 3: 40, doi:10.3389/frai.2020.00040, PMC\\xa07861254, PMID\\xa033733157, S2CID\\xa0220252321\\n\\n^ Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V (2014). \"Sequence to Sequence Learning with Neural Networks\". Advances in Neural Information Processing Systems. 27. Curran Associates, Inc. arXiv:1409.3215.\\n\\n^ Luong, Minh-Thang; Pham, Hieu; Manning, Christopher D. (2015). \"Effective Approaches to Attention-based Neural Machine Translation\". arXiv:1508.04025 [cs.CL].\\n\\n^ Wu, Yonghui; et\\xa0al. (1 September 2016). \"Google\\'s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\". arXiv:1609.08144 [cs.CL].\\n\\n^ Lewis-Kraus, Gideon (14 December 2016). \"The Great A.I. Awakening\". The New York Times. ISSN\\xa00362-4331. Archived from the original on 24 May 2023. Retrieved 22 June 2023.\\n\\n^ Parikh, Ankur P.; Täckström, Oscar; Das, Dipanjan; Uszkoreit, Jakob (25 September 2016). \"A Decomposable Attention Model for Natural Language Inference\". arXiv:1606.01933 [cs.CL].\\n\\n^ a b Levy, Steven. \"8 Google Employees Invented Modern AI. Here\\'s the Inside Story\". Wired. ISSN\\xa01059-1028. Archived from the original on 20 March 2024. Retrieved 6 August 2024.\\n\\n^ Cheng, Jianpeng; Dong, Li; Lapata, Mirella (November 2016). \"Long Short-Term Memory-Networks for Machine Reading\". In Su, Jian; Duh, Kevin; Carreras, Xavier (eds.). Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas: Association for Computational Linguistics. pp.\\xa0551–561. doi:10.18653/v1/D16-1053.\\n\\n^ Peng, Bo; Alcaide, Eric; Anthony, Quentin; Albalak, Alon; Arcadinho, Samuel; Biderman, Stella; Cao, Huanqi; Cheng, Xin; Chung, Michael (10 December 2023), RWKV: Reinventing RNNs for the Transformer Era, arXiv:2305.13048\\n\\n^ Marche, Stephen (23 August 2024). \"Was Linguistic A.I. Created by Accident?\". The New Yorker. ISSN\\xa00028-792X. Retrieved 27 August 2024.\\n\\n^ Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805v2 [cs.CL].\\n\\n^ \"Google: BERT now used on almost every English query\". Search Engine Land. 15 October 2020. Retrieved 24 November 2020.\\n\\n^ \"Recent Advances in Google Translate\". research.google. Retrieved 8 May 2024.\\n\\n^ \"The inside story of how ChatGPT was built from the people who made it\". MIT Technology Review. Retrieved 6 August 2024.\\n\\n^ \"Improving language understanding with unsupervised learning\". openai.com. 11 June 2018. Archived from the original on 18 March 2023. Retrieved 18 March 2023.\\n\\n^ finetune-transformer-lm, OpenAI, 11 June 2018, retrieved 1 May 2023\\n\\n^ Dosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob (3 June 2021). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\". arXiv:2010.11929 [cs.CV].\\n\\n^ Gulati, Anmol; Qin, James; Chiu, Chung-Cheng; Parmar, Niki; Zhang, Yu; Yu, Jiahui; Han, Wei; Wang, Shibo; Zhang, Zhengdong; Wu, Yonghui; Pang, Ruoming (2020). \"Conformer: Convolution-augmented Transformer for Speech Recognition\". arXiv:2005.08100 [eess.AS].\\n\\n^ Chen, Lili; Lu, Kevin; Rajeswaran, Aravind; Lee, Kimin; Grover, Aditya; Laskin, Michael; Abbeel, Pieter; Srinivas, Aravind; Mordatch, Igor (24 June 2021), Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345\\n\\n^ Choromanski, Krzysztof; Likhosherstov, Valerii; Dohan, David; Song, Xingyou; Gane, Andreea; Sarlos, Tamas; Hawkins, Peter; Davis, Jared; Mohiuddin, Afroz (19 November 2022), Rethinking Attention with Performers, arXiv:2009.14794\\n\\n^ Liu, Zhuang; Mao, Hanzi; Wu, Chao-Yuan; Feichtenhofer, Christoph; Darrell, Trevor; Xie, Saining (2022). A ConvNet for the 2020s. Conference on Computer Vision and Pattern Recognition. pp.\\xa011976–11986.\\n\\n^ Esser, Patrick; Kulal, Sumith; Blattmann, Andreas; Entezari, Rahim; Müller, Jonas; Saini, Harry; Levi, Yam; Lorenz, Dominik; Sauer, Axel (5 March 2024), Scaling Rectified Flow Transformers for High-Resolution Image Synthesis, arXiv:2403.03206\\n\\n^ \"AI and compute\". openai.com. 9 June 2022. Retrieved 29 April 2025.\\n\\n\\nExternal links[edit]\\nUszkoreit, Jakob (31 August 2017). \"Transformer: A Novel Neural Network Architecture for Language Understanding\". research.google. Retrieved 9 August 2024. A concurrent blog post on Google Research blog.\\nvteGoogle AI\\nGoogle\\nGoogle Brain\\nGoogle DeepMind\\nComputer programsAlphaGoVersions\\nAlphaGo (2015)\\nMaster (2016)\\nAlphaGo Zero (2017)\\nAlphaZero (2017)\\nMuZero (2019)\\nCompetitions\\nFan Hui (2015)\\nLee Sedol (2016)\\nKe Jie (2017)\\nIn popular culture\\nAlphaGo (2017)\\nThe MANIAC (2023)\\nOther\\nAlphaFold (2018)\\nAlphaStar (2019)\\nAlphaDev (2023)\\nAlphaGeometry (2024)\\nMachine learningNeural networks\\nInception (2014)\\nWaveNet (2016)\\nMobileNet (2017)\\nTransformer (2017)\\nEfficientNet (2019)\\nGato (2022)\\nOther\\nQuantum Artificial Intelligence Lab\\nTensorFlow\\nTensor Processing Unit\\nGenerative AIChatbots\\nAssistant (2016)\\nSparrow (2022)\\nGemini (2023)\\nModels\\nBERT (2018)\\nXLNet (2019)\\nT5 (2019)\\nLaMDA (2021)\\nChinchilla (2022)\\nPaLM (2022)\\nImagen (2023)\\nGemini (2023)\\nVideoPoet (2024)\\nVeo (text-to-video model) (2024)\\nOther\\nDreamBooth (2022)\\nNotebookLM (2023)\\nVids (2024)\\nGemini Robotics (2025)\\nSee also\\n\"Attention Is All You Need\"\\nFuture of Go Summit\\nGenerative pre-trained transformer\\nGoogle Labs\\nGoogle Pixel\\nGoogle Workspace\\nRobot Constitution\\n\\n Category\\n Commons\\n\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Attention_Is_All_You_Need&oldid=1296709299\"\\nCategories: 2017 documentsArtificial intelligence papersGoogle2017 in artificial intelligenceHidden categories: Articles with short descriptionShort description is different from WikidataUse dmy dates from December 2023Articles containing potentially dated statements from 2025All articles containing potentially dated statements\\n\\n\\n\\n\\n\\n\\n This page was last edited on 21 June 2025, at 19:00\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License;\\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nAttention Is All You Need\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n13 languages\\n\\n\\nAdd topic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://en.wikipedia.org/wiki/Seq2seq', 'title': 'Seq2seq - Wikipedia', 'language': 'en'}, page_content='\\n\\n\\n\\nSeq2seq - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDonate\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\nDonate Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1\\nHistory\\n\\n\\n\\n\\nToggle History subsection\\n\\n\\n\\n\\n\\n1.1\\nPriority dispute\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\nArchitecture\\n\\n\\n\\n\\nToggle Architecture subsection\\n\\n\\n\\n\\n\\n2.1\\nEncoder\\n\\n\\n\\n\\n\\n\\n\\n\\n2.2\\nDecoder\\n\\n\\n\\n\\n\\n\\n\\n\\n2.3\\nTraining vs prediction\\n\\n\\n\\n\\n\\n\\n\\n\\n2.4\\nAttention for seq2seq\\n\\n\\n\\n\\n\\n\\n\\n\\n2.5\\nAttention weights\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\nOther applications\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\nSee also\\n\\n\\n\\n\\n\\n\\n\\n\\n5\\nReferences\\n\\n\\n\\n\\n\\n\\n\\n\\n6\\nExternal links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nSeq2seq\\n\\n\\n\\n8 languages\\n\\n\\n\\n\\nالعربيةCatalàفارسی한국어日本語ไทยTiếng Việt中文\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\nWikidata item\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\nFamily of machine learning approaches\\nAnimation of seq2seq with RNN and attention mechanism\\nSeq2seq is a family of machine learning approaches used for natural language processing.[1] Applications include language translation,[2] image captioning,[3] conversational models,[4] speech recognition,[5] and text summarization.[6]\\nSeq2seq uses sequence transformation: it turns one sequence into another sequence.\\n\\n\\nHistory[edit]\\nOne naturally wonders if the problem of translation could conceivably be treated as a problem in cryptography. When I look at an article in Russian, I say: \\'This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.—\\u200aWarren Weaver, Letter to Norbert Wiener, March 4, 1947\\nShannon\\'s diagram of a general communications system, showing the process by which a message sent becomes the message received (possibly corrupted by noise)\\nseq2seq is an approach to machine translation (or more generally, sequence transduction) with roots in information theory, where communication is understood as an encode-transmit-decode process, and machine translation can be studied as a special case of communication. This viewpoint was elaborated, for example, in the noisy channel model of machine translation.\\nIn practice, seq2seq maps an input sequence into a real-numerical vector by using a neural network (the encoder), and then maps it back to an output sequence using another neural network (the decoder).\\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s (see [2][1] for previous papers). The papers most commonly cited as the originators that produced seq2seq are two papers from 2014.[2][1]\\nIn the seq2seq as proposed by them, both the encoder and the decoder were LSTMs. This had the \"bottleneck\" problem, since the encoding vector has a fixed size, so for long input sequences, information would tend to be lost, as they are difficult to fit into the fixed-length encoding vector. The attention mechanism, proposed in 2014,[7] resolved the bottleneck problem. They called their model RNNsearch, as it \"emulates searching through a source sentence during decoding a translation\".\\nA problem with seq2seq models at this point was that recurrent neural networks are difficult to parallelize. The 2017 publication of Transformers[8] resolved the problem by replacing the encoding RNN with self-attention Transformer blocks (\"encoder blocks\"), and the decoding RNN with cross-attention causally-masked Transformer blocks (\"decoder blocks\").\\n\\nPriority dispute[edit]\\nOne of the papers cited as the originator for seq2seq is (Sutskever et al 2014),[1] published at Google Brain while they were on Google\\'s machine translation project. The research allowed Google to overhaul Google Translate into Google Neural Machine Translation in 2016.[1][9] Tomáš Mikolov claims to have developed the idea (before joining Google Brain) of using a \"neural language model on pairs of sentences... and then [generating] translation after seeing the first sentence\"—which he equates with seq2seq machine translation, and to have mentioned the idea to Ilya Sutskever and Quoc Le (while at Google Brain), who failed to acknowledge him in their paper.[10] Mikolov had worked on RNNLM (using RNN for language modelling) for his PhD thesis,[11] and is more notable for developing word2vec.\\n\\nArchitecture[edit]\\nThe main reference for this section is.[12]\\n\\nEncoder[edit]\\nRNN encoder\\nThe encoder is responsible for processing the input sequence and capturing its essential information, which is stored as the hidden state of the network and, in a model with attention mechanism, a context vector. The context vector is the weighted sum of the input hidden states and is generated for every time instance in the output sequences.\\n\\nDecoder[edit]\\nRNN decoder\\nThe decoder takes the context vector and hidden states from the encoder and generates the final output sequence. The decoder operates in an autoregressive manner, producing one element of the output sequence at a time. At each step, it considers the previously generated elements, the context vector, and the input sequence information to make predictions for the next element in the output sequence. Specifically, in a model with attention mechanism, the context vector and the hidden state are concatenated together to form an attention hidden vector, which is used as an input for the decoder.\\nThe seq2seq method developed in the early 2010s uses two neural networks:  an encoder network converts an input sentence into numerical vectors, and a decoder network converts those vectors to sentences in the target language.  The Attention mechanism was grafted onto this structure in 2014 and shown below. Later it was refined into the encoder-decoder Transformer architecture of 2017.\\n\\nTraining vs prediction[edit]\\nTraining a seq2seq model via teacher forcing.\\nPredicting a sequence using a seq2seq model.\\nThere is a subtle difference between training and prediction. During training time, both the input and the output sequences are known. During prediction time, only the input sequence is known, and the output sequence must be decoded by the network itself.\\nSpecifically, consider an input sequence \\n\\n\\n\\n\\nx\\n\\n1\\n:\\nn\\n\\n\\n\\n\\n{\\\\displaystyle x_{1:n}}\\n\\n and output sequence \\n\\n\\n\\n\\ny\\n\\n1\\n:\\nm\\n\\n\\n\\n\\n{\\\\displaystyle y_{1:m}}\\n\\n. The encoder would process the input \\n\\n\\n\\n\\nx\\n\\n1\\n:\\nn\\n\\n\\n\\n\\n{\\\\displaystyle x_{1:n}}\\n\\n step by step. After that, the decoder would take the output from the encoder, as well as the <bos> as input, and produce a prediction \\n\\n\\n\\n\\n\\n\\n\\ny\\n^\\n\\n\\n\\n\\n1\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\hat {y}}_{1}}\\n\\n. Now, the question is: what should be input to the decoder in the next step?\\nA standard method for training is \"teacher forcing\". In teacher forcing, no matter what is output by the decoder, the next input to the decoder is always the reference. That is, even if \\n\\n\\n\\n\\n\\n\\n\\ny\\n^\\n\\n\\n\\n\\n1\\n\\n\\n≠\\n\\ny\\n\\n1\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\hat {y}}_{1}\\\\neq y_{1}}\\n\\n, the next input to the decoder is still \\n\\n\\n\\n\\ny\\n\\n1\\n\\n\\n\\n\\n{\\\\displaystyle y_{1}}\\n\\n, and so on.\\nDuring prediction time, the \"teacher\" \\n\\n\\n\\n\\ny\\n\\n1\\n:\\nm\\n\\n\\n\\n\\n{\\\\displaystyle y_{1:m}}\\n\\n would be unavailable. Therefore, the input to the decoder must be \\n\\n\\n\\n\\n\\n\\n\\ny\\n^\\n\\n\\n\\n\\n1\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\hat {y}}_{1}}\\n\\n, then \\n\\n\\n\\n\\n\\n\\n\\ny\\n^\\n\\n\\n\\n\\n2\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\hat {y}}_{2}}\\n\\n, and so on.\\nIt is found that if a model is trained purely by teacher forcing, its performance would degrade during prediction time, since generation based on the model\\'s own output is different from generation based on the teacher\\'s output. This is called exposure bias or a train/test distribution shift. A 2015 paper recommends that, during training, randomly switch between teacher forcing and no teacher forcing.[13]\\n\\nAttention for seq2seq[edit]\\nThe attention mechanism is an enhancement introduced by Bahdanau et al. in 2014  to address limitations in the basic Seq2Seq architecture where a longer input sequence results in the hidden state output of the encoder becoming irrelevant for the decoder. It enables the model to selectively focus on different parts of the input sequence during the decoding process. At each decoder step, an alignment model calculates the attention score using the current decoder state and all of the attention hidden vectors as input. An alignment model is another neural network model that is trained jointly with the seq2seq model used to calculate how well an input, represented by the hidden state, matches with the previous output, represented by attention hidden state. A softmax function is then applied to the attention score to get the attention weight.Seq2seq RNN encoder-decoder with attention mechanism, where the detailed construction of attention mechanism is exposed. See attention mechanism page for details.\\nIn some models, the encoder states are directly fed into an activation function, removing the need for alignment model. An activation function receives one decoder state and one encoder state and returns a scalar value of their relevance.\\n\\nAnimation of seq2seq with RNN and attention mechanism\\nConsider the seq2seq language English-to-French translation task. To be concrete, let us consider the translation of \"the zone of international control <end>\", which should translate to \"la zone de contrôle international <end>\". Here, we use the special <end> token as a control character to delimit the end of input for both the encoder and the decoder.\\nAn input sequence of text \\n\\n\\n\\n\\nx\\n\\n0\\n\\n\\n,\\n\\nx\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle x_{0},x_{1},\\\\dots }\\n\\n is processed by a neural network (which can be an LSTM, a Transformer encoder, or some other network) into a sequence of real-valued vectors \\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle h_{0},h_{1},\\\\dots }\\n\\n, where \\n\\n\\n\\nh\\n\\n\\n{\\\\displaystyle h}\\n\\n stands for \"hidden vector\".\\nAfter the encoder has finished processing, the decoder starts operating over the hidden vectors, to produce an output sequence \\n\\n\\n\\n\\ny\\n\\n0\\n\\n\\n,\\n\\ny\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle y_{0},y_{1},\\\\dots }\\n\\n, autoregressively. That is, it always takes as input both the hidden vectors produced by the encoder, and what the decoder itself has produced before, to produce the next output word:\\n\\n(\\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle h_{0},h_{1},\\\\dots }\\n\\n, \"<start>\") → \"la\"\\n(\\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle h_{0},h_{1},\\\\dots }\\n\\n, \"<start> la\") → \"la zone\"\\n(\\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle h_{0},h_{1},\\\\dots }\\n\\n, \"<start> la zone\") → \"la zone de\"\\n...\\n(\\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle h_{0},h_{1},\\\\dots }\\n\\n, \"<start> la zone de contrôle international\") → \"la zone de contrôle international <end>\"\\nHere, we use the special <start> token as a control character to delimit the start of input for the decoder. The decoding terminates as soon as \"<end>\" appears in the decoder output.\\n\\nAttention weights[edit]\\nAttention mechanism with attention weights, overview.\\nAs hand-crafting weights defeats the purpose of machine learning, the model must compute the attention weights on its own. Taking analogy from the language of database queries, we make the model construct a triple of vectors: key, query, and value. The rough idea is that we have a \"database\" in the form of a list of key-value pairs. The decoder sends in a query, and obtains a reply in the form of a weighted sum of the values, where the weight is proportional to how closely the query resembles each key.\\nThe decoder first processes the \"<start>\" input partially, to obtain an intermediate vector \\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\nd\\n\\n\\n\\n\\n{\\\\displaystyle h_{0}^{d}}\\n\\n, the 0th hidden vector of decoder. Then, the intermediate vector is transformed by a linear map \\n\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n\\n\\n{\\\\displaystyle W^{Q}}\\n\\n into a query vector \\n\\n\\n\\n\\nq\\n\\n0\\n\\n\\n=\\n\\nh\\n\\n0\\n\\n\\nd\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n\\n\\n{\\\\displaystyle q_{0}=h_{0}^{d}W^{Q}}\\n\\n. Meanwhile, the hidden vectors outputted by the encoder are transformed by another linear map \\n\\n\\n\\n\\nW\\n\\nK\\n\\n\\n\\n\\n{\\\\displaystyle W^{K}}\\n\\n into key vectors \\n\\n\\n\\n\\nk\\n\\n0\\n\\n\\n=\\n\\nh\\n\\n0\\n\\n\\n\\nW\\n\\nK\\n\\n\\n,\\n\\nk\\n\\n1\\n\\n\\n=\\n\\nh\\n\\n1\\n\\n\\n\\nW\\n\\nK\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle k_{0}=h_{0}W^{K},k_{1}=h_{1}W^{K},\\\\dots }\\n\\n. The linear maps are useful for providing the model with enough freedom to find the best way to represent the data.\\nNow, the query and keys are compared by taking dot products: \\n\\n\\n\\n\\nq\\n\\n0\\n\\n\\n\\nk\\n\\n0\\n\\n\\nT\\n\\n\\n,\\n\\nq\\n\\n0\\n\\n\\n\\nk\\n\\n1\\n\\n\\nT\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle q_{0}k_{0}^{T},q_{0}k_{1}^{T},\\\\dots }\\n\\n. Ideally, the model should have learned to compute the keys and values, such that \\n\\n\\n\\n\\nq\\n\\n0\\n\\n\\n\\nk\\n\\n0\\n\\n\\nT\\n\\n\\n\\n\\n{\\\\displaystyle q_{0}k_{0}^{T}}\\n\\n is large, \\n\\n\\n\\n\\nq\\n\\n0\\n\\n\\n\\nk\\n\\n1\\n\\n\\nT\\n\\n\\n\\n\\n{\\\\displaystyle q_{0}k_{1}^{T}}\\n\\n is small, and the rest are very small. This can be interpreted as saying that the attention weight should be mostly applied to the 0th hidden vector of the encoder, a little to the 1st, and essentially none to the rest.\\nIn order to make a properly weighted sum, we need to transform this list of dot products into a probability distribution over \\n\\n\\n\\n0\\n,\\n1\\n,\\n…\\n\\n\\n{\\\\displaystyle 0,1,\\\\dots }\\n\\n. This can be accomplished by the softmax function, thus giving us the attention weights:\\n\\n\\n\\n(\\n\\nw\\n\\n00\\n\\n\\n,\\n\\nw\\n\\n01\\n\\n\\n,\\n…\\n)\\n=\\n\\ns\\no\\nf\\nt\\nm\\na\\nx\\n\\n(\\n\\nq\\n\\n0\\n\\n\\n\\nk\\n\\n0\\n\\n\\nT\\n\\n\\n,\\n\\nq\\n\\n0\\n\\n\\n\\nk\\n\\n1\\n\\n\\nT\\n\\n\\n,\\n…\\n)\\n\\n\\n{\\\\displaystyle (w_{00},w_{01},\\\\dots )=\\\\mathrm {softmax} (q_{0}k_{0}^{T},q_{0}k_{1}^{T},\\\\dots )}\\n\\nThis is then used to compute the context vector:\\n\\n\\n\\n\\nc\\n\\n0\\n\\n\\n=\\n\\nw\\n\\n00\\n\\n\\n\\nv\\n\\n0\\n\\n\\n+\\n\\nw\\n\\n01\\n\\n\\n\\nv\\n\\n1\\n\\n\\n+\\n⋯\\n\\n\\n{\\\\displaystyle c_{0}=w_{00}v_{0}+w_{01}v_{1}+\\\\cdots }\\n\\n\\nwhere \\n\\n\\n\\n\\nv\\n\\n0\\n\\n\\n=\\n\\nh\\n\\n0\\n\\n\\n\\nW\\n\\nV\\n\\n\\n,\\n\\nv\\n\\n1\\n\\n\\n=\\n\\nh\\n\\n1\\n\\n\\n\\nW\\n\\nV\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle v_{0}=h_{0}W^{V},v_{1}=h_{1}W^{V},\\\\dots }\\n\\n are the value vectors, linearly transformed by another matrix to provide the model with freedom to find the best way to represent values. Without the matrices \\n\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n,\\n\\nW\\n\\nK\\n\\n\\n,\\n\\nW\\n\\nV\\n\\n\\n\\n\\n{\\\\displaystyle W^{Q},W^{K},W^{V}}\\n\\n, the model would be forced to use the same hidden vector for both key and value, which might not be appropriate, as these two tasks are not the same.\\n\\nComputing the attention weights by dot-product. This is the \"decoder cross-attention\".\\nThis is the dot-attention mechanism. The particular version described in this section is \"decoder cross-attention\", as the output context vector is used by the decoder, and the input keys and values come from the encoder, but the query comes from the decoder, thus \"cross-attention\".\\n\\nMore succinctly, we can write it as\\n\\n\\n\\n\\nc\\n\\n0\\n\\n\\n=\\n\\nA\\nt\\nt\\ne\\nn\\nt\\ni\\no\\nn\\n\\n(\\n\\nh\\n\\n0\\n\\n\\nd\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n,\\nH\\n\\nW\\n\\nK\\n\\n\\n,\\nH\\n\\nW\\n\\nV\\n\\n\\n)\\n=\\n\\ns\\no\\nf\\nt\\nm\\na\\nx\\n\\n(\\n(\\n\\nh\\n\\n0\\n\\n\\nd\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n)\\n\\n(\\nH\\n\\nW\\n\\nK\\n\\n\\n\\n)\\n\\nT\\n\\n\\n)\\n(\\nH\\n\\nW\\n\\nV\\n\\n\\n)\\n\\n\\n{\\\\displaystyle c_{0}=\\\\mathrm {Attention} (h_{0}^{d}W^{Q},HW^{K},HW^{V})=\\\\mathrm {softmax} ((h_{0}^{d}W^{Q})\\\\;(HW^{K})^{T})(HW^{V})}\\n\\nwhere the matrix \\n\\n\\n\\nH\\n\\n\\n{\\\\displaystyle H}\\n\\n is the matrix whose rows are \\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle h_{0},h_{1},\\\\dots }\\n\\n. Note that the querying vector, \\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\nd\\n\\n\\n\\n\\n{\\\\displaystyle h_{0}^{d}}\\n\\n, is not necessarily the same as the key-value vector \\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n\\n\\n{\\\\displaystyle h_{0}}\\n\\n. In fact, it is theoretically possible for query, key, and value vectors to all be different, though that is rarely done in practice.Seq2seq RNN encoder-decoder with attention mechanism, training\\nSeq2seq RNN encoder-decoder with attention mechanism, training and inferring\\nOther applications[edit]\\nIn 2019, Facebook announced its use in symbolic integration and resolution of differential equations. The company claimed that it could solve complex equations more rapidly and with greater accuracy than commercial solutions such as Mathematica, MATLAB and Maple. First, the equation is parsed into a tree structure to avoid notational idiosyncrasies. An LSTM neural network then applies its standard pattern recognition facilities to process the tree.[14][15]\\nIn 2020, Google released Meena, a 2.6 billion parameter seq2seq-based chatbot trained on a 341 GB data set. Google claimed that the chatbot has 1.7 times greater model capacity than OpenAI\\'s GPT-2.[4]\\nIn 2022, Amazon introduced AlexaTM 20B, a moderate-sized (20 billion parameter) seq2seq language model. It uses an encoder-decoder to accomplish few-shot learning. The encoder outputs a representation of the input that the decoder uses as input to perform a specific task, such as translating the input into another language. The model outperforms the much larger GPT-3 in language translation and summarization. Training mixes denoising (appropriately inserting missing text in strings) and causal-language-modeling (meaningfully extending an input text). It allows adding features across different languages without massive training workflows. AlexaTM 20B achieved state-of-the-art performance in few-shot-learning tasks across all Flores-101 language pairs, outperforming GPT-3 on several tasks.[16]\\n\\nSee also[edit]\\nArtificial neural network\\nReferences[edit]\\n\\n\\n^ a b c d e Sutskever, Ilya; Vinyals, Oriol; Le, Quoc Viet (2014). \"Sequence to sequence learning with neural networks\". arXiv:1409.3215 [cs.CL].\\n\\n^ a b c Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (2014-06-03). \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\". arXiv:1406.1078 [cs.CL].\\n\\n^ Xu, Kelvin; Ba, Jimmy; Kiros, Ryan; Cho, Kyunghyun; Courville, Aaron; Salakhudinov, Ruslan; Zemel, Rich; Bengio, Yoshua (2015-06-01). \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\". Proceedings of the 32nd International Conference on Machine Learning. PMLR: 2048–2057.\\n\\n^ a b Adiwardana, Daniel; Luong, Minh-Thang; So, David R.; Hall, Jamie; Fiedel, Noah; Thoppilan, Romal; Yang, Zi; Kulshreshtha, Apoorv; Nemade, Gaurav; Lu, Yifeng; Le, Quoc V. (2020-01-31). \"Towards a Human-like Open-Domain Chatbot\". arXiv:2001.09977 [cs.CL].\\n\\n^ Chan, William; Jaitly, Navdeep; Le, Quoc; Vinyals, Oriol (March 2016). \"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition\". 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. pp.\\xa04960–4964. doi:10.1109/ICASSP.2016.7472621. ISBN\\xa0978-1-4799-9988-0.\\n\\n^ Rush, Alexander M.; Chopra, Sumit; Weston, Jason (September 2015). \"A Neural Attention Model for Abstractive Sentence Summarization\". In Màrquez, Lluís; Callison-Burch, Chris; Su, Jian (eds.). Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal: Association for Computational Linguistics. pp.\\xa0379–389. doi:10.18653/v1/D15-1044.\\n\\n^ Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (2014). \"Neural Machine Translation by Jointly Learning to Align and Translate\". arXiv:1409.0473 [cs.CL].\\n\\n^ Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Ł ukasz; Polosukhin, Illia (2017). \"Attention is All you Need\". Advances in Neural Information Processing Systems. 30. Curran Associates, Inc.\\n\\n^ Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V.; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin; Macherey, Klaus; Klingner, Jeff; Shah, Apurva; Johnson, Melvin; Liu, Xiaobing; Kaiser, Łukasz (2016). \"Google\\'s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\". arXiv:1609.08144 [cs.CL].\\n\\n^ Mikolov, Tomáš (December 13, 2023). \"Yesterday we received a Test of Time Award at NeurIPS for the word2vec paper from ten years ago\". Facebook. Archived from the original on 24 Dec 2023.\\n\\n^ Mikolov, Tomáš. \"Statistical language models based on neural networks.\" (2012).\\n\\n^ Zhang, Aston; Lipton, Zachary; Li, Mu; Smola, Alexander J. (2024). \"10.7. Sequence-to-Sequence Learning for Machine Translation\". Dive into deep learning. Cambridge New York Port Melbourne New Delhi Singapore: Cambridge University Press. ISBN\\xa0978-1-009-38943-3.\\n\\n^ Bengio, Samy; Vinyals, Oriol; Jaitly, Navdeep; Shazeer, Noam (2015). \"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks\". Advances in Neural Information Processing Systems. 28. Curran Associates, Inc.\\n\\n^ \"Facebook has a neural network that can do advanced math\". MIT Technology Review. December 17, 2019. Retrieved 2019-12-17.\\n\\n^ Lample, Guillaume; Charton, François (2019). \"Deep Learning for Symbolic Mathematics\". arXiv:1912.01412 [cs.SC].\\n\\n^ Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; Gupta, Rahul; Hamza, Wael; Khan, Haidar; Peris, Charith; Rawls, Stephen; Rosenbaum, Andy; Rumshisky, Anna; Chandana Satya Prakash; Sridhar, Mukund; Triefenbach, Fabian; Verma, Apurv; Tur, Gokhan; Natarajan, Prem (2022). \"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model\". arXiv:2208.01448 [cs.CL].\\n\\n\\nExternal links[edit]\\nVoita, Lena. \"Sequence to Sequence (seq2seq) and Attention\". Retrieved 2023-12-20.\\n\"A ten-minute introduction to sequence-to-sequence learning in Keras\". blog.keras.io. Retrieved 2019-12-19.\\nvteNatural language processingGeneral terms\\nAI-complete\\nBag-of-words\\nn-gram\\nBigram\\nTrigram\\nComputational linguistics\\nNatural language understanding\\nStop words\\nText processing\\nText analysis\\nArgument mining\\nCollocation extraction\\nConcept mining\\nCoreference resolution\\nDeep linguistic processing\\nDistant reading\\nInformation extraction\\nNamed-entity recognition\\nOntology learning\\nParsing\\nSemantic parsing\\nSyntactic parsing\\nPart-of-speech tagging\\nSemantic analysis\\nSemantic role labeling\\nSemantic decomposition\\nSemantic similarity\\nSentiment analysis\\nTerminology extraction\\nText mining\\nTextual entailment\\nTruecasing\\nWord-sense disambiguation\\nWord-sense induction\\nText segmentation\\nCompound-term processing\\nLemmatisation\\nLexical analysis\\nText chunking\\nStemming\\nSentence segmentation\\nWord segmentation\\n\\nAutomatic summarization\\nMulti-document summarization\\nSentence extraction\\nText simplification\\nMachine translation\\nComputer-assisted\\nExample-based\\nRule-based\\nStatistical\\nTransfer-based\\nNeural\\nDistributional semantics models\\nBERT\\nDocument-term matrix\\nExplicit semantic analysis\\nfastText\\nGloVe\\nLanguage model (large)\\nLatent semantic analysis\\nSeq2seq\\nWord embedding\\nWord2vec\\nLanguage resources,datasets and corporaTypes andstandards\\nCorpus linguistics\\nLexical resource\\nLinguistic Linked Open Data\\nMachine-readable dictionary\\nParallel text\\nPropBank\\nSemantic network\\nSimple Knowledge Organization System\\nSpeech corpus\\nText corpus\\nThesaurus (information retrieval)\\nTreebank\\nUniversal Dependencies\\nData\\nBabelNet\\nBank of English\\nDBpedia\\nFrameNet\\nGoogle Ngram Viewer\\nUBY\\nWordNet\\nWikidata\\nAutomatic identificationand data capture\\nSpeech recognition\\nSpeech segmentation\\nSpeech synthesis\\nNatural language generation\\nOptical character recognition\\nTopic model\\nDocument classification\\nLatent Dirichlet allocation\\nPachinko allocation\\nComputer-assistedreviewing\\nAutomated essay scoring\\nConcordancer\\nGrammar checker\\nPredictive text\\nPronunciation assessment\\nSpell checker\\nNatural languageuser interface\\nChatbot\\nInteractive fiction\\nQuestion answering\\nVirtual assistant\\nVoice user interface\\nRelated\\nFormal semantics\\nHallucination\\nNatural Language Toolkit\\nspaCy\\n\\nvteArtificial intelligence (AI)History (timeline)Concepts\\nParameter\\nHyperparameter\\nLoss functions\\nRegression\\nBias–variance tradeoff\\nDouble descent\\nOverfitting\\nClustering\\nGradient descent\\nSGD\\nQuasi-Newton method\\nConjugate gradient method\\nBackpropagation\\nAttention\\nConvolution\\nNormalization\\nBatchnorm\\nActivation\\nSoftmax\\nSigmoid\\nRectifier\\nGating\\nWeight initialization\\nRegularization\\nDatasets\\nAugmentation\\nPrompt engineering\\nReinforcement learning\\nQ-learning\\nSARSA\\nImitation\\nPolicy gradient\\nDiffusion\\nLatent diffusion model\\nAutoregression\\nAdversary\\nRAG\\nUncanny valley\\nRLHF\\nSelf-supervised learning\\nReflection\\nRecursive self-improvement\\nHallucination\\nWord embedding\\nVibe coding\\nApplications\\nMachine learning\\nIn-context learning\\nArtificial neural network\\nDeep learning\\nLanguage model\\nLarge language model\\nNMT\\nReasoning language model\\nModel Context Protocol\\nIntelligent agent\\nArtificial human companion\\nHumanity\\'s Last Exam\\nArtificial general intelligence (AGI)\\nImplementationsAudio–visual\\nAlexNet\\nWaveNet\\nHuman image synthesis\\nHWR\\nOCR\\nSpeech synthesis\\n15.ai\\nElevenLabs\\nSpeech recognition\\nWhisper\\nFacial recognition\\nAlphaFold\\nText-to-image models\\nAurora\\nDALL-E\\nFirefly\\nFlux\\nIdeogram\\nImagen\\nMidjourney\\nStable Diffusion\\nText-to-video models\\nDream Machine\\nRunway Gen\\nHailuo AI\\nKling\\nSora\\nVeo\\nMusic generation\\nSuno AI\\nUdio\\nText\\nWord2vec\\nSeq2seq\\nGloVe\\nBERT\\nT5\\nLlama\\nChinchilla AI\\nPaLM\\nGPT\\n1\\n2\\n3\\nJ\\nChatGPT\\n4\\n4o\\no1\\no3\\n4.5\\n4.1\\no4-mini\\nClaude\\nGemini\\nchatbot\\nGrok\\nLaMDA\\nBLOOM\\nProject Debater\\nIBM Watson\\nIBM Watsonx\\nGranite\\nPanGu-Σ\\nDeepSeek\\nQwen\\nDecisional\\nAlphaGo\\nAlphaZero\\nOpenAI Five\\nSelf-driving car\\nMuZero\\nAction selection\\nAutoGPT\\nRobot control\\nPeople\\nAlan Turing\\nWarren Sturgis McCulloch\\nWalter Pitts\\nJohn von Neumann\\nClaude Shannon\\nMarvin Minsky\\nJohn McCarthy\\nNathaniel Rochester\\nAllen Newell\\nCliff Shaw\\nHerbert A. Simon\\nOliver Selfridge\\nFrank Rosenblatt\\nBernard Widrow\\nJoseph Weizenbaum\\nSeymour Papert\\nSeppo Linnainmaa\\nPaul Werbos\\nJürgen Schmidhuber\\nYann LeCun\\nGeoffrey Hinton\\nJohn Hopfield\\nYoshua Bengio\\nLotfi A. Zadeh\\nStephen Grossberg\\nAlex Graves\\nJames Goodnight\\nAndrew Ng\\nFei-Fei Li\\nIlya Sutskever\\nAlex Krizhevsky\\nIan Goodfellow\\nDemis Hassabis\\nDavid Silver\\nAndrej Karpathy\\nAshish Vaswani\\nNoam Shazeer\\nAidan Gomez\\nArchitectures\\nNeural Turing machine\\nDifferentiable neural computer\\nTransformer\\nVision transformer (ViT)\\nRecurrent neural network (RNN)\\nLong short-term memory (LSTM)\\nGated recurrent unit (GRU)\\nEcho state network\\nMultilayer perceptron (MLP)\\nConvolutional neural network (CNN)\\nResidual neural network (RNN)\\nHighway network\\nMamba\\nAutoencoder\\nVariational autoencoder (VAE)\\nGenerative adversarial network (GAN)\\nGraph neural network (GNN)\\n\\n Portals\\nTechnology\\n Category\\nArtificial neural networks\\nMachine learning\\n List\\nCompanies\\nProjects\\n\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Seq2seq&oldid=1296120849\"\\nCategories: Artificial neural networksNatural language processingHidden categories: Articles with short descriptionShort description matches Wikidata\\n\\n\\n\\n\\n\\n\\n This page was last edited on 17 June 2025, at 23:29\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License;\\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nSeq2seq\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n8 languages\\n\\n\\nAdd topic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "docs = [\n",
    "  'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need',\n",
    "  'https://en.wikipedia.org/wiki/Seq2seq'\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(doc).load() for doc in docs]\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7f07af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Attention_Is_All_You_Need', 'title': 'Attention Is All You Need - Wikipedia', 'language': 'en'}, page_content='\\n\\n\\n\\nAttention Is All You Need - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDonate\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\nDonate Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1\\nAuthors\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\nMethods discussed and introduced\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\nHistorical context\\n\\n\\n\\n\\nToggle Historical context subsection\\n\\n\\n\\n\\n\\n3.1\\nPredecessors\\n\\n\\n\\n\\n\\n\\n\\n\\n3.2\\nAttention with seq2seq\\n\\n\\n\\n\\n\\n\\n\\n\\n3.3\\nParallelizing attention\\n\\n\\n\\n\\n\\n\\n\\n\\n3.4\\nAI boom era\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\nTraining\\n\\n\\n\\n\\n\\n\\n\\n\\n5\\nNotes\\n\\n\\n\\n\\n\\n\\n\\n\\n6\\nReferences\\n\\n\\n\\n\\n\\n\\n\\n\\n7\\nExternal links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nAttention Is All You Need\\n\\n\\n\\n13 languages\\n\\n\\n\\n\\nالعربيةCatalàEspañolفارسی한국어עבריתမြန်မာဘာသာ日本語PortuguêsРусскийTürkçeУкраїнська中文\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\nWikidata item\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\n2017 research paper by Google\\n\\n\\nAn illustration of main components of the transformer model from the paper\\n\"Attention Is All You Need\"[1] is a 2017 landmark[2][3] research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al.[4] It is considered a foundational[5] paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide variety of AI, such as large language models.[6][7] At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as multimodal generative AI.[1]\\nThe paper\\'s title is a reference to the song \"All You Need Is Love\" by the Beatles.[8] The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper\\'s authors, liked the sound of that word.[9]\\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers franchise. The team was named Team Transformer.[8]\\nSome early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.[9]\\nAs of 2025,[update] the paper has been cited more than 173,000 times,[10] placing it among top ten most-cited papers of the 21st century.[11]\\n\\n\\nAuthors[edit]\\nThe authors of the paper are: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones\\xa0[wikidata], Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. All eight authors were \"equal contributors\" to the paper; the listed order was randomized. The Wired article highlights the group\\'s diversity:[8]\\nSix of the eight authors were born outside the United States; the other two are children of two green-card-carrying Germans who were temporarily in California and a first-generation American whose family had fled persecution, respectively.\\n\\nAfter the paper, each of the authors left Google to join other companies or to found startups. Several of them expressed feelings of being unable to innovate and expand the Transformer in a direction they want, if they had stayed at Google.[12]\\nMethods discussed and introduced[edit]\\nThe paper is most well known for the introduction of the Transformer architecture, which forms the underlying architecture for most forms of modern Large Language Models (LLMs). A key reason for why the architecture is preferred by most modern LLMs is the parallelizability of the architecture over its predecessors. This ensures that the operations necessary for training can be accelerated on a GPU allowing both faster training times and models of bigger sizes to be trained.\\nThe following mechanisms were introduced by the paper as part of the development of the transformer architecture.\\nScaled dot-product attention & self-attention\\nThe use of the scaled dot-product attention and self-attention mechanism instead of a Recurrent neural network or Long short-term memory (which rely on recurrence instead) allow for better performance as described in the following paragraph. The paper described the scaled dot-product attention as follows:\\n\\n\\n\\n\\n\\n\\nA\\nt\\nt\\ne\\nn\\nt\\ni\\no\\nn\\n\\n\\n(\\nQ\\n,\\nK\\n,\\nV\\n)\\n:=\\n\\n\\ns\\no\\nf\\nt\\nm\\na\\nx\\n\\n\\n\\n(\\n\\n\\n\\nQ\\n×\\n\\nK\\n\\nT\\n\\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n\\n)\\n\\n×\\nV\\n\\n\\n{\\\\displaystyle {\\\\rm {Attention}}(Q,K,V):={\\\\rm {softmax}}\\\\left({\\\\frac {Q\\\\times K^{T}}{\\\\sqrt {d_{k}}}}\\\\right)\\\\times V}\\n\\n\\nwhere \\n\\n\\n\\nQ\\n\\n\\n{\\\\displaystyle Q}\\n\\n, \\n\\n\\n\\nK\\n\\n\\n{\\\\displaystyle K}\\n\\n, \\n\\n\\n\\nV\\n\\n\\n{\\\\displaystyle V}\\n\\n are respectively the query, key, value matrices, and  \\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n{\\\\displaystyle d_{k}}\\n\\n is the dimension of the values.\\nSince the model relies on Query (Q), Key (K) and Value (V) matrices that come from the same source itself (i.e. the input sequence / context window), this eliminates the need for RNNs completely ensuring parallelizability for the architecture. This differs from the original form of the Attention mechanism introduced in 2014. Additionally, the paper also discusses the use of an additional scaling factor that was found to be most effective with respect to the dimension of the key vectors (represented as \\n\\n\\n\\n\\nd\\n\\nk\\n\\n\\n\\n\\n{\\\\displaystyle d_{k}}\\n\\n and initially set to 64 within the paper) in the manner shown above.\\nIn the specific context of translation which the paper focused on, the Query and Key matrices are usually represented in embeddings corresponding to the source language while the Value matrix corresponds to the target language.\\nMulti-head attention\\nIn the self-attention mechanism, queries (Q), keys (K), and values (V) are dynamically generated for each input sequence (limited typically by the size of the context window), allowing the model to focus on different parts of the input sequence at different steps. Multi-head attention enhances this process by introducing multiple parallel attention heads. Each attention head learns different linear projections of the Q, K, and V matrices. This allows the model to capture different aspects of the relationships between words in the sequence simultaneously, rather than focusing on a single aspect.\\nBy doing this, multi-head attention ensures that the input embeddings are updated from a more varied and diverse set of perspectives. After the attention outputs from all heads are calculated, they are concatenated and passed through a final linear transformation to generate the output.\\nPositional encoding\\nSince the Transformer model is not a seq2seq model and does not rely on the sequence of the text in order to perform encoding and decoding, the paper relied on the use of sine and cosine wave functions to encode the position of the token into the embedding. The methods introduced in the paper are discussed below:\\n\\n\\n\\n\\nP\\n\\nE\\n\\n(\\n\\n\\np\\no\\ns\\n\\n\\n,\\n2\\ni\\n)\\n\\n\\n=\\nsin\\n\\u2061\\n(\\n\\n\\np\\no\\ns\\n\\n\\n\\n/\\n\\n\\n\\n10000\\n\\n\\n2\\ni\\n\\n/\\n\\n\\nd\\n\\n\\nm\\no\\nd\\ne\\nl\\n\\n\\n\\n\\n\\n)\\n\\n\\n{\\\\displaystyle PE_{({\\\\rm {pos}},2i)}=\\\\sin({\\\\rm {pos}}/{10000}^{2i/d_{\\\\rm {model}}})}\\n\\n\\n\\n\\n\\n\\nP\\n\\nE\\n\\n(\\n\\n\\np\\no\\ns\\n\\n\\n,\\n2\\ni\\n+\\n1\\n)\\n\\n\\n=\\ncos\\n\\u2061\\n(\\n\\n\\np\\no\\ns\\n\\n\\n\\n/\\n\\n\\n\\n10000\\n\\n\\n2\\ni\\n\\n/\\n\\n\\nd\\n\\n\\nm\\no\\nd\\ne\\nl\\n\\n\\n\\n\\n\\n)\\n\\n\\n{\\\\displaystyle PE_{({\\\\rm {pos}},2i+1)}=\\\\cos({\\\\rm {pos}}/{10000}^{2i/d_{\\\\rm {model}}})}\\n\\n\\nwherein \\n\\n\\n\\n\\n\\np\\no\\ns\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\rm {pos}}}\\n\\n, \\n\\n\\n\\ni\\n\\n\\n{\\\\displaystyle i}\\n\\n, \\n\\n\\n\\n\\n\\nd\\n\\n\\nm\\no\\nd\\ne\\nl\\n\\n\\n\\n\\n\\n\\n{\\\\displaystyle {d_{\\\\rm {model}}}}\\n\\n correspond to the position of the word, the current dimension index and the dimension of the model respectively. The sine function is used for even indices of the embedding while the cosine function is used for odd indices. The resultant \\n\\n\\n\\nP\\nE\\n\\n\\n{\\\\displaystyle PE}\\n\\n embedding is then added to the word at that corresponding position with respect to the current context window. The paper specifically comments on why this method was chosen describing:\\n\"We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.\"[1]\\n\\nHistorical context[edit]\\nMain articles: Transformer (deep learning architecture) §\\xa0History, and Seq2seq §\\xa0History\\nSee also: Timeline of machine learning\\n Predecessors[edit]\\nFor many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model\\'s state at the end of a long sentence without precise, extractable information about preceding tokens.\\nA key breakthrough was LSTM (1995),[note 1] a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units.[13] Neural networks using multiplicative units were later called sigma-pi networks[14] or higher-order networks.[15] LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\\nHowever, LSTM still used sequential processing, like most other RNNs.[note 2] Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence.\\nModern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window. The linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input.[16] One of its two networks has \"fast weights\" or \"dynamic links\" (1981).[17][18][19] A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries.[16] This was later shown to be equivalent to the unnormalized linear Transformer.[20][21]\\n\\n Attention with seq2seq[edit]\\nMain article: Seq2seq §\\xa0History\\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.[22][23]\\nA 380M-parameter model for machine translation uses two long short-term memories (LSTM).[23] Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM.[22] Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.[24][25]\\nThese early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.[26]\\nThe RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\".[4]\\nThe relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.[27]\\nIn 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM.[28] It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.[29]\\n\\n Parallelizing attention[edit]\\nMain article: Attention (machine learning) §\\xa0History\\nSeq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude fewer parameters than LSTMs.[30] One of its authors, Jakob Uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title \"attention is all you need\".[31] That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical.[31] In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.[32]\\nIn 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance.[1] This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.[33]\\n\\n\\n AI boom era[edit]\\nAlready in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles.[34] Transformer architecture is now used alongside many generative models that contribute to the ongoing AI boom.\\nIn language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model.[35] In 2019 October, Google started using BERT to process search queries.[36] In 2020, Google Translate replaced the previous RNN-encoder–RNN-decoder model by a Transformer-encoder–RNN-decoder model.[37]\\nStarting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly[38] popular, triggering a boom around large language models.[39][40]\\nSince 2020, Transformers have been applied in modalities beyond text, including the vision transformer,[41] speech recognition,[42] robotics,[43] and multimodal.[44] The vision transformer, in turn, stimulated new developments in convolutional neural networks.[45] Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024),[46] and Sora (2024), use Transformers to analyse input data (like text prompts) by breaking it down into \"tokens\" and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data.\\n\\nTraining[edit]\\nWhile the primary focus of the paper at the time was to improve machine translation, the paper also discussed the use of the architecture on English Constituency Parsing, both with limited and large-sized datasets, achieving a high-score without specific tuning for the task indicating the promising nature of the model for use in a wide-variety of general purpose of seq2seq tasks.\\nDataset\\nThe English-to-German translation model was trained on the 2014 WMT (Workshop on Statistical Machine Translation) English-German dataset, consisting of nearly 4.5 million sentences derived from TED Talks and high-quality news articles. A separate translation model was trained on the much larger 2014 WMT English-French dataset, consisting of 36 million sentences. Both datasets were encoded with byte-pair encoding.\\nHardware\\nThe models were trained using 8 NVIDIA P100 GPUs. The base models were trained for 100,000 steps and the big models were trained for 300,000 steps - each step taking about 0.4 seconds to complete for the base models and 1.0 seconds for the big models. The base model trained for a total of 12 hours, and the big model trained for a total of 3.5 days. Both the base and big models outperforms the 2017 state-of-the-art in both English-German and English-French while achieving the comparatively lowest training cost.[1] The estimated computing cost was 0.089 petaFLOP-days.[47]\\nHyperparameters and regularization\\nFor their 100M-parameter Transformer model, the authors increased the learning rate linearly for the first 4000 (warmup) steps and decreased it proportionally to inverse square root of the current step number. Dropout layers were applied to the output of each sub-layer before normalization, the sums of the embeddings, and the positional encodings. The dropout rate was set to 0.1. Label smoothing was applied with a value of 0.1 which \"improves accuracy and BLEU score\".[1]\\n\\nNotes[edit]\\n\\n\\n^ Gated recurrent units (2014) further reduced its complexity.\\n\\n^ Some architectures, such as RWKV or state space models, avoid the issue.\\n\\n\\nReferences[edit]\\n\\n\\n^ a b c d e f Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (December 2017). \"Attention is All you Need\". In I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett (ed.). 31st Conference on  Neural Information Processing Systems (NIPS). Advances in Neural Information Processing Systems. Vol.\\xa030. Curran Associates, Inc. arXiv:1706.03762.\\n\\n^ Love, Julia (10 July 2023). \"AI Researcher Who Helped Write Landmark Paper Is Leaving Google\". Bloomberg News. Retrieved 1 April 2024.\\n\\n^ Goldman, Sharon (20 March 2024). \"\\'Attention is All You Need\\' creators look beyond Transformers for AI at Nvidia GTC: \\'The world needs something better\\'\". VentureBeat. Retrieved 1 April 2024.\\n\\n^ a b Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (19 May 2016). \"Neural Machine Translation by Jointly Learning to Align and Translate\". arXiv:1409.0473 [cs.CL].\\n\\n^ Shinde, Gitanjali; Wasatkar, Namrata; Mahalle, Parikshit (6 June 2024). Data-Centric Artificial Intelligence for Multidisciplinary Applications. CRC Press. p.\\xa075. ISBN\\xa09781040031131.\\n\\n^ Toews, Rob (3 September 2023). \"Transformers Revolutionized AI. What Will Replace Them?\". Forbes. Archived from the original on 26 September 2023. Retrieved 3 December 2023.\\n\\n^ Murgia, Madhumita (23 July 2023). \"Transformers: the Google scientists who pioneered an AI revolution\". Financial Times. Archived from the original on 28 December 2023. Retrieved 22 March 2024.\\n\\n^ a b c Levy, Steven. \"8 Google Employees Invented Modern AI. Here\\'s the Inside Story\". Wired. ISSN\\xa01059-1028. Retrieved 20 March 2024.\\n\\n^ a b Marche, Stephen (23 August 2024). \"Was Linguistic A.I. Created by Accident?\". The New Yorker. ISSN\\xa00028-792X. Retrieved 24 August 2024.\\n\\n^ \"Meet the $4 Billion AI Superstars That Google Lost\". Bloomberg. 13 July 2023 – via www.bloomberg.com.\\n\\n^ \"Exclusive: the most-cited papers of the twenty-first century\". Nature. 15 April 2025. Retrieved 18 April 2025.\\n\\n^ Murgia, Madhumita (23 July 2023). \"Transformers: the Google scientists who pioneered an AI revolution\". Financial Times. Retrieved 22 March 2025.\\n\\n^ Feldman, J. A.; Ballard, D. H. (1 July 1982). \"Connectionist models and their properties\". Cognitive Science. 6 (3): 205–254. doi:10.1016/S0364-0213(82)80001-3. ISSN\\xa00364-0213.\\n\\n^ Rumelhart, David E.; McClelland, James L.; Hinton, Geoffrey E. (29 July 1987). Parallel Distributed Processing, Volume 1: Explorations in the Microstructure of Cognition: Foundations, Chapter 2 (PDF). Cambridge, Mass: Bradford Books. ISBN\\xa0978-0-262-68053-0.\\n\\n^ Giles, C. Lee; Maxwell, Tom (1 December 1987). \"Learning, invariance, and generalization in high-order neural networks\". Applied Optics. 26 (23): 4972–4978. doi:10.1364/AO.26.004972. ISSN\\xa00003-6935. PMID\\xa020523475.\\n\\n^ a b Schmidhuber, Jürgen (1992). \"Learning to control fast-weight memories: an alternative to recurrent nets\" (PDF). Neural Computation. 4 (1): 131–139. doi:10.1162/neco.1992.4.1.131. S2CID\\xa016683347.\\n\\n^ Christoph von der Malsburg: The correlation theory of brain function. Internal Report 81-2, MPI Biophysical Chemistry, 1981. http://cogprints.org/1380/1/vdM_correlation.pdf See Reprint in Models of Neural Networks II, chapter 2, pages 95–119. Springer, Berlin, 1994.\\n\\n^ Jerome A. Feldman, \"Dynamic connections in neural networks,\" Biological Cybernetics, vol. 46, no. 1, pp. 27–39, Dec. 1982.\\n\\n^ Hinton, Geoffrey E.; Plaut, David C. (1987). \"Using Fast Weights to Deblur Old Memories\". Proceedings of the Annual Meeting of the Cognitive Science Society. 9.\\n\\n^ Katharopoulos, Angelos; Vyas, Apoorv; Pappas, Nikolaos; Fleuret, François (2020). \"Transformers are RNNs: Fast autoregressive Transformers with linear attention\". ICML 2020. PMLR. pp.\\xa05156–5165.\\n\\n^ Schlag, Imanol; Irie, Kazuki; Schmidhuber, Jürgen (2021). \"Linear Transformers Are Secretly Fast Weight Programmers\". ICML 2021. Springer. pp.\\xa09355–9366.\\n\\n^ a b Cho, Kyunghyun; van Merriënboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (October 2014). \"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation\". In Moschitti, Alessandro; Pang, Bo; Daelemans, Walter (eds.). Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar: Association for Computational Linguistics. pp.\\xa01724–1734. arXiv:1406.1078. doi:10.3115/v1/D14-1179.\\n\\n^ a b Sutskever, Ilya; Vinyals, Oriol; Le, Quoc Viet (14 December 2014). \"Sequence to sequence learning with neural networks\". arXiv:1409.3215 [cs.CL]. [first version posted to arXiv on 10 Sep 2014]\\n\\n^ Chung, Junyoung; Gulcehre, Caglar; Cho, KyungHyun; Bengio, Yoshua (2014). \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\". arXiv:1412.3555 [cs.NE].\\n\\n^ Gruber, N.; Jockisch, A. (2020), \"Are GRU cells more specific and LSTM cells more sensitive in motive classification of text?\", Frontiers in Artificial Intelligence, 3: 40, doi:10.3389/frai.2020.00040, PMC\\xa07861254, PMID\\xa033733157, S2CID\\xa0220252321\\n\\n^ Sutskever, Ilya; Vinyals, Oriol; Le, Quoc V (2014). \"Sequence to Sequence Learning with Neural Networks\". Advances in Neural Information Processing Systems. 27. Curran Associates, Inc. arXiv:1409.3215.\\n\\n^ Luong, Minh-Thang; Pham, Hieu; Manning, Christopher D. (2015). \"Effective Approaches to Attention-based Neural Machine Translation\". arXiv:1508.04025 [cs.CL].\\n\\n^ Wu, Yonghui; et\\xa0al. (1 September 2016). \"Google\\'s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\". arXiv:1609.08144 [cs.CL].\\n\\n^ Lewis-Kraus, Gideon (14 December 2016). \"The Great A.I. Awakening\". The New York Times. ISSN\\xa00362-4331. Archived from the original on 24 May 2023. Retrieved 22 June 2023.\\n\\n^ Parikh, Ankur P.; Täckström, Oscar; Das, Dipanjan; Uszkoreit, Jakob (25 September 2016). \"A Decomposable Attention Model for Natural Language Inference\". arXiv:1606.01933 [cs.CL].\\n\\n^ a b Levy, Steven. \"8 Google Employees Invented Modern AI. Here\\'s the Inside Story\". Wired. ISSN\\xa01059-1028. Archived from the original on 20 March 2024. Retrieved 6 August 2024.\\n\\n^ Cheng, Jianpeng; Dong, Li; Lapata, Mirella (November 2016). \"Long Short-Term Memory-Networks for Machine Reading\". In Su, Jian; Duh, Kevin; Carreras, Xavier (eds.). Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Austin, Texas: Association for Computational Linguistics. pp.\\xa0551–561. doi:10.18653/v1/D16-1053.\\n\\n^ Peng, Bo; Alcaide, Eric; Anthony, Quentin; Albalak, Alon; Arcadinho, Samuel; Biderman, Stella; Cao, Huanqi; Cheng, Xin; Chung, Michael (10 December 2023), RWKV: Reinventing RNNs for the Transformer Era, arXiv:2305.13048\\n\\n^ Marche, Stephen (23 August 2024). \"Was Linguistic A.I. Created by Accident?\". The New Yorker. ISSN\\xa00028-792X. Retrieved 27 August 2024.\\n\\n^ Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (11 October 2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\". arXiv:1810.04805v2 [cs.CL].\\n\\n^ \"Google: BERT now used on almost every English query\". Search Engine Land. 15 October 2020. Retrieved 24 November 2020.\\n\\n^ \"Recent Advances in Google Translate\". research.google. Retrieved 8 May 2024.\\n\\n^ \"The inside story of how ChatGPT was built from the people who made it\". MIT Technology Review. Retrieved 6 August 2024.\\n\\n^ \"Improving language understanding with unsupervised learning\". openai.com. 11 June 2018. Archived from the original on 18 March 2023. Retrieved 18 March 2023.\\n\\n^ finetune-transformer-lm, OpenAI, 11 June 2018, retrieved 1 May 2023\\n\\n^ Dosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob (3 June 2021). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\". arXiv:2010.11929 [cs.CV].\\n\\n^ Gulati, Anmol; Qin, James; Chiu, Chung-Cheng; Parmar, Niki; Zhang, Yu; Yu, Jiahui; Han, Wei; Wang, Shibo; Zhang, Zhengdong; Wu, Yonghui; Pang, Ruoming (2020). \"Conformer: Convolution-augmented Transformer for Speech Recognition\". arXiv:2005.08100 [eess.AS].\\n\\n^ Chen, Lili; Lu, Kevin; Rajeswaran, Aravind; Lee, Kimin; Grover, Aditya; Laskin, Michael; Abbeel, Pieter; Srinivas, Aravind; Mordatch, Igor (24 June 2021), Decision Transformer: Reinforcement Learning via Sequence Modeling, arXiv:2106.01345\\n\\n^ Choromanski, Krzysztof; Likhosherstov, Valerii; Dohan, David; Song, Xingyou; Gane, Andreea; Sarlos, Tamas; Hawkins, Peter; Davis, Jared; Mohiuddin, Afroz (19 November 2022), Rethinking Attention with Performers, arXiv:2009.14794\\n\\n^ Liu, Zhuang; Mao, Hanzi; Wu, Chao-Yuan; Feichtenhofer, Christoph; Darrell, Trevor; Xie, Saining (2022). A ConvNet for the 2020s. Conference on Computer Vision and Pattern Recognition. pp.\\xa011976–11986.\\n\\n^ Esser, Patrick; Kulal, Sumith; Blattmann, Andreas; Entezari, Rahim; Müller, Jonas; Saini, Harry; Levi, Yam; Lorenz, Dominik; Sauer, Axel (5 March 2024), Scaling Rectified Flow Transformers for High-Resolution Image Synthesis, arXiv:2403.03206\\n\\n^ \"AI and compute\". openai.com. 9 June 2022. Retrieved 29 April 2025.\\n\\n\\nExternal links[edit]\\nUszkoreit, Jakob (31 August 2017). \"Transformer: A Novel Neural Network Architecture for Language Understanding\". research.google. Retrieved 9 August 2024. A concurrent blog post on Google Research blog.\\nvteGoogle AI\\nGoogle\\nGoogle Brain\\nGoogle DeepMind\\nComputer programsAlphaGoVersions\\nAlphaGo (2015)\\nMaster (2016)\\nAlphaGo Zero (2017)\\nAlphaZero (2017)\\nMuZero (2019)\\nCompetitions\\nFan Hui (2015)\\nLee Sedol (2016)\\nKe Jie (2017)\\nIn popular culture\\nAlphaGo (2017)\\nThe MANIAC (2023)\\nOther\\nAlphaFold (2018)\\nAlphaStar (2019)\\nAlphaDev (2023)\\nAlphaGeometry (2024)\\nMachine learningNeural networks\\nInception (2014)\\nWaveNet (2016)\\nMobileNet (2017)\\nTransformer (2017)\\nEfficientNet (2019)\\nGato (2022)\\nOther\\nQuantum Artificial Intelligence Lab\\nTensorFlow\\nTensor Processing Unit\\nGenerative AIChatbots\\nAssistant (2016)\\nSparrow (2022)\\nGemini (2023)\\nModels\\nBERT (2018)\\nXLNet (2019)\\nT5 (2019)\\nLaMDA (2021)\\nChinchilla (2022)\\nPaLM (2022)\\nImagen (2023)\\nGemini (2023)\\nVideoPoet (2024)\\nVeo (text-to-video model) (2024)\\nOther\\nDreamBooth (2022)\\nNotebookLM (2023)\\nVids (2024)\\nGemini Robotics (2025)\\nSee also\\n\"Attention Is All You Need\"\\nFuture of Go Summit\\nGenerative pre-trained transformer\\nGoogle Labs\\nGoogle Pixel\\nGoogle Workspace\\nRobot Constitution\\n\\n Category\\n Commons\\n\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Attention_Is_All_You_Need&oldid=1296709299\"\\nCategories: 2017 documentsArtificial intelligence papersGoogle2017 in artificial intelligenceHidden categories: Articles with short descriptionShort description is different from WikidataUse dmy dates from December 2023Articles containing potentially dated statements from 2025All articles containing potentially dated statements\\n\\n\\n\\n\\n\\n\\n This page was last edited on 21 June 2025, at 19:00\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License;\\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nAttention Is All You Need\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n13 languages\\n\\n\\nAdd topic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'),\n",
       " Document(metadata={'source': 'https://en.wikipedia.org/wiki/Seq2seq', 'title': 'Seq2seq - Wikipedia', 'language': 'en'}, page_content='\\n\\n\\n\\nSeq2seq - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDonate\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\nDonate Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1\\nHistory\\n\\n\\n\\n\\nToggle History subsection\\n\\n\\n\\n\\n\\n1.1\\nPriority dispute\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\nArchitecture\\n\\n\\n\\n\\nToggle Architecture subsection\\n\\n\\n\\n\\n\\n2.1\\nEncoder\\n\\n\\n\\n\\n\\n\\n\\n\\n2.2\\nDecoder\\n\\n\\n\\n\\n\\n\\n\\n\\n2.3\\nTraining vs prediction\\n\\n\\n\\n\\n\\n\\n\\n\\n2.4\\nAttention for seq2seq\\n\\n\\n\\n\\n\\n\\n\\n\\n2.5\\nAttention weights\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\nOther applications\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\nSee also\\n\\n\\n\\n\\n\\n\\n\\n\\n5\\nReferences\\n\\n\\n\\n\\n\\n\\n\\n\\n6\\nExternal links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nSeq2seq\\n\\n\\n\\n8 languages\\n\\n\\n\\n\\nالعربيةCatalàفارسی한국어日本語ไทยTiếng Việt中文\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadEditView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\nWikidata item\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\nFamily of machine learning approaches\\nAnimation of seq2seq with RNN and attention mechanism\\nSeq2seq is a family of machine learning approaches used for natural language processing.[1] Applications include language translation,[2] image captioning,[3] conversational models,[4] speech recognition,[5] and text summarization.[6]\\nSeq2seq uses sequence transformation: it turns one sequence into another sequence.\\n\\n\\nHistory[edit]\\nOne naturally wonders if the problem of translation could conceivably be treated as a problem in cryptography. When I look at an article in Russian, I say: \\'This is really written in English, but it has been coded in some strange symbols. I will now proceed to decode.—\\u200aWarren Weaver, Letter to Norbert Wiener, March 4, 1947\\nShannon\\'s diagram of a general communications system, showing the process by which a message sent becomes the message received (possibly corrupted by noise)\\nseq2seq is an approach to machine translation (or more generally, sequence transduction) with roots in information theory, where communication is understood as an encode-transmit-decode process, and machine translation can be studied as a special case of communication. This viewpoint was elaborated, for example, in the noisy channel model of machine translation.\\nIn practice, seq2seq maps an input sequence into a real-numerical vector by using a neural network (the encoder), and then maps it back to an output sequence using another neural network (the decoder).\\nThe idea of encoder-decoder sequence transduction had been developed in the early 2010s (see [2][1] for previous papers). The papers most commonly cited as the originators that produced seq2seq are two papers from 2014.[2][1]\\nIn the seq2seq as proposed by them, both the encoder and the decoder were LSTMs. This had the \"bottleneck\" problem, since the encoding vector has a fixed size, so for long input sequences, information would tend to be lost, as they are difficult to fit into the fixed-length encoding vector. The attention mechanism, proposed in 2014,[7] resolved the bottleneck problem. They called their model RNNsearch, as it \"emulates searching through a source sentence during decoding a translation\".\\nA problem with seq2seq models at this point was that recurrent neural networks are difficult to parallelize. The 2017 publication of Transformers[8] resolved the problem by replacing the encoding RNN with self-attention Transformer blocks (\"encoder blocks\"), and the decoding RNN with cross-attention causally-masked Transformer blocks (\"decoder blocks\").\\n\\nPriority dispute[edit]\\nOne of the papers cited as the originator for seq2seq is (Sutskever et al 2014),[1] published at Google Brain while they were on Google\\'s machine translation project. The research allowed Google to overhaul Google Translate into Google Neural Machine Translation in 2016.[1][9] Tomáš Mikolov claims to have developed the idea (before joining Google Brain) of using a \"neural language model on pairs of sentences... and then [generating] translation after seeing the first sentence\"—which he equates with seq2seq machine translation, and to have mentioned the idea to Ilya Sutskever and Quoc Le (while at Google Brain), who failed to acknowledge him in their paper.[10] Mikolov had worked on RNNLM (using RNN for language modelling) for his PhD thesis,[11] and is more notable for developing word2vec.\\n\\nArchitecture[edit]\\nThe main reference for this section is.[12]\\n\\nEncoder[edit]\\nRNN encoder\\nThe encoder is responsible for processing the input sequence and capturing its essential information, which is stored as the hidden state of the network and, in a model with attention mechanism, a context vector. The context vector is the weighted sum of the input hidden states and is generated for every time instance in the output sequences.\\n\\nDecoder[edit]\\nRNN decoder\\nThe decoder takes the context vector and hidden states from the encoder and generates the final output sequence. The decoder operates in an autoregressive manner, producing one element of the output sequence at a time. At each step, it considers the previously generated elements, the context vector, and the input sequence information to make predictions for the next element in the output sequence. Specifically, in a model with attention mechanism, the context vector and the hidden state are concatenated together to form an attention hidden vector, which is used as an input for the decoder.\\nThe seq2seq method developed in the early 2010s uses two neural networks:  an encoder network converts an input sentence into numerical vectors, and a decoder network converts those vectors to sentences in the target language.  The Attention mechanism was grafted onto this structure in 2014 and shown below. Later it was refined into the encoder-decoder Transformer architecture of 2017.\\n\\nTraining vs prediction[edit]\\nTraining a seq2seq model via teacher forcing.\\nPredicting a sequence using a seq2seq model.\\nThere is a subtle difference between training and prediction. During training time, both the input and the output sequences are known. During prediction time, only the input sequence is known, and the output sequence must be decoded by the network itself.\\nSpecifically, consider an input sequence \\n\\n\\n\\n\\nx\\n\\n1\\n:\\nn\\n\\n\\n\\n\\n{\\\\displaystyle x_{1:n}}\\n\\n and output sequence \\n\\n\\n\\n\\ny\\n\\n1\\n:\\nm\\n\\n\\n\\n\\n{\\\\displaystyle y_{1:m}}\\n\\n. The encoder would process the input \\n\\n\\n\\n\\nx\\n\\n1\\n:\\nn\\n\\n\\n\\n\\n{\\\\displaystyle x_{1:n}}\\n\\n step by step. After that, the decoder would take the output from the encoder, as well as the <bos> as input, and produce a prediction \\n\\n\\n\\n\\n\\n\\n\\ny\\n^\\n\\n\\n\\n\\n1\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\hat {y}}_{1}}\\n\\n. Now, the question is: what should be input to the decoder in the next step?\\nA standard method for training is \"teacher forcing\". In teacher forcing, no matter what is output by the decoder, the next input to the decoder is always the reference. That is, even if \\n\\n\\n\\n\\n\\n\\n\\ny\\n^\\n\\n\\n\\n\\n1\\n\\n\\n≠\\n\\ny\\n\\n1\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\hat {y}}_{1}\\\\neq y_{1}}\\n\\n, the next input to the decoder is still \\n\\n\\n\\n\\ny\\n\\n1\\n\\n\\n\\n\\n{\\\\displaystyle y_{1}}\\n\\n, and so on.\\nDuring prediction time, the \"teacher\" \\n\\n\\n\\n\\ny\\n\\n1\\n:\\nm\\n\\n\\n\\n\\n{\\\\displaystyle y_{1:m}}\\n\\n would be unavailable. Therefore, the input to the decoder must be \\n\\n\\n\\n\\n\\n\\n\\ny\\n^\\n\\n\\n\\n\\n1\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\hat {y}}_{1}}\\n\\n, then \\n\\n\\n\\n\\n\\n\\n\\ny\\n^\\n\\n\\n\\n\\n2\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\hat {y}}_{2}}\\n\\n, and so on.\\nIt is found that if a model is trained purely by teacher forcing, its performance would degrade during prediction time, since generation based on the model\\'s own output is different from generation based on the teacher\\'s output. This is called exposure bias or a train/test distribution shift. A 2015 paper recommends that, during training, randomly switch between teacher forcing and no teacher forcing.[13]\\n\\nAttention for seq2seq[edit]\\nThe attention mechanism is an enhancement introduced by Bahdanau et al. in 2014  to address limitations in the basic Seq2Seq architecture where a longer input sequence results in the hidden state output of the encoder becoming irrelevant for the decoder. It enables the model to selectively focus on different parts of the input sequence during the decoding process. At each decoder step, an alignment model calculates the attention score using the current decoder state and all of the attention hidden vectors as input. An alignment model is another neural network model that is trained jointly with the seq2seq model used to calculate how well an input, represented by the hidden state, matches with the previous output, represented by attention hidden state. A softmax function is then applied to the attention score to get the attention weight.Seq2seq RNN encoder-decoder with attention mechanism, where the detailed construction of attention mechanism is exposed. See attention mechanism page for details.\\nIn some models, the encoder states are directly fed into an activation function, removing the need for alignment model. An activation function receives one decoder state and one encoder state and returns a scalar value of their relevance.\\n\\nAnimation of seq2seq with RNN and attention mechanism\\nConsider the seq2seq language English-to-French translation task. To be concrete, let us consider the translation of \"the zone of international control <end>\", which should translate to \"la zone de contrôle international <end>\". Here, we use the special <end> token as a control character to delimit the end of input for both the encoder and the decoder.\\nAn input sequence of text \\n\\n\\n\\n\\nx\\n\\n0\\n\\n\\n,\\n\\nx\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle x_{0},x_{1},\\\\dots }\\n\\n is processed by a neural network (which can be an LSTM, a Transformer encoder, or some other network) into a sequence of real-valued vectors \\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle h_{0},h_{1},\\\\dots }\\n\\n, where \\n\\n\\n\\nh\\n\\n\\n{\\\\displaystyle h}\\n\\n stands for \"hidden vector\".\\nAfter the encoder has finished processing, the decoder starts operating over the hidden vectors, to produce an output sequence \\n\\n\\n\\n\\ny\\n\\n0\\n\\n\\n,\\n\\ny\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle y_{0},y_{1},\\\\dots }\\n\\n, autoregressively. That is, it always takes as input both the hidden vectors produced by the encoder, and what the decoder itself has produced before, to produce the next output word:\\n\\n(\\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle h_{0},h_{1},\\\\dots }\\n\\n, \"<start>\") → \"la\"\\n(\\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle h_{0},h_{1},\\\\dots }\\n\\n, \"<start> la\") → \"la zone\"\\n(\\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle h_{0},h_{1},\\\\dots }\\n\\n, \"<start> la zone\") → \"la zone de\"\\n...\\n(\\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle h_{0},h_{1},\\\\dots }\\n\\n, \"<start> la zone de contrôle international\") → \"la zone de contrôle international <end>\"\\nHere, we use the special <start> token as a control character to delimit the start of input for the decoder. The decoding terminates as soon as \"<end>\" appears in the decoder output.\\n\\nAttention weights[edit]\\nAttention mechanism with attention weights, overview.\\nAs hand-crafting weights defeats the purpose of machine learning, the model must compute the attention weights on its own. Taking analogy from the language of database queries, we make the model construct a triple of vectors: key, query, and value. The rough idea is that we have a \"database\" in the form of a list of key-value pairs. The decoder sends in a query, and obtains a reply in the form of a weighted sum of the values, where the weight is proportional to how closely the query resembles each key.\\nThe decoder first processes the \"<start>\" input partially, to obtain an intermediate vector \\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\nd\\n\\n\\n\\n\\n{\\\\displaystyle h_{0}^{d}}\\n\\n, the 0th hidden vector of decoder. Then, the intermediate vector is transformed by a linear map \\n\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n\\n\\n{\\\\displaystyle W^{Q}}\\n\\n into a query vector \\n\\n\\n\\n\\nq\\n\\n0\\n\\n\\n=\\n\\nh\\n\\n0\\n\\n\\nd\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n\\n\\n{\\\\displaystyle q_{0}=h_{0}^{d}W^{Q}}\\n\\n. Meanwhile, the hidden vectors outputted by the encoder are transformed by another linear map \\n\\n\\n\\n\\nW\\n\\nK\\n\\n\\n\\n\\n{\\\\displaystyle W^{K}}\\n\\n into key vectors \\n\\n\\n\\n\\nk\\n\\n0\\n\\n\\n=\\n\\nh\\n\\n0\\n\\n\\n\\nW\\n\\nK\\n\\n\\n,\\n\\nk\\n\\n1\\n\\n\\n=\\n\\nh\\n\\n1\\n\\n\\n\\nW\\n\\nK\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle k_{0}=h_{0}W^{K},k_{1}=h_{1}W^{K},\\\\dots }\\n\\n. The linear maps are useful for providing the model with enough freedom to find the best way to represent the data.\\nNow, the query and keys are compared by taking dot products: \\n\\n\\n\\n\\nq\\n\\n0\\n\\n\\n\\nk\\n\\n0\\n\\n\\nT\\n\\n\\n,\\n\\nq\\n\\n0\\n\\n\\n\\nk\\n\\n1\\n\\n\\nT\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle q_{0}k_{0}^{T},q_{0}k_{1}^{T},\\\\dots }\\n\\n. Ideally, the model should have learned to compute the keys and values, such that \\n\\n\\n\\n\\nq\\n\\n0\\n\\n\\n\\nk\\n\\n0\\n\\n\\nT\\n\\n\\n\\n\\n{\\\\displaystyle q_{0}k_{0}^{T}}\\n\\n is large, \\n\\n\\n\\n\\nq\\n\\n0\\n\\n\\n\\nk\\n\\n1\\n\\n\\nT\\n\\n\\n\\n\\n{\\\\displaystyle q_{0}k_{1}^{T}}\\n\\n is small, and the rest are very small. This can be interpreted as saying that the attention weight should be mostly applied to the 0th hidden vector of the encoder, a little to the 1st, and essentially none to the rest.\\nIn order to make a properly weighted sum, we need to transform this list of dot products into a probability distribution over \\n\\n\\n\\n0\\n,\\n1\\n,\\n…\\n\\n\\n{\\\\displaystyle 0,1,\\\\dots }\\n\\n. This can be accomplished by the softmax function, thus giving us the attention weights:\\n\\n\\n\\n(\\n\\nw\\n\\n00\\n\\n\\n,\\n\\nw\\n\\n01\\n\\n\\n,\\n…\\n)\\n=\\n\\ns\\no\\nf\\nt\\nm\\na\\nx\\n\\n(\\n\\nq\\n\\n0\\n\\n\\n\\nk\\n\\n0\\n\\n\\nT\\n\\n\\n,\\n\\nq\\n\\n0\\n\\n\\n\\nk\\n\\n1\\n\\n\\nT\\n\\n\\n,\\n…\\n)\\n\\n\\n{\\\\displaystyle (w_{00},w_{01},\\\\dots )=\\\\mathrm {softmax} (q_{0}k_{0}^{T},q_{0}k_{1}^{T},\\\\dots )}\\n\\nThis is then used to compute the context vector:\\n\\n\\n\\n\\nc\\n\\n0\\n\\n\\n=\\n\\nw\\n\\n00\\n\\n\\n\\nv\\n\\n0\\n\\n\\n+\\n\\nw\\n\\n01\\n\\n\\n\\nv\\n\\n1\\n\\n\\n+\\n⋯\\n\\n\\n{\\\\displaystyle c_{0}=w_{00}v_{0}+w_{01}v_{1}+\\\\cdots }\\n\\n\\nwhere \\n\\n\\n\\n\\nv\\n\\n0\\n\\n\\n=\\n\\nh\\n\\n0\\n\\n\\n\\nW\\n\\nV\\n\\n\\n,\\n\\nv\\n\\n1\\n\\n\\n=\\n\\nh\\n\\n1\\n\\n\\n\\nW\\n\\nV\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle v_{0}=h_{0}W^{V},v_{1}=h_{1}W^{V},\\\\dots }\\n\\n are the value vectors, linearly transformed by another matrix to provide the model with freedom to find the best way to represent values. Without the matrices \\n\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n,\\n\\nW\\n\\nK\\n\\n\\n,\\n\\nW\\n\\nV\\n\\n\\n\\n\\n{\\\\displaystyle W^{Q},W^{K},W^{V}}\\n\\n, the model would be forced to use the same hidden vector for both key and value, which might not be appropriate, as these two tasks are not the same.\\n\\nComputing the attention weights by dot-product. This is the \"decoder cross-attention\".\\nThis is the dot-attention mechanism. The particular version described in this section is \"decoder cross-attention\", as the output context vector is used by the decoder, and the input keys and values come from the encoder, but the query comes from the decoder, thus \"cross-attention\".\\n\\nMore succinctly, we can write it as\\n\\n\\n\\n\\nc\\n\\n0\\n\\n\\n=\\n\\nA\\nt\\nt\\ne\\nn\\nt\\ni\\no\\nn\\n\\n(\\n\\nh\\n\\n0\\n\\n\\nd\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n,\\nH\\n\\nW\\n\\nK\\n\\n\\n,\\nH\\n\\nW\\n\\nV\\n\\n\\n)\\n=\\n\\ns\\no\\nf\\nt\\nm\\na\\nx\\n\\n(\\n(\\n\\nh\\n\\n0\\n\\n\\nd\\n\\n\\n\\nW\\n\\nQ\\n\\n\\n)\\n\\n(\\nH\\n\\nW\\n\\nK\\n\\n\\n\\n)\\n\\nT\\n\\n\\n)\\n(\\nH\\n\\nW\\n\\nV\\n\\n\\n)\\n\\n\\n{\\\\displaystyle c_{0}=\\\\mathrm {Attention} (h_{0}^{d}W^{Q},HW^{K},HW^{V})=\\\\mathrm {softmax} ((h_{0}^{d}W^{Q})\\\\;(HW^{K})^{T})(HW^{V})}\\n\\nwhere the matrix \\n\\n\\n\\nH\\n\\n\\n{\\\\displaystyle H}\\n\\n is the matrix whose rows are \\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n,\\n\\nh\\n\\n1\\n\\n\\n,\\n…\\n\\n\\n{\\\\displaystyle h_{0},h_{1},\\\\dots }\\n\\n. Note that the querying vector, \\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\nd\\n\\n\\n\\n\\n{\\\\displaystyle h_{0}^{d}}\\n\\n, is not necessarily the same as the key-value vector \\n\\n\\n\\n\\nh\\n\\n0\\n\\n\\n\\n\\n{\\\\displaystyle h_{0}}\\n\\n. In fact, it is theoretically possible for query, key, and value vectors to all be different, though that is rarely done in practice.Seq2seq RNN encoder-decoder with attention mechanism, training\\nSeq2seq RNN encoder-decoder with attention mechanism, training and inferring\\nOther applications[edit]\\nIn 2019, Facebook announced its use in symbolic integration and resolution of differential equations. The company claimed that it could solve complex equations more rapidly and with greater accuracy than commercial solutions such as Mathematica, MATLAB and Maple. First, the equation is parsed into a tree structure to avoid notational idiosyncrasies. An LSTM neural network then applies its standard pattern recognition facilities to process the tree.[14][15]\\nIn 2020, Google released Meena, a 2.6 billion parameter seq2seq-based chatbot trained on a 341 GB data set. Google claimed that the chatbot has 1.7 times greater model capacity than OpenAI\\'s GPT-2.[4]\\nIn 2022, Amazon introduced AlexaTM 20B, a moderate-sized (20 billion parameter) seq2seq language model. It uses an encoder-decoder to accomplish few-shot learning. The encoder outputs a representation of the input that the decoder uses as input to perform a specific task, such as translating the input into another language. The model outperforms the much larger GPT-3 in language translation and summarization. Training mixes denoising (appropriately inserting missing text in strings) and causal-language-modeling (meaningfully extending an input text). It allows adding features across different languages without massive training workflows. AlexaTM 20B achieved state-of-the-art performance in few-shot-learning tasks across all Flores-101 language pairs, outperforming GPT-3 on several tasks.[16]\\n\\nSee also[edit]\\nArtificial neural network\\nReferences[edit]\\n\\n\\n^ a b c d e Sutskever, Ilya; Vinyals, Oriol; Le, Quoc Viet (2014). \"Sequence to sequence learning with neural networks\". arXiv:1409.3215 [cs.CL].\\n\\n^ a b c Cho, Kyunghyun; van Merrienboer, Bart; Gulcehre, Caglar; Bahdanau, Dzmitry; Bougares, Fethi; Schwenk, Holger; Bengio, Yoshua (2014-06-03). \"Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\". arXiv:1406.1078 [cs.CL].\\n\\n^ Xu, Kelvin; Ba, Jimmy; Kiros, Ryan; Cho, Kyunghyun; Courville, Aaron; Salakhudinov, Ruslan; Zemel, Rich; Bengio, Yoshua (2015-06-01). \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\". Proceedings of the 32nd International Conference on Machine Learning. PMLR: 2048–2057.\\n\\n^ a b Adiwardana, Daniel; Luong, Minh-Thang; So, David R.; Hall, Jamie; Fiedel, Noah; Thoppilan, Romal; Yang, Zi; Kulshreshtha, Apoorv; Nemade, Gaurav; Lu, Yifeng; Le, Quoc V. (2020-01-31). \"Towards a Human-like Open-Domain Chatbot\". arXiv:2001.09977 [cs.CL].\\n\\n^ Chan, William; Jaitly, Navdeep; Le, Quoc; Vinyals, Oriol (March 2016). \"Listen, attend and spell: A neural network for large vocabulary conversational speech recognition\". 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. pp.\\xa04960–4964. doi:10.1109/ICASSP.2016.7472621. ISBN\\xa0978-1-4799-9988-0.\\n\\n^ Rush, Alexander M.; Chopra, Sumit; Weston, Jason (September 2015). \"A Neural Attention Model for Abstractive Sentence Summarization\". In Màrquez, Lluís; Callison-Burch, Chris; Su, Jian (eds.). Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lisbon, Portugal: Association for Computational Linguistics. pp.\\xa0379–389. doi:10.18653/v1/D15-1044.\\n\\n^ Bahdanau, Dzmitry; Cho, Kyunghyun; Bengio, Yoshua (2014). \"Neural Machine Translation by Jointly Learning to Align and Translate\". arXiv:1409.0473 [cs.CL].\\n\\n^ Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Ł ukasz; Polosukhin, Illia (2017). \"Attention is All you Need\". Advances in Neural Information Processing Systems. 30. Curran Associates, Inc.\\n\\n^ Wu, Yonghui; Schuster, Mike; Chen, Zhifeng; Le, Quoc V.; Norouzi, Mohammad; Macherey, Wolfgang; Krikun, Maxim; Cao, Yuan; Gao, Qin; Macherey, Klaus; Klingner, Jeff; Shah, Apurva; Johnson, Melvin; Liu, Xiaobing; Kaiser, Łukasz (2016). \"Google\\'s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\". arXiv:1609.08144 [cs.CL].\\n\\n^ Mikolov, Tomáš (December 13, 2023). \"Yesterday we received a Test of Time Award at NeurIPS for the word2vec paper from ten years ago\". Facebook. Archived from the original on 24 Dec 2023.\\n\\n^ Mikolov, Tomáš. \"Statistical language models based on neural networks.\" (2012).\\n\\n^ Zhang, Aston; Lipton, Zachary; Li, Mu; Smola, Alexander J. (2024). \"10.7. Sequence-to-Sequence Learning for Machine Translation\". Dive into deep learning. Cambridge New York Port Melbourne New Delhi Singapore: Cambridge University Press. ISBN\\xa0978-1-009-38943-3.\\n\\n^ Bengio, Samy; Vinyals, Oriol; Jaitly, Navdeep; Shazeer, Noam (2015). \"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks\". Advances in Neural Information Processing Systems. 28. Curran Associates, Inc.\\n\\n^ \"Facebook has a neural network that can do advanced math\". MIT Technology Review. December 17, 2019. Retrieved 2019-12-17.\\n\\n^ Lample, Guillaume; Charton, François (2019). \"Deep Learning for Symbolic Mathematics\". arXiv:1912.01412 [cs.SC].\\n\\n^ Soltan, Saleh; Ananthakrishnan, Shankar; FitzGerald, Jack; Gupta, Rahul; Hamza, Wael; Khan, Haidar; Peris, Charith; Rawls, Stephen; Rosenbaum, Andy; Rumshisky, Anna; Chandana Satya Prakash; Sridhar, Mukund; Triefenbach, Fabian; Verma, Apurv; Tur, Gokhan; Natarajan, Prem (2022). \"AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model\". arXiv:2208.01448 [cs.CL].\\n\\n\\nExternal links[edit]\\nVoita, Lena. \"Sequence to Sequence (seq2seq) and Attention\". Retrieved 2023-12-20.\\n\"A ten-minute introduction to sequence-to-sequence learning in Keras\". blog.keras.io. Retrieved 2019-12-19.\\nvteNatural language processingGeneral terms\\nAI-complete\\nBag-of-words\\nn-gram\\nBigram\\nTrigram\\nComputational linguistics\\nNatural language understanding\\nStop words\\nText processing\\nText analysis\\nArgument mining\\nCollocation extraction\\nConcept mining\\nCoreference resolution\\nDeep linguistic processing\\nDistant reading\\nInformation extraction\\nNamed-entity recognition\\nOntology learning\\nParsing\\nSemantic parsing\\nSyntactic parsing\\nPart-of-speech tagging\\nSemantic analysis\\nSemantic role labeling\\nSemantic decomposition\\nSemantic similarity\\nSentiment analysis\\nTerminology extraction\\nText mining\\nTextual entailment\\nTruecasing\\nWord-sense disambiguation\\nWord-sense induction\\nText segmentation\\nCompound-term processing\\nLemmatisation\\nLexical analysis\\nText chunking\\nStemming\\nSentence segmentation\\nWord segmentation\\n\\nAutomatic summarization\\nMulti-document summarization\\nSentence extraction\\nText simplification\\nMachine translation\\nComputer-assisted\\nExample-based\\nRule-based\\nStatistical\\nTransfer-based\\nNeural\\nDistributional semantics models\\nBERT\\nDocument-term matrix\\nExplicit semantic analysis\\nfastText\\nGloVe\\nLanguage model (large)\\nLatent semantic analysis\\nSeq2seq\\nWord embedding\\nWord2vec\\nLanguage resources,datasets and corporaTypes andstandards\\nCorpus linguistics\\nLexical resource\\nLinguistic Linked Open Data\\nMachine-readable dictionary\\nParallel text\\nPropBank\\nSemantic network\\nSimple Knowledge Organization System\\nSpeech corpus\\nText corpus\\nThesaurus (information retrieval)\\nTreebank\\nUniversal Dependencies\\nData\\nBabelNet\\nBank of English\\nDBpedia\\nFrameNet\\nGoogle Ngram Viewer\\nUBY\\nWordNet\\nWikidata\\nAutomatic identificationand data capture\\nSpeech recognition\\nSpeech segmentation\\nSpeech synthesis\\nNatural language generation\\nOptical character recognition\\nTopic model\\nDocument classification\\nLatent Dirichlet allocation\\nPachinko allocation\\nComputer-assistedreviewing\\nAutomated essay scoring\\nConcordancer\\nGrammar checker\\nPredictive text\\nPronunciation assessment\\nSpell checker\\nNatural languageuser interface\\nChatbot\\nInteractive fiction\\nQuestion answering\\nVirtual assistant\\nVoice user interface\\nRelated\\nFormal semantics\\nHallucination\\nNatural Language Toolkit\\nspaCy\\n\\nvteArtificial intelligence (AI)History (timeline)Concepts\\nParameter\\nHyperparameter\\nLoss functions\\nRegression\\nBias–variance tradeoff\\nDouble descent\\nOverfitting\\nClustering\\nGradient descent\\nSGD\\nQuasi-Newton method\\nConjugate gradient method\\nBackpropagation\\nAttention\\nConvolution\\nNormalization\\nBatchnorm\\nActivation\\nSoftmax\\nSigmoid\\nRectifier\\nGating\\nWeight initialization\\nRegularization\\nDatasets\\nAugmentation\\nPrompt engineering\\nReinforcement learning\\nQ-learning\\nSARSA\\nImitation\\nPolicy gradient\\nDiffusion\\nLatent diffusion model\\nAutoregression\\nAdversary\\nRAG\\nUncanny valley\\nRLHF\\nSelf-supervised learning\\nReflection\\nRecursive self-improvement\\nHallucination\\nWord embedding\\nVibe coding\\nApplications\\nMachine learning\\nIn-context learning\\nArtificial neural network\\nDeep learning\\nLanguage model\\nLarge language model\\nNMT\\nReasoning language model\\nModel Context Protocol\\nIntelligent agent\\nArtificial human companion\\nHumanity\\'s Last Exam\\nArtificial general intelligence (AGI)\\nImplementationsAudio–visual\\nAlexNet\\nWaveNet\\nHuman image synthesis\\nHWR\\nOCR\\nSpeech synthesis\\n15.ai\\nElevenLabs\\nSpeech recognition\\nWhisper\\nFacial recognition\\nAlphaFold\\nText-to-image models\\nAurora\\nDALL-E\\nFirefly\\nFlux\\nIdeogram\\nImagen\\nMidjourney\\nStable Diffusion\\nText-to-video models\\nDream Machine\\nRunway Gen\\nHailuo AI\\nKling\\nSora\\nVeo\\nMusic generation\\nSuno AI\\nUdio\\nText\\nWord2vec\\nSeq2seq\\nGloVe\\nBERT\\nT5\\nLlama\\nChinchilla AI\\nPaLM\\nGPT\\n1\\n2\\n3\\nJ\\nChatGPT\\n4\\n4o\\no1\\no3\\n4.5\\n4.1\\no4-mini\\nClaude\\nGemini\\nchatbot\\nGrok\\nLaMDA\\nBLOOM\\nProject Debater\\nIBM Watson\\nIBM Watsonx\\nGranite\\nPanGu-Σ\\nDeepSeek\\nQwen\\nDecisional\\nAlphaGo\\nAlphaZero\\nOpenAI Five\\nSelf-driving car\\nMuZero\\nAction selection\\nAutoGPT\\nRobot control\\nPeople\\nAlan Turing\\nWarren Sturgis McCulloch\\nWalter Pitts\\nJohn von Neumann\\nClaude Shannon\\nMarvin Minsky\\nJohn McCarthy\\nNathaniel Rochester\\nAllen Newell\\nCliff Shaw\\nHerbert A. Simon\\nOliver Selfridge\\nFrank Rosenblatt\\nBernard Widrow\\nJoseph Weizenbaum\\nSeymour Papert\\nSeppo Linnainmaa\\nPaul Werbos\\nJürgen Schmidhuber\\nYann LeCun\\nGeoffrey Hinton\\nJohn Hopfield\\nYoshua Bengio\\nLotfi A. Zadeh\\nStephen Grossberg\\nAlex Graves\\nJames Goodnight\\nAndrew Ng\\nFei-Fei Li\\nIlya Sutskever\\nAlex Krizhevsky\\nIan Goodfellow\\nDemis Hassabis\\nDavid Silver\\nAndrej Karpathy\\nAshish Vaswani\\nNoam Shazeer\\nAidan Gomez\\nArchitectures\\nNeural Turing machine\\nDifferentiable neural computer\\nTransformer\\nVision transformer (ViT)\\nRecurrent neural network (RNN)\\nLong short-term memory (LSTM)\\nGated recurrent unit (GRU)\\nEcho state network\\nMultilayer perceptron (MLP)\\nConvolutional neural network (CNN)\\nResidual neural network (RNN)\\nHighway network\\nMamba\\nAutoencoder\\nVariational autoencoder (VAE)\\nGenerative adversarial network (GAN)\\nGraph neural network (GNN)\\n\\n Portals\\nTechnology\\n Category\\nArtificial neural networks\\nMachine learning\\n List\\nCompanies\\nProjects\\n\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Seq2seq&oldid=1296120849\"\\nCategories: Artificial neural networksNatural language processingHidden categories: Articles with short descriptionShort description matches Wikidata\\n\\n\\n\\n\\n\\n\\n This page was last edited on 17 June 2025, at 23:29\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike 4.0 License;\\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nSeq2seq\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n8 languages\\n\\n\\nAdd topic\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fe5465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "docs_split = text_splitter.split_documents(docs_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e33c6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(\n",
    "  documents=docs_split,\n",
    "  embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4159ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "  search_type=\"similarity\",\n",
    "  search_kwargs={\"k\": 2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "724c822e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c22f2d5a-b674-4aff-9c3c-93e4ce310c08', metadata={'source': 'https://en.wikipedia.org/wiki/Seq2seq', 'title': 'Seq2seq - Wikipedia', 'language': 'en'}, page_content='Attention for seq2seq[edit]'),\n",
       " Document(id='d9eea084-2075-41c3-9f5e-5b89efafc603', metadata={'source': 'https://en.wikipedia.org/wiki/Seq2seq', 'title': 'Seq2seq - Wikipedia', 'language': 'en'}, page_content='The seq2seq method developed in the early 2010s uses two neural networks:  an encoder network converts an input sentence into numerical vectors, and a decoder network converts those vectors to sentences in the target language.  The Attention mechanism was grafted onto this structure in 2014 and shown below. Later it was refined into the encoder-decoder Transformer architecture of 2017.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"what is seq2seq?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bc8d107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"retriever\",\n",
    "    description=\"Useful for retrieving information from the knowledge base.\"\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e27a3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "12c4d275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"How's it going? Is there something I can help you with or would you like to chat?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 12, 'total_tokens': 33, 'completion_time': 0.044791566, 'prompt_time': 0.002184924, 'queue_time': 0.049280266, 'total_time': 0.04697649}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_37da608fc1', 'finish_reason': 'stop', 'logprobs': None}, id='run-24819ade-16c7-4045-aa4a-c2a32768359b-0', usage_metadata={'input_tokens': 12, 'output_tokens': 21, 'total_tokens': 33})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm=ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "llm.invoke(\"Hey there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b8e69d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "  print(\"---CALL AGENT---\")\n",
    "  llm = ChatGroq(model=\"meta-llama/llama-4-scout-17b-16e-instruct\")\n",
    "  llm = llm.bind_tools(tools)\n",
    "\n",
    "  response = llm.invoke(state['messages'])\n",
    "  return {'messages': [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c3f2c379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf9078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_document(state) -> Literal['rewrite','generate']:\n",
    "    print(\"---CHECK RELEVANCE ---\")\n",
    "\n",
    "    class Grade(BaseModel):\n",
    "        binary_score: str = Field(\"description='Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    model = ChatGroq(\n",
    "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "            temperature=0.0\n",
    "        )\n",
    "    \n",
    "    model = model.with_structured_output(Grade)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "\n",
    "    chain = prompt | model\n",
    "\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    context = last_message.content\n",
    "\n",
    "    score_result = chain.invoke({\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    })\n",
    "\n",
    "    score = score_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2875ecfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "\n",
    "  print(\"---GENERATE ANSWER---\")\n",
    "\n",
    "  messages = state['messages']\n",
    "  last_message = messages[-1]\n",
    "\n",
    "  question = messages[0].content\n",
    "  context = last_message.content\n",
    "\n",
    "  prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "  llm = ChatGroq(\n",
    "      model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "      temperature=0.0\n",
    "  )\n",
    "\n",
    "  llm = llm.bind_tools(tools)\n",
    "\n",
    "  rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "  response = rag_chain.invoke({\n",
    "      \"context\": context,\n",
    "      \"question\": question\n",
    "  })\n",
    "\n",
    "  return {'messages': [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a513f9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "165f076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "retriever = ToolNode([retriever_tool])\n",
    "\n",
    "graph.add_node('agent', agent)\n",
    "graph.add_node('retriever', retriever)\n",
    "graph.add_node('generate', generate)\n",
    "graph.add_node('rewrite', rewrite)\n",
    "\n",
    "graph.add_edge(START, 'agent')\n",
    "graph.add_conditional_edges(\n",
    "    'agent', \n",
    "\n",
    "    tools_condition,\n",
    "    {\n",
    "      \"tools\":\"retriever\",\n",
    "      END:END,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    'retriever',\n",
    "    grade_document\n",
    ")\n",
    "\n",
    "graph.add_edge(\"generate\", END)\n",
    "graph.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61fb2d4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6505fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='How does seq2seq work and why do we use it and is attention needed?', additional_kwargs={}, response_metadata={}, id='8e5c20bf-e923-4713-b89d-d29347514581'),\n",
       "  AIMessage(content='Seq2seq, short for sequence-to-sequence, is a type of neural network architecture used primarily in natural language processing (NLP) tasks such as machine translation, text summarization, and chatbots. It works by taking a sequence of elements (like words or characters) as input and generating another sequence as output.\\n\\nThe seq2seq model typically consists of two main components: an encoder and a decoder. The encoder reads the input sequence and converts it into a fixed-length vector representation. The decoder then generates the output sequence, one element at a time, based on this vector representation.\\n\\nThe process can be broken down into the following steps:\\n\\n1. **Encoding**: The encoder processes the input sequence, which could be a sentence in a source language, and outputs a continuous representation of the input, often a vector.\\n\\n2. **Decoding**: The decoder takes this vector and generates the output sequence, which could be a translation of the input sentence into a target language.\\n\\nHowever, traditional seq2seq models have a limitation: they use a fixed-length vector to represent the entire input sequence, which can lead to loss of information, especially for long sequences.\\n\\nTo address this limitation, **attention mechanisms** were introduced. Attention allows the model to focus on different parts of the input sequence at different steps of the output generation. Instead of encoding the entire input sequence into a single fixed-length vector, the attention mechanism helps the decoder to \"attend\" to different parts of the input sequence and weigh their importance when generating each word of the output sequence.\\n\\nAttention is not strictly needed for seq2seq to work, but it significantly improves the model\\'s performance, especially in tasks involving long sequences. It helps in:\\n\\n- **Preserving Information**: By focusing on relevant parts of the input sequence, the model can better preserve the information needed for generating the output sequence.\\n\\n- **Handling Long Sequences**: For tasks like translation, where sentences can vary greatly in length, attention helps the model to selectively focus on parts of the input sequence that are relevant for generating each part of the output sequence.\\n\\n- **Improving Accuracy**: Attention mechanisms can significantly improve the accuracy of seq2seq models by allowing them to handle complex dependencies and relationships within the input sequence more effectively.\\n\\nIn summary, seq2seq is a powerful architecture for sequence generation tasks, and attention mechanisms are a crucial component that enhances its capability to handle complex sequences and improve output accuracy.\\n\\nSince the provided function list only includes a retriever function for accessing knowledge base information and does not directly relate to explaining or implementing seq2seq or attention mechanisms, the response is based on internal knowledge. \\n\\nNo function call is needed here as the query is knowledge-based. \\n\\nTherefore, the response remains as is, without a function call.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 550, 'prompt_tokens': 1395, 'total_tokens': 1945, 'completion_time': 1.123087553, 'prompt_time': 0.036922122, 'queue_time': -1.351766387, 'total_time': 1.160009675}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'finish_reason': 'stop', 'logprobs': None}, id='run-5ab77a7e-5299-44bb-adc3-5eb917eca7be-0', usage_metadata={'input_tokens': 1395, 'output_tokens': 550, 'total_tokens': 1945})]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({\"messages\":\"How does seq2seq work and why do we use it and is attention needed?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2519c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
