{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.Completions object at 0x0000023C6578F6D0> async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000023C65211590> root_client=<openai.OpenAI object at 0x0000023C64C37750> root_async_client=<openai.AsyncOpenAI object at 0x0000023C6559A090> model_name='gpt-4o-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The terms \"AI agent\" and \"agentic AI\" refer to different concepts within the field of artificial intelligence, and it's important to understand their distinctions:\n",
      "\n",
      "1. **AI Agent**:\n",
      "   - An AI agent refers to any system or program that performs tasks autonomously or semi-autonomously based on data and pre-defined rules. It can perceive its environment through sensors, make decisions, and take actions to achieve specific goals.\n",
      "   - Examples of AI agents include chatbots, recommendation systems, self-driving cars, and game-playing AI like those used in chess or Go.\n",
      "   - AI agents can be classified based on their functionalities, such as reactive agents (which respond to immediate stimuli), deliberative agents (which plan over time), or hybrid agents (which employ both reactive and deliberative approaches).\n",
      "\n",
      "2. **Agentic AI**:\n",
      "   - Agentic AI typically refers to a more advanced or sophisticated form of AI that exhibits a higher level of autonomy and decision-making capability. This type of AI can make independent choices, learn from its experiences, and adapt its behaviors to achieve complex goals in changing environments.\n",
      "   - The term \"agentic\" suggests a degree of agency or autonomy, implying that the AI system has a deeper understanding of its goals and can proactively act to fulfill them, rather than merely responding to external commands.\n",
      "   - Discussions around agentic AI often address ethical considerations and implications, particularly the potential risks associated with granting AI systems significant decision-making power.\n",
      "\n",
      "In summary, while **AI agents** encompass a broad range of autonomous systems, **agentic AI** refers to a more advanced type of AI that possesses greater autonomy, decision-making capabilities, and potentially a level of self-awareness in its operation.\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke('What is the difference between AI agent and Agentic AI')\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert critic that roasts in subtle yet brutal way'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "  [\n",
    "    ('system', 'You are an expert critic that roasts in subtle yet brutal way'),\n",
    "    ('user', '{input}')\n",
    "  ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, the classic transformation from non-profit to for-profit‚Äîlike watching a noble knight trade in his shining armor for a slick suit and a briefcase full of dreams. This AI company must have figured that altruism just doesn‚Äôt pay the bills‚Äîwho knew that changing the world with artificial intelligence doesn‚Äôt come with a salary? It‚Äôs like they went from feeding the hungry to catering their own banquet, claiming it‚Äôs all in the name of ‚Äúsustainability.‚Äù I guess the only thing more artificial than their intelligence is their commitment to non-profit ideals. Congratulations, folks, you‚Äôve officially mastered the art of virtue signaling while cashing in on the very innovations that were supposed to save humanity. \n",
      "\n",
      "Welcome to the future: the robots might just take over, but at least they'll be wearing designer labels.\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({'input' : 'Roast an ai company that was launched as an non profit org and now is a for profit org'})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  template='Answer the user question. \\n {question} \\n {format}\\n',\n",
    "  input_variables= ['question'],\n",
    "  partial_variables= {'format' : output_parser.get_format_instructions()},\n",
    "\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "response = chain.invoke({'question': 'What is tachyon?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'definition': 'A tachyon is a theoretical particle in physics that is said to travel faster than the speed of light. Its existence has not been confirmed experimentally.', 'properties': {'speed': 'Faster than light', 'mass': 'Imaginary mass (as suggested by some theories)', 'theoretical_status': 'Not yet observed or proven to exist'}, 'relevance': {'theories': ['Quantum field theory', 'String theory'], 'implications': ['Potential violation of causality', 'Could lead to time travel scenarios']}}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building a Basic RAG System Using Langchain and a Webpage as Data**  \n",
    "\n",
    "#### **1Ô∏è‚É£ Load Webpage Content (Web Base Loader)**  \n",
    "Use `WebBaseLoader` to extract text from a webpage. This allows retrieval of raw text data from any given URL, which will serve as the knowledge source for retrieval.  \n",
    "\n",
    "#### **2Ô∏è‚É£ Split Text into Chunks**  \n",
    "Since LLMs have token limits, use `RecursiveCharacterTextSplitter` to break the extracted text into manageable chunks. This ensures better context retrieval while maintaining semantic integrity.  \n",
    "\n",
    "#### **3Ô∏è‚É£ Convert Chunks to Vectors & Store in Vector Database**  \n",
    "Each chunk is converted into vector embeddings using an embedding model (e.g., OpenAI, Hugging Face). These vectors are then stored in a **vector database** (e.g., FAISS, Chroma, Pinecone) for efficient retrieval based on similarity search.  \n",
    "\n",
    "#### **4Ô∏è‚É£ Query Processing & Context Retrieval**  \n",
    "When a user asks a question, the query is also converted into a vector. A similarity search is performed in the vector database to find the most relevant chunks. These retrieved chunks are then passed as context to the LLM, which generates a response based on them.  \n",
    "\n",
    "This process ensures that the LLM provides **accurate, knowledge-grounded responses** instead of relying solely on its pretrained data. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader('https://python.langchain.com/docs/introduction/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nIntroduction | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyIntroductionOn this pageIntroduction\\nLangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:\\n\\nDevelopment: Build your applications using LangChain\\'s open-source components and third-party integrations.\\nUse LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\\n\\n\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the integrations page for\\nmore.\\n\\nSelect chat model:Groq‚ñæGroqOpenAIAnthropicAzureGoogleAWSCohereNVIDIAFireworks AIMistral AITogether AIDatabrickspip install -qU langchain-groqimport getpassimport osif not os.environ.get(\"GROQ_API_KEY\"):  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")from langchain_groq import ChatGroqmodel = ChatGroq(model=\"llama3-8b-8192\")\\nmodel.invoke(\"Hello, world!\")\\nnoteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\nArchitecture\\u200b\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\nArchitecture page.\\n\\nlangchain-core: Base abstractions for chat models and other components.\\nIntegration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\nlangchain: Chains, agents, and retrieval strategies that make up an application\\'s cognitive architecture.\\nlangchain-community: Third-party integrations that are community maintained.\\nlanggraph: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See LangGraph documentation.\\n\\nGuides\\u200b\\nTutorials\\u200b\\nIf you\\'re looking to build something specific or are more of a hands-on learner, check out our tutorials section.\\nThis is the best place to get started.\\nThese are the best ones to get started with:\\n\\nBuild a Simple LLM Application\\nBuild a Chatbot\\nBuild an Agent\\nIntroduction to LangGraph\\n\\nExplore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here.\\nHow-to guides\\u200b\\nHere you‚Äôll find short answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\\nThese how-to guides don‚Äôt cover topics in depth ‚Äì you‚Äôll find that material in the Tutorials and the API Reference.\\nHowever, these guides will help you quickly accomplish common tasks using chat models,\\nvector stores, and other common LangChain components.\\nCheck out LangGraph-specific how-tos here.\\nConceptual guide\\u200b\\nIntroductions to all the key parts of LangChain you‚Äôll need to know! Here you\\'ll find high level explanations of all LangChain concepts.\\nFor a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you\\'re looking to get up and running quickly with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our growing list of integrations.\\nAPI reference\\u200b\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\nEcosystem\\u200b\\nü¶úüõ†Ô∏è LangSmith\\u200b\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\nü¶úüï∏Ô∏è LangGraph\\u200b\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\nAdditional resources\\u200b\\nVersions\\u200b\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\nSecurity\\u200b\\nRead up on security best practices to make sure you\\'re developing safely with LangChain.\\nContributing\\u200b\\nCheck out the developer\\'s guide for guidelines on contributing and help getting your dev environment set up.Edit this pageWas this page helpful?NextTutorialsArchitectureGuidesTutorialsHow-to guidesConceptual guideIntegrationsAPI referenceEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphAdditional resourcesVersionsSecurityContributingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Top 3 Text Splitters in Langchain**  \n",
    "\n",
    "1Ô∏è‚É£ **RecursiveCharacterTextSplitter**  \n",
    "   - Best for general-purpose text splitting while preserving semantic meaning.  \n",
    "   - Splits based on characters (e.g., \".\", \"\\n\", \" \") and allows overlap.  \n",
    "\n",
    "2Ô∏è‚É£ **TokenTextSplitter**  \n",
    "   - Splits text based on token count instead of characters.  \n",
    "   - Useful for models with strict token limits (e.g., OpenAI models).  \n",
    "\n",
    "3Ô∏è‚É£ **MarkdownTextSplitter**  \n",
    "   - Specifically designed for **Markdown documents**.  \n",
    "   - Preserves headers, lists, and structured formatting.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document  # Ensure correct Document usage\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='Introduction | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data directly to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyIntroductionOn this pageIntroduction'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='LangChain is a framework for developing applications powered by large language models (LLMs).\\nLangChain simplifies every stage of the LLM application lifecycle:'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"Development: Build your applications using LangChain's open-source components and third-party integrations.\\nUse LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\nProductionization: Use LangSmith to inspect, monitor and evaluate your applications, so that you can continuously optimize and deploy with confidence.\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Platform.\\n\\n\\n\\nLangChain implements a standard interface for large language models and related\\ntechnologies, such as embedding models and vector stores, and integrates with\\nhundreds of providers. See the integrations page for\\nmore.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content='Select chat model:Groq‚ñæGroqOpenAIAnthropicAzureGoogleAWSCohereNVIDIAFireworks AIMistral AITogether AIDatabrickspip install -qU langchain-groqimport getpassimport osif not os.environ.get(\"GROQ_API_KEY\"):  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")from langchain_groq import ChatGroqmodel = ChatGroq(model=\"llama3-8b-8192\")\\nmodel.invoke(\"Hello, world!\")\\nnoteThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\nArchitecture\\u200b\\nThe LangChain framework consists of multiple open-source libraries. Read more in the\\nArchitecture page.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"langchain-core: Base abstractions for chat models and other components.\\nIntegration packages (e.g. langchain-openai, langchain-anthropic, etc.): Important integrations have been split into lightweight packages that are co-maintained by the LangChain team and the integration developers.\\nlangchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\\nlangchain-community: Third-party integrations that are community maintained.\\nlanggraph: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features. See LangGraph documentation.\\n\\nGuides\\u200b\\nTutorials\\u200b\\nIf you're looking to build something specific or are more of a hands-on learner, check out our tutorials section.\\nThis is the best place to get started.\\nThese are the best ones to get started with:\\n\\nBuild a Simple LLM Application\\nBuild a Chatbot\\nBuild an Agent\\nIntroduction to LangGraph\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"Explore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here.\\nHow-to guides\\u200b\\nHere you‚Äôll find short answers to ‚ÄúHow do I‚Ä¶.?‚Äù types of questions.\\nThese how-to guides don‚Äôt cover topics in depth ‚Äì you‚Äôll find that material in the Tutorials and the API Reference.\\nHowever, these guides will help you quickly accomplish common tasks using chat models,\\nvector stores, and other common LangChain components.\\nCheck out LangGraph-specific how-tos here.\\nConceptual guide\\u200b\\nIntroductions to all the key parts of LangChain you‚Äôll need to know! Here you'll find high level explanations of all LangChain concepts.\\nFor a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"For a deeper dive into LangGraph concepts, check out this page.\\nIntegrations\\u200b\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it.\\nIf you're looking to get up and running quickly with chat models, vector stores,\\nor other LangChain components from a specific provider, check out our growing list of integrations.\\nAPI reference\\u200b\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\nEcosystem\\u200b\\nü¶úüõ†Ô∏è LangSmith\\u200b\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\nü¶úüï∏Ô∏è LangGraph\\u200b\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.\\nAdditional resources\\u200b\\nVersions\\u200b\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/docs/introduction/', 'title': 'Introduction | ü¶úÔ∏èüîó LangChain', 'description': 'LangChain is a framework for developing applications powered by large language models (LLMs).', 'language': 'en'}, page_content=\"Additional resources\\u200b\\nVersions\\u200b\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\nSecurity\\u200b\\nRead up on security best practices to make sure you're developing safely with LangChain.\\nContributing\\u200b\\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.Edit this pageWas this page helpful?NextTutorialsArchitectureGuidesTutorialsHow-to guidesConceptual guideIntegrationsAPI referenceEcosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphAdditional resourcesVersionsSecurityContributingCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\")]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = text_splitter.split_documents(document)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents split correctly!\n"
     ]
    }
   ],
   "source": [
    "if isinstance(documents, list) and isinstance(documents[0], Document):\n",
    "    print(\"Documents split correctly!\")\n",
    "else:\n",
    "    raise TypeError(\"Split documents are not in the expected format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectors = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x23c6657dd90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Prompt Templates are responsible for'\n",
    "\n",
    "result = vectors.similarity_search(query)\n",
    "\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Answer the question based on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "| PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the provided context:\\n<context>\\n{context}\\n</context>\\n\\nQuestion: {question}\\nAnswer:')\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000023C6578F6D0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000023C65211590>, root_client=<openai.OpenAI object at 0x0000023C64C37750>, root_async_client=<openai.AsyncOpenAI object at 0x0000023C6559A090>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "| StrOutputParser() kwargs={} config={'run_name': 'stuff_documents_chain'} config_factories=[]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "print(document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, top_k=3):\n",
    "    results = vectors.similarity_search(query, k=top_k)\n",
    "    \n",
    "    # Ensure results are LangChain Document objects\n",
    "    if isinstance(results[0], str):\n",
    "        return [Document(page_content=text) for text in results]\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
      "LangChain simplifies every stage of the LLM application lifecycle:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 2:\n",
      "Introduction | ü¶úÔ∏èüîó LangChain\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 3:\n",
      "storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyIntroductionOn this pageIntroduction\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is LangChain and how does it work?\"\n",
    "retrieved_docs = retrieve_documents(query)\n",
    "\n",
    "# Print retrieved content\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Document {i+1}:\\n{doc.page_content}\\n{'-'*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document  # Ensure this is imported\n",
    "\n",
    "# Convert strings to Document objects if necessary\n",
    "retrieved_docs = [\n",
    "    doc if isinstance(doc, Document) else Document(page_content=doc)\n",
    "    for doc in retrieved_docs\n",
    "]\n",
    "\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Response:\n",
      " LangChain is a framework designed for developing applications that utilize large language models (LLMs). It simplifies the entire lifecycle of LLM applications by providing tools and functionalities that assist in various stages, from building to deploying. By leveraging LangChain, developers can streamline the integration of LLMs into their applications, making it easier to manage and develop LLM-powered functionalities. The framework facilitates tasks such as memory management, data retrieval, and conversation flow, ensuring that developers can focus on enhancing user experience and application performance.\n"
     ]
    }
   ],
   "source": [
    "# Ensure retrieved_docs is a list of Document objects\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "if retrieved_docs and isinstance(retrieved_docs[0], str):\n",
    "    retrieved_docs = [Document(page_content=doc) for doc in retrieved_docs]\n",
    "\n",
    "query = \"What is LangChain and how does it work?\"\n",
    "\n",
    "# Invoke the chain with the expected keys\n",
    "response = document_chain.invoke({\n",
    "    \"context\": retrieved_docs,  # List of Document objects\n",
    "    \"question\": query                    # The question to answer\n",
    "})\n",
    "\n",
    "print(\"RAG Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
