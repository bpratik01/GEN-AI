{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')\n",
    "os.environ['LANGSMITH_TRACING'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x0000022C4231EFD0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x0000022C4288F590>, root_client=<openai.OpenAI object at 0x0000022C4270EF10>, root_async_client=<openai.AsyncOpenAI object at 0x0000022C427D80D0>, model_name='o1-mini', temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='o1-mini')\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(\n",
    "  load_max_docs=3,\n",
    "  query='Attention is all you need',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2024-07-22', 'Title': \"Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\", 'Authors': 'Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini', 'Summary': 'The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with low latencies remains challenging due to the quadratic input length\\ncomplexity of the attention layers. In this work, we investigate the effect of\\ndropping MLP and attention layers at inference time on the performance of\\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\\ndecreases performance but leads to the best speedups alongside dropping entire\\nlayers. For example, removing 33\\\\% of attention layers in a 13B Llama2 model\\nresults in a 1.8\\\\% drop in average performance over the OpenLLM benchmark. We\\nalso observe that skipping layers except the latter layers reduces performances\\nfor more layers skipped, except for skipping the attention layers.'}, page_content='Attention Is All You Need But You Don’t Need All Of It\\nFor Inference of Large Language Models\\nGeorgy Tyukin * 1 Gbetondji J-S Dovonon 1 Jean Kaddour 1 Pasquale Minervini 2\\nAbstract\\nThe inference demand for LLMs has skyrocketed\\nin recent months, and serving models with low\\nlatencies remains challenging due to the quadratic\\ninput length complexity of the attention layers.\\nIn this work, we investigate the effect of drop-\\nping MLP and attention layers at inference time\\non the performance of Llama-v2 models. We\\nfind that dropping dreeper attention layers only\\nmarginally decreases performance but leads to the\\nbest speedups alongside dropping entire layers.\\nFor example, removing 33% of attention layers\\nin a 13B Llama2 model results in a 1.8% drop in\\naverage performance over the OpenLLM bench-\\nmark. We also observe that skipping layers except\\nthe latter layers reduces performances for more\\nlayers skipped, except for skipping the attention\\nlayers.\\n1. Introduction\\nThe ubiquitous deployment of Large Language Models\\n(LLMs) results in ever-growing amounts of compute spent\\non inference (Patterson et al., 2021; Chen et al., 2023; Kad-\\ndour et al., 2023a; Xia et al., 2024; Reid et al., 2024). Fur-\\nther, serving models with low latencies remains challenging\\nbecause contemporary Transformer architectures employ\\nthe self-attention mechanism with quadratic input complex-\\nity (Touvron et al., 2023b; Jiang et al., 2023; Bi et al., 2024).\\nIn this work, we delve deeper into the concept of layer\\nskipping (Fan et al., 2019; Wang et al., 2022a) to reduce\\nthe computation on superfluous LLM components. Our\\nfindings demonstrate that pruning deeper attention layers\\ndoes not significantly affect performance. When applied\\nto Llama-v2 (Touvron et al., 2023b), we maintain good\\nperformance on the OpenLLM (ARC (Clark et al., 2018),\\n*Equal contribution\\n1University College London,\\nUK\\n2University of Edinburgh, UK. Correspondence to: Georgy Tyukin\\n<tyukinegor@gmail.com>.\\nWork presented at TF2M workshop at ICML 2024, Vienna, Austria.\\nPMLR 235, 2024. Copyright 2024 by the author(s).\\nHellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al.,\\n2021), TruthfulQA (Lin et al., 2022)) benchmarks (Beech-\\ning et al., 2023), recording only minimal performance devi-\\nations compared to the full model.\\n2. Method\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\nLayer\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\n1.00\\nCosine Similarity\\nCosine Similarity with previous layer for LLaMA-v2 7b and LLaMA-v2 13\\nLLaMA-v2 7b\\nLLaMA-v2 13b\\nFigure 1. Cosine similarity of Llama-v2 layers with the previous\\nlayer: We observe that the deeper the layer, the more its features\\nare similar to the previous layer except for the very last layer.\\n2.1. Layer skipping\\nConsider a Transformer model M with L layers, each\\nconsisting of an attention sub-layer followed by a multi-\\nlayer perceptron (MLP) sub-layer. We denote each layer as\\nMi = (Attentioni, MLPi) for i ∈{1, 2, . . . , L}.\\nTo compare the performance of Transformer models when\\nskipping specific sub-layers, we create two variants of the\\nmodel:\\n1. Skipping MLP Layers: We construct a model Mskip MLP\\n1\\narXiv:2407.15516v1  [cs.LG]  22 Jul 2024\\nAttention Is All You Need But You Don’t Need All Of It\\nby skipping the MLP sub-layer from the last k layers. The\\nresulting model is Mskip MLP = {(Attentioni, MLPi) | i ∈\\n{1, 2, . . . , L −k}} ∪{(Attentioni, ∅) | i ∈{L −k +\\n1, . . . , L}}.\\n2. Skipping Attention Layers: We construct a model\\nMskip Attention by skipping the attention sub-layer from the\\nlast k layers.\\nThe resulting model is Mskip Attention =\\n{(Attentioni, MLPi)\\n|\\ni\\n∈\\n{1, 2, . . . , L −k}} ∪\\n{(∅, MLPi) | i ∈{L −k + 1, . . . , L}}.\\n3. Skipping Transformer Blocks: We construct a model\\nMskip Attention by skipping the entire last k layers. The re-\\nsulting model is Mskip Block = {(Attentioni, MLPi) | i ∈\\n{1, 2, . . . , L −k}} ∪{(∅) | i ∈{L −k + 1, . . . , L}}.\\nWe then evaluate the performance of these modified models\\non the OpenLLM benchmark (Beeching et al., 2023), com-\\nparing metrics such as accuracy, computational efficiency,\\nand memory usage. This comparison helps in understand-\\ning the individual contributions of the attention and MLP\\nsub-layers to the overall performance of the Transformer\\nmodel.\\n(a) Skip attention lay-\\ners.\\n(b) Skip attention lay-\\ners,\\nkeep last full\\nblock.\\n(c) Skip ffwd layers.\\n(d) Skip ffwd layers,\\nkeep last full block.\\n(e) Skip full blocks.\\n(f) Skip full blocks,\\nkeep last full block.\\nFigure 2. Skip mechanisms for skipping single layers and entire\\nTransformer blocks (ffwd and attention layers) during inference.\\n2.2. Motivation: Are Deeper Layers More Redundant?\\nIn Transformer models, the last layers have been shown to\\ncontribute less information than earlier layers, making it\\npossible to drop those layers at a minimal performance cost\\n(Fan et al., 2019; Zhang & He, 2020; Wang et al., 2022a;\\nSchuster et al., 2022; Kaddour et al., 2023b; Belrose et al.,\\n2023).\\nTo verify this, we experiment with removing either the at-\\ntention sublayers or the MLP sublayers. Figure 1 shows the\\ncosine similarities between a layer’s features and the previ-\\nous layer showing that deeper layers have a lower impact\\non the features than earlier layers. One notable exception\\nto this trend is that the last layer for both Llama-v2 7B and\\n13B has the lowest cosine similarity with the previous layer.\\nPrevious analysis of the attention mechanism has shown\\nthat they can converge to the same value due to attention\\ncollapse (Zhai et al., 2023) and token features that also con-\\nverge to the same value due to over-smoothing (Wang et al.,\\n2022b; Dovonon et al., 2024) or rank collapse (Dong et al.,\\n2023), with solutions to these issues typically improving\\nperformance (Ali et al., 2023; Choi et al., 2024).\\n3. Results\\nExperimental Setup\\nFor all experiments, we use either\\nLlama-v2-7B or Llama-v2-13B (Touvron et al., 2023a;b),\\ntwo LLMs trained on trillions of publically available tokens.\\nWe experiment with keeping 66%, 75%, 90% and 100% of\\nthe network and report the corresponding results in Table 1.\\nWe also experiment with removing attention sublayers in\\nTable 2, MLP sublayers in Table 3, and a varying number of\\nlayers similar to Table 1 but keeping the last layer in Table 4.\\n3.1. Chopping Layers\\nTable 1. Llama-v2 skipping full layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n35.2\\n46.8\\n46.2\\n40.3\\n42.1\\n7B-75%\\n38.3\\n53.0\\n45.1\\n45.9\\n45.6\\n7B-90%\\n47.7\\n69.3\\n39.6\\n46.4\\n50.8\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n37.8\\n46.8\\n45.3\\n51.8\\n45.4\\n13B-75%\\n40.9\\n53.6\\n42.5\\n53.2\\n47.6\\n13B-90%\\n51.3\\n71.3\\n37.1\\n54.8\\n53.6\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nOn all datasets except TruthfulQA, performance drops\\nwhich is expected. It had already been observed that larger\\nlanguage models are less truthful (Lin et al., 2022), but we\\nnow also observe that reducing the size of already trained\\nmodels can also make them more truthful. The observa-\\ntion still holds when the last layer is preserved. Skipping\\n2\\nAttention Is All You Need But You Don’t Need All Of It\\nTable 2. Llama-v2 skipping attention sublayers\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n51.2\\n77.0\\n42.2\\n39.4\\n52.5\\n7B-75%\\n52.5\\n78.3\\n42.3\\n41.4\\n53.6\\n7B-90%\\n52.8\\n78.9\\n40.0\\n44.0\\n53.9\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n55.6\\n80.1\\n40.1\\n51.3\\n56.8\\n13B-75%\\n55.9\\n79.7\\n39.9\\n52.1\\n56.9\\n13B-90%\\n57.0\\n81.3\\n38.2\\n54.8\\n57.8\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nTable 3. Llama-v2 skipping ffwd sublayers\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n35.1\\n52.5\\n42.2\\n43.9\\n43.4\\n7B-75%\\n40.4\\n60.3\\n39.2\\n46.3\\n46.6\\n7B-90%\\n48.5\\n71.4\\n38.0\\n46.1\\n51.0\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n41.6\\n56.9\\n40.7\\n53.4\\n48.2\\n13B-75%\\n47.3\\n65.2\\n40.0\\n53.2\\n51.4\\n13B-90%\\n54.2\\n75.8\\n38.3\\n54.7\\n55.8\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nattention layers only leads to better results with only a 1.8%\\ndecrease in performance when keeping 66% of the network\\ncompared to a 13.1% decrease in performance when drop-\\nping dropping the MLP layers only. This seems to indicate\\nthat MLP layers are more important than attention layers, at\\nleast in deeper parts of the network.\\n3.2. Last Layer Inclusion\\nTable 4. Llama-v2 skip full layers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n32.0\\n45.8\\n46.9\\n40.7\\n41.3\\n7B-75%\\n34.5\\n49.4\\n45.9\\n38.3\\n42.0\\n7B-90%\\n46.5\\n73.1\\n41.8\\n41.4\\n50.7\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n35.1\\n50.0\\n46.9\\n19.1\\n37.8\\n13B-75%\\n38.7\\n56.6\\n43.7\\n25.2\\n41.1\\n13B-90%\\n51.2\\n78.1\\n38.0\\n27.1\\n47.9\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nSurprisingly, we notice that skipping layers except the lat-\\nter layers reduces performances for more layers skipped,\\nexcept for skipping the attention layers. This is even more\\nexaggerated compared to just dropping layers, including the\\nlast one. The reason for this could be attributed to the (lack\\nof) robustness of feedforward sublayers, as the last layer\\nnow has to process perturbed information from earlier lay-\\ners. For future work, it would be interesting to see if these\\nperformance drops can be compensated by a small amount\\nTable 5. Llama-v2 skip attention sublayers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n49.3\\n77.1\\n40.5\\n42.5\\n52.4\\n7B-75%\\n51.8\\n78.3\\n41.1\\n44.1\\n53.8\\n7B-90%\\n51.9\\n78.7\\n39.4\\n45.7\\n53.9\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n56.8\\n82.1\\n38.0\\n50.3\\n56.8\\n13B-75%\\n57.5\\n82.1\\n37.0\\n51.4\\n57.0\\n13B-90%\\n58.9\\n82.4\\n36.6\\n54.5\\n58.1\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nTable 6. Llama-v2 skip ffwd sublayers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n32.0\\n45.8\\n46.9\\n39.4\\n41.0\\n7B-75%\\n34.5\\n49.4\\n45.9\\n40.2\\n42.5\\n7B-90%\\n46.5\\n73.1\\n41.8\\n40.2\\n50.4\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n35.1\\n50.0\\n46.9\\n20.4\\n38.1\\n13B-75%\\n38.7\\n56.6\\n43.7\\n33.6\\n43.2\\n13B-90%\\n51.2\\n78.1\\n38.0\\n34.4\\n50.4\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nof continued training; since model growing techniques for\\ntraining seem to not suffer from instabilities (Kaddour et al.,\\n2023b).\\n3.3. Compute-matched Comparison\\nTo measure the efficiency of the networks we conducted\\na separate experiment, where we record the time it takes\\nfor the model to output a sequence of length 1, averaging\\nover 1000 sequences. We conducted this experiment for\\nboth 50 and 100 length input sequences. We notice that full\\nlayer droppings do improve time costs the best, followed by\\nattention sublayers, and then feedforward sublayers which\\ndo not impact the speed of processing a lot.\\nWe report the time×102 (for clarity) it takes to predict 1\\ntoken for 1000 sequences as well as the percentage improve-\\nment. We show the results of this experiment for Llama 2\\n7B with 0%, 10%, 25%, 33% of layers skipped and we label\\nthese as 7B-100%, 7B-90%, 7B-75%, 7B-66% respectively.\\nTable 7. Llama-v2 time results, 50 length sequence, no last layer\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\n7B-66%\\n31.35\\n32.96\\n36.72\\n21.47\\n43.51\\n6.95\\n7B-75%\\n35.48\\n24.12\\n39.46\\n15.61\\n42.88\\n8.30\\n7B-90%\\n43.31\\n7.38\\n42.93\\n8.19\\n44.17\\n5.53\\n7B-100%\\n46.76\\n0\\n-\\n-\\n-\\n-\\n3\\nAttention Is All You Need But You Don’t Need All Of It\\nTable 8. Llama-v2 time results, 50 length sequence, last layer in-\\ncluded\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\n7B-66%\\n31.78\\n32.04\\n36.92\\n21.04\\n41.31\\n11.66\\n7B-75%\\n34.98\\n25.19\\n40.24\\n13.94\\n42.62\\n8.85\\n7B-90%\\n40.92\\n12.49\\n42.43\\n9.26\\n43.51\\n6.95\\n7B-100%\\n46.76\\n0\\n-\\n-\\n-\\n-\\nTable 9. Llama-v2 time results, 100 length sequence, no last layer\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\n7B-66%\\n32.36\\n32.58\\n38.97\\n18.18\\n43.08\\n10.25\\n7B-75%\\n36.58\\n23.79\\n41.27\\n14.02\\n44.13\\n8.06\\n7B-90%\\n43.65\\n9.06\\n44.62\\n7.04\\n46.30\\n3.54\\n7B-100%\\n48.00\\n0\\n-\\n-\\n-\\n-\\nTable 10. Llama-v2 time results, 100 length sequence, last layer\\nincluded\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\nTime(s) ×102\\n(%)\\n7B-66%\\n32.05\\n33.23\\n38.52\\n19.75\\n42.66\\n11.13\\n7B-75%\\n36.41\\n24.15\\n41.00\\n14.58\\n43.92\\n8.50\\n7B-90%\\n43.28\\n9.83\\n44.27\\n7.77\\n45.20\\n5.83\\n7B-100%\\n48.00\\n0\\n-\\n-\\n-\\n-\\n4. Related Work\\nEarly Exit during inference\\nEarly exit methods have also\\nbeen proposed in other domains (Graves, 2017; Teerapit-\\ntayanon et al., 2017) before getting adapted to autoregressive\\nmodels (Elbayad et al., 2020; Schuster et al., 2022; Din et al.,\\n2023; Elhoushi et al., 2024; Fan et al., 2024; Chen et al.,\\n2024). The idea works by dynamically allocating compute\\nbased on the difficulty of the input sequence. Our method\\nprunes the deepest layers and does not involve any level of\\nadaptability. This is beneficial because it does not require\\nthe entire model to be loaded in memory. Dropping layers\\nduring inference has been done on BERT-like models in\\n(Wang et al., 2022a; Sajjad et al., 2023). We apply a similar\\nanalysis to more recent LLMs and study the impact of skip-\\nping attention and/or MLP layers in more detail. Concurrent\\nwork to ours by Gromov et al. (2024) yields similar results\\nby pruning deeper layers and applying fine-tuning on the\\npruned model.\\nLayer dropping/growing during training\\nThere are var-\\nious works studying the dropping/growing layers dynami-\\ncally during training (Fan et al., 2019; Gong et al., 2019;\\nKaddour et al., 2023b; Jiang et al., 2020; Liu et al., 2023). In\\ncontrast, this work focuses on dropping layers of an already\\npre-trained model in a way similar to Men et al. (2024).\\nOther Inference Speedup Methods\\nOther works to speed\\nup inference include compressing KV caches (Nawrot et al.,\\n2024; Wu & Tu, 2024; Bi et al., 2024), speculative decoding\\n(Chen et al., 2023), efficient memory management (Kwon\\net al., 2023), or subqudratic attention architectures (Fu et al.,\\n2022; Peng et al., 2023; Gu & Dao, 2023), an overview has\\nbeen provided by Kaddour et al. (2023a).\\n5. Conclusion\\nWe investigated the effect of dropping the last layers from\\nthe 7B and 13B Llama2 models. We observe that dropping\\nattention sublayers lead to much lower drops in performance\\nthan dropping the MLP sublayers, whether the last layer\\nis included or not, while also leading to better inference\\nspeedups. For example, removing 33% of attention layers\\nleads to an 18% speedup in a 13B Llama2 model at the cost\\nof a 1.8% drop in average performance. This shows that\\nmassive improvements can be made over dropping entire\\nlayers from just dropping the attention sublayer.\\nReferences\\nAli, A., Galanti, T., and Wolf, L. Centered self-attention\\nlayers, 2023.\\nBeeching,\\nE.,\\nFourrier,\\nC.,\\nHabib,\\nN.,\\nHan,\\nS.,\\nLambert,\\nN.,\\nRajani,\\nN.,\\nSanseviero,\\nO.,\\nTun-\\nstall,\\nL.,\\nand\\nWolf,\\nT.\\nOpen\\nllm\\nleader-\\nboard.\\nhttps://huggingface.co/spaces/\\nHuggingFaceH4/open_llm_leaderboard,\\n2023.\\nBelrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I.,\\nMcKinney, L., Biderman, S., and Steinhardt, J. Eliciting\\nlatent predictions from transformers with the tuned lens.\\narXiv preprint arXiv:2303.08112, 2023.\\nBi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\\nDing, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\\nScaling open-source language models with longtermism.\\narXiv preprint arXiv:2401.02954, 2024.\\nChen, C., Borgeaud, S., Irving, G., Lespiau, J., Sifre, L., and\\nJumper, J. Accelerating large language model decoding\\nwith speculative sampling. CoRR, abs/2302.01318, 2023.\\ndoi: 10.48550/ARXIV.2302.01318. URL https://\\ndoi.org/10.48550/arXiv.2302.01318.\\nChen, Y., Pan, X., Li, Y., Ding, B., and Zhou, J. Ee-llm:\\nLarge-scale training and inference of early-exit large lan-\\nguage models with 3d parallelism, 2024.\\nChoi, J., Wi, H., Kim, J., Shin, Y., Lee, K., Trask, N., and\\nPark, N. Graph convolutions enrich the self-attention in\\ntransformers!, 2024.\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved\\n4\\nAttention Is All You Need But You Don’t Need All Of It\\nquestion answering? try arc, the ai2 reasoning challenge,\\n2018.\\nDin, A. Y., Karidi, T., Choshen, L., and Geva, M. Jump\\nto conclusions: Short-cutting transformers with linear\\ntransformations. arXiv preprint arXiv:2303.09435, 2023.\\nDong, Y., Cordonnier, J.-B., and Loukas, A. Attention\\nis not all you need: Pure attention loses rank doubly\\nexponentially with depth, 2023.\\nDovonon, G. J.-S., Bronstein, M. M., and Kusner, M. J.\\nSetting the record straight on transformer oversmoothing,\\n2024.\\nElbayad, M., Gu, J., Grave, E., and Auli, M. Depth-adaptive\\ntransformer. In International Conference on Learning\\nRepresentations, 2020. URL https://openreview.\\nnet/forum?id=SJg7KhVKPH.\\nElhoushi, M., Shrivastava, A., Liskovich, D., Hosmer, B.,\\nWasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal,\\nS., Roman, A., et al. Layer skip: Enabling early exit\\ninference and self-speculative decoding. arXiv preprint\\narXiv:2404.16710, 2024.\\nFan, A., Grave, E., and Joulin, A. Reducing transformer\\ndepth on demand with structured dropout, 2019.\\nFan, S., Jiang, X., Li, X., Meng, X., Han, P., Shang, S., Sun,\\nA., Wang, Y., and Wang, Z. Not all layers of llms are\\nnecessary during inference, 2024.\\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra,\\nA., and R´e, C. Hungry hungry hippos: Towards lan-\\nguage modeling with state space models. arXiv preprint\\narXiv:2212.14052, 2022.\\nGong, L., He, D., Li, Z., Qin, T., Wang, L., and Liu, T.\\nEfficient training of bert by progressively stacking. In\\nInternational conference on machine learning, pp. 2337–\\n2346. PMLR, 2019.\\nGraves, A. Adaptive computation time for recurrent neural\\nnetworks, 2017.\\nGromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and\\nRoberts, D. A. The unreasonable ineffectiveness of the\\ndeeper layers, 2024.\\nGu, A. and Dao, T. Mamba: Linear-time sequence modeling\\nwith selective state spaces, 2023.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\\nSong, D., and Steinhardt, J. Measuring massive multitask\\nlanguage understanding, 2021.\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\\narXiv:2310.06825, 2023.\\nJiang, Y.-G., Cheng, C., Lin, H., and Fu, Y.\\nLearning\\nlayer-skippable inference network. IEEE Transactions on\\nImage Processing, 29:8747–8759, 2020. doi: 10.1109/\\nTIP.2020.3018269.\\nKaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu,\\nR., and McHardy, R. Challenges and applications of\\nlarge language models. CoRR, abs/2307.10169, 2023a.\\ndoi: 10.48550/ARXIV.2307.10169. URL https://\\ndoi.org/10.48550/arXiv.2307.10169.\\nKaddour, J., Key, O., Nawrot, P., Minervini, P., and Kusner,\\nM. J.\\nNo train no gain: Revisiting efficient training\\nalgorithms for transformer-based language models. In\\nOh, A., Naumann, T., Globerson, A., Saenko, K., Hardt,\\nM., and Levine, S. (eds.), Advances in Neural Information\\nProcessing Systems 36: Annual Conference on Neural\\nInformation Processing Systems 2023, NeurIPS 2023,\\nNew Orleans, LA, USA, December 10 - 16, 2023, 2023b.\\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\\nC. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\\nmemory management for large language model serving\\nwith pagedattention. In Proceedings of the 29th Sym-\\nposium on Operating Systems Principles, pp. 611–626,\\n2023.\\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\\nhow models mimic human falsehoods, 2022.\\nLiu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,\\nShrivastava, A., Zhang, C., Tian, Y., Re, C., and Chen,\\nB. Deja vu: Contextual sparsity for efficient LLMs at\\ninference time. In Krause, A., Brunskill, E., Cho, K.,\\nEngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro-\\nceedings of the 40th International Conference on Ma-\\nchine Learning, volume 202 of Proceedings of Machine\\nLearning Research, pp. 22137–22176. PMLR, 23–29 Jul\\n2023. URL https://proceedings.mlr.press/\\nv202/liu23am.html.\\nMen, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y., Han,\\nX., and Chen, W. Shortgpt: Layers in large language\\nmodels are more redundant than you expect, 2024. URL\\nhttps://arxiv.org/abs/2403.03853.\\nNawrot, P., Ła´ncucki, A., Chochowski, M., Tarjan, D., and\\nPonti, E. M. Dynamic memory compression: Retrofitting\\nllms for accelerated inference, 2024.\\nPatterson, D. A., Gonzalez, J., Le, Q. V., Liang, C., Munguia,\\nL., Rothchild, D., So, D. R., Texier, M., and Dean, J. Car-\\nbon emissions and large neural network training. CoRR,\\n5\\nAttention Is All You Need But You Don’t Need All Of It\\nabs/2104.10350, 2021. URL https://arxiv.org/\\nabs/2104.10350.\\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\\nS., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K.,\\net al. Rwkv: Reinventing rnns for the transformer era.\\narXiv preprint arXiv:2305.13048, 2023.\\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lilli-\\ncrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat,\\nO., Schrittwieser, J., et al. Gemini 1.5: Unlocking multi-\\nmodal understanding across millions of tokens of context.\\narXiv preprint arXiv:2403.05530, 2024.\\nSajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the\\neffect of dropping layers of pre-trained transformer mod-\\nels.\\nComputer Speech & Language, 77:101429, jan\\n2023. doi: 10.1016/j.csl.2022.101429. URL https:\\n//doi.org/10.1016%2Fj.csl.2022.101429.\\nSchuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D.,\\nTran, V., Tay, Y., and Metzler, D. Confident adaptive\\nlanguage modeling. Advances in Neural Information\\nProcessing Systems, 35:17456–17472, 2022.\\nTeerapittayanon, S., McDanel, B., and Kung, H. T.\\nBranchynet: Fast inference via early exiting from deep\\nneural networks, 2017.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\\nple, G. Llama: Open and efficient foundation language\\nmodels, 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\\nand Scialom, T. Llama 2: Open foundation and fine-tuned\\nchat models, 2023b.\\nWang, J., Chen, K., Chen, G., Shou, L., and McAuley, J.\\nSkipbert: Efficient inference with shallow layer skipping.\\nIn Proceedings of the 60th Annual Meeting of the Asso-\\nciation for Computational Linguistics (Volume 1: Long\\nPapers), pp. 7287–7301, 2022a.\\nWang, P., Zheng, W., Chen, T., and Wang, Z.\\nAnti-\\noversmoothing in deep vision transformers via the fourier\\ndomain analysis:\\nFrom theory to practice.\\nIn In-\\nternational Conference on Learning Representations,\\n2022b. URL https://openreview.net/forum?\\nid=O476oWmiNNp.\\nWu, H. and Tu, K. Layer-condensed kv cache for efficient\\ninference of large language models, 2024.\\nXia, H., Yang, Z., Dong, Q., Wang, P., Li, Y., Ge, T., Liu, T.,\\nLi, W., and Sui, Z. Unlocking efficiency in large language\\nmodel inference: A comprehensive survey of speculative\\ndecoding. arXiv preprint arXiv:2401.07851, 2024.\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\\nHellaswag: Can a machine really finish your sentence?,\\n2019.\\nZhai, S., Likhomanenko, T., Littwin, E., Busbridge, D.,\\nRamapuram, J., Zhang, Y., Gu, J., and Susskind, J. Sta-\\nbilizing transformer training by preventing attention en-\\ntropy collapse, 2023.\\nZhang, M. and He, Y. Accelerating training of transformer-\\nbased language models with progressive layer dropping.\\nAdvances in neural information processing systems, 33:\\n14011–14023, 2020.\\n6\\n'),\n",
       " Document(metadata={'Published': '2021-07-16', 'Title': 'All the attention you need: Global-local, spatial-channel attention for image retrieval', 'Authors': 'Chull Hwan Song, Hye Joo Han, Yannis Avrithis', 'Summary': 'We address representation learning for large-scale instance-level image\\nretrieval. Apart from backbone, training pipelines and loss functions, popular\\napproaches have focused on different spatial pooling and attention mechanisms,\\nwhich are at the core of learning a powerful global image representation. There\\nare different forms of attention according to the interaction of elements of\\nthe feature tensor (local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses only one or two\\nforms of attention and applies it to different problems like classification,\\ndetection or retrieval.\\n  We present global-local attention module (GLAM), which is attached at the end\\nof a backbone network and incorporates all four forms of attention: local and\\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\\npooling, we learn a powerful embedding for image retrieval. Focusing on global\\ndescriptors, we provide empirical evidence of the interaction of all forms of\\nattention and improve the state of the art on standard benchmarks.'}, page_content='All the attention you need:\\nGlobal-local, spatial-channel attention for image retrieval\\nChull Hwan Song\\nOdd Concepts\\nHye Joo Han\\nOdd Concepts\\nYannis Avrithis\\nInria, Univ Rennes, CNRS, IRISA\\nAbstract\\nWe address representation learning for large-scale\\ninstance-level image retrieval. Apart from backbone, train-\\ning pipelines and loss functions, popular approaches have\\nfocused on different spatial pooling and attention mecha-\\nnisms, which are at the core of learning a powerful global\\nimage representation. There are different forms of attention\\naccording to the interaction of elements of the feature tensor\\n(local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses\\nonly one or two forms of attention and applies it to different\\nproblems like classiﬁcation, detection or retrieval.\\nWe present global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network and\\nincorporates all four forms of attention: local and global,\\nspatial and channel. We obtain a new feature tensor and, by\\nspatial pooling, we learn a powerful embedding for image\\nretrieval. Focusing on global descriptors, we provide em-\\npirical evidence of the interaction of all forms of attention\\nand improve the state of the art on standard benchmarks.\\n1. Introduction\\nInstance-level image retrieval is at the core of visual rep-\\nresentation learning and is connected with many problems\\nof visual recognition and machine learning, for instance\\nmetric learning [30, 26], few-shot learning [42] and unsu-\\npervised learning [8]. Many large-scale open datasets [3,\\n37, 16, 29, 53], and competitions1 have accelerated progress\\nin instance-level image retrieval, which has been trans-\\nformed by deep learning [3].\\nMany studies on instance-level image retrieval focus\\non learning features from convolutional neural networks\\n(CNN), while others focus on re-ranking, for instance by\\ngraph-based methods [11]. The former can be distinguished\\naccording to feature types: local descriptors, reminiscent of\\nSIFT [27], where an image is mapped to a few hundred vec-\\ntors; and global descriptors, where an image is mapped to a\\n1https://www.kaggle.com/c/landmark-retrieval-2020\\nsingle vector. In fact, deep learning has brought global de-\\nscriptors with astounding performance, while allowing efﬁ-\\ncient search. Our study belongs to this type.\\nStudies on global descriptors have focused on spatial\\npooling [2, 37]. The need for compact, discriminative rep-\\nresentations that are resistant to clutter has naturally given\\nrise to spatial attention methods [24, 28]. Different kinds\\nof attention have been studied in many areas of computer\\nvision research. There is also channel attention [20, 9]; lo-\\ncal attention, applied independently to elements of the rep-\\nresentation (feature map) [54, 25]; global attention, based\\non interaction between elements [52, 9]; and combinations\\nthereof. Unfortunately, each study has been limited to one or\\ntwo kinds of attention only; attention is not always learned;\\nand applications vary.\\nIt is the objective of our work to perform a compre-\\nhensive study of all forms of attention above, apply them\\nto instance-level image retrieval and provide a detailed ac-\\ncount of their interaction and impact on performance. As\\nshown in Figure 1, we collect contextual information from\\nimages with both local and global attention, giving rise to\\ntwo parallel network streams. Importantly, each operates\\non both spatial locations and feature channels. Local at-\\ntention is about individual locations and channels; global is\\nabout interaction between locations and between channels.\\nThe extracted information is separately embedded in local\\nand global attention feature maps, which are combined in a\\nglobal-local attention feature map before pooling.\\nOur contributions can be summarized as follows:\\n1. We propose a novel network that consists of both\\nglobal and local attention for image retrieval. This is\\nthe ﬁrst study that employs both mechanisms.\\n2. Each of the global and local attention mechanisms\\ncomprises both spatial and channel attention.\\n3. Focusing on global descriptors, we provide empirical\\nevidence of the interaction of all forms of attention and\\nimprove the state of the art on standard benchmarks.\\n1\\narXiv:2107.08000v1  [cs.CV]  16 Jul 2021\\nAl\\nc\\nc × 1 × 1\\n×\\n+\\nFl\\nc\\nAl\\ns\\n1 × h × w\\n×\\n+\\nFl\\n×\\nc × h × w\\nF\\n×\\n+\\nc × h × w\\nFgl\\nAg\\nc\\nc × c\\n×\\nFg\\nc\\nAg\\ns\\nhw × hw\\n×\\n+\\nFg\\n×\\nwl\\nw\\nwg\\nchannel attention\\nspatial attention\\nfusion\\nlocal attention\\nglobal attention\\nFigure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-\\ntion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention\\n(based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al\\nc),\\nlocal spatial (Al\\ns), global channel (Ag\\nc) and global spatial (Ag\\ns). The input feature map F is weighted into local (Fl) and\\nglobal (Fg) attention feature maps, which are fused with F to yield the global-local attention feature map Fgl. The diagram\\nis abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\\n2. Related work\\nInstance-level image retrieval\\nStudies on instance-level\\nimage retrieval can be roughly, but not exclusively, di-\\nvided into three types: (1) studies on global descriptors\\n[3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and\\ngeometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking\\nby graph-based methods [11, 21, 55]. The ﬁrst two types\\nof studies focus on the feature representation, while the last\\ntype focuses on re-ranking extracted features.\\nStudies on global descriptors focus on spatial pooling\\nof CNN feature maps into vectors, including MAC [38],\\nSPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37],\\nand NetVLAD [1, 25], as well as learning the representa-\\ntion [3, 15, 16, 36, 37]. Studies before deep learning dom-\\ninated image retrieval were mostly based on local descrip-\\ntors like SIFT [27] and bag-of-words representation [32] or\\naggregated descriptors like VLAD [22] or ASMK [46]. Lo-\\ncal descriptors have been revived in deep learning, e.g. with\\nDELF [29], DELG [5] and ASMK extensions [45, 47].\\nWe focus on learning a global descriptor in this work, be-\\ncause it is the most efﬁcient in terms of storage and search.\\nHowever, our generic attention mechanism produces a fea-\\nture tensor and could be applicable to local descriptors as\\nwell, if global pooling were replaced by local feature detec-\\ntion. Re-ranking methods are complementary to the repre-\\nsentation and we do not consider them in this work.\\nAttention\\nAttention mechanisms have been ﬁrst proposed\\nin image classiﬁcation studies focusing on channel at-\\nMETHOD\\nLOCAL\\nGLOBAL\\nLRN RET\\nSpatial Channel Spatial Channel\\nSENet [20]\\n✓\\n✓\\nECA-Net [51]\\n✓\\n✓\\nGCNet [6]\\n✓\\n✓\\nCBAM [54]\\n✓\\n✓\\n✓\\nGE [19]\\n✓\\n✓\\nNL-Net [52]\\n✓\\n✓\\nAA-Net [4]\\n✓\\n✓\\nSAN [59]\\n✓\\n✓\\nN3Net [34]\\n✓\\n✓\\nA2-Net [9]\\n✓\\n✓\\nGSoP [14]\\n✓\\n✓\\nOnA [23]\\n✓\\n✓\\nAGeM [17]\\n✓\\n✓\\nCroW [24]\\n✓\\n✓\\n✓\\nCRN [25]\\n✓\\n✓\\n✓\\nDELF [29]\\n✓\\n✓\\n✓\\nDELG [5]\\n✓\\n✓\\n✓\\nTolias et al. [47]\\n✓\\n✓\\n✓\\nSOLAR [28]\\n✓\\n✓\\n✓\\nOurs\\n✓\\n✓\\n✓\\n✓\\n✓\\n✓\\nTable 1: Related work on attention. LRN: learned; RET: ap-\\nplied to instance-level image retrieval.\\ntention [20, 51, 6], spatial attention [19] or both, like\\nCBAM [54]. In image retrieval, CroW [24] also employs\\n2\\nfeature map\\nGAP\\nconv1d(k)\\nsigmoid\\nattention map\\nc × h × w\\nc × 1 × 1\\nc × 1 × 1\\nF\\nAl\\nc\\nFigure 2: Local channel attention.\\nboth spatial and channel attention and can be seen as a pre-\\ncursor of CBAM, but, like other studies of spatial attention\\non retrieval [41, 23, 17], it is not learned. CRN [25] ap-\\nplies spatial attention for feature reweighting and is learned.\\nLearned spatial attention mechanisms are common for local\\ndescriptors [29, 5, 47].\\nWe call the above methods local attention, in the sense\\nthat elements of the feature tensor (channels / spatial loca-\\ntions), are weighted independently, based on contextual in-\\nformation obtained by pooling or learned. By constrast, by\\nglobal attention we refer to mechanisms that model inter-\\naction between elements of the feature tensor, for example\\nbetween channels or between locations.\\nIn image classiﬁcation, non-local neural network (NL-\\nNet) [52] is maybe the ﬁrst global attention mechanism, fol-\\nlowed by similar studies [4, 59, 34]. It is global spatial at-\\ntention, allowing interaction between any pair of spatial lo-\\ncations. Similarly, there are studies of global channel atten-\\ntion, allowing interaction between channels [9, 14]. Global\\nattention has focused mostly on image recognition and has\\nbeen applied to either spatial or channel attention so far, not\\nboth. In image retrieval, SOLAR [28] is a direct application\\nof the global spatial attention mechanism of [52].\\nTable 1 attempts to categorize related work on atten-\\ntion according to whether attention is local or global, spa-\\ntial or channel, whether it is learned and whether it is ap-\\nplied to instance-level image retrieval. We observe that all\\nmethods limit to one or two forms of attention only. Of\\nthose studies that focus on image retrieval, many are not\\nlearned [23, 17, 24], and of those that are, some are de-\\nsigned for local descriptors [29, 47].\\nBy contrast, we provide a comprehensive study of all\\nforms of attention, global and local, spatial and channel, to\\nobtain a learned representation in the form of a tensor that\\ncan be used in any way. We spatially pool it into a global\\ndescriptor and we study the relative gain of different forms\\nof attention in image retrieval.\\nfeature map\\nconv 1 × 1\\nconv 3 × 3\\nconv 5 × 5\\nconv 7 × 7\\nconcat\\nconv 1 × 1\\nattention map\\nc × h × w\\n4c′ × h × w\\n1 × h × w\\nc′ × h × w\\ndilated\\nconv\\nF\\nF′\\nAl\\ns\\nFigure 3: Local spatial attention. Convolutional layers in\\nblue implemented by dilated convolutions with kernel size\\n3 × 3 and dilation factors 1, 3, 5.\\n3. Global-local attention\\nWe design a global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network. Figure 1\\nillustrates its main components. We are given a c × h × w\\nfeature tensor F, where c is the number of channels, and\\nh × w is the spatial resolution. Local attention collects con-\\ntext from the image and applies pooling to obtain a c×1×1\\nlocal channel attention map Al\\nc and a 1 × h × w local spa-\\ntial attention map Al\\ns. Global attention allows interaction\\nbetween channels, resulting in a c × c global channel at-\\ntention map Ag\\nc, and between spatial locations, resulting in\\na hw × hw global spatial attention map Ag\\ns. The feature\\nmaps produced by the two attention streams are combined\\nwith the original one by a learned fusion mechanism into\\nthe global-local attention feature map Fgl before being spa-\\ntially pooled into a global image descriptor.\\n3.1. Local attention\\nWe extract an 1D channel and a 2D spatial attention map\\nto weigh the feature map in the corresponding dimensions.\\nLocal channel attention\\nFollowing ECA-Net [51], this\\nattention captures local channel information. As shown in\\nFigure 2, we are given a c×h×w feature tensor F from our\\nbackbone. We ﬁrst reduce it to a c × 1 × 1 tensor by global\\naverage pooling (GAP). Channel attention is then captured\\nby a 1D convolution of kernel size k along the channel di-\\nmension, where k controls the extent of cross-channel inter-\\naction. This is followed by a sigmoid function, resulting in\\nthe c × 1 × 1 local channel attention map Al\\nc.\\nLocal spatial attention\\nInspired by the inception mod-\\nule [43] and similar to [25], this attention map captures local\\nspatial information at different scales. As shown in Figure 3,\\n3\\nfeature map\\nGAP\\nconv1d(k)\\nconv1d(k)\\nsigmoid\\nsigmoid\\n×\\n×\\nsoftmax\\nattention feature map\\n1 × c\\n1 × c\\n1 × c\\nQc\\nc × c\\nhw × c\\nVc\\nAg\\nc\\nc × h × w\\n1 × c\\n1 × c\\nKc\\nF\\nGc\\nFigure 4: Global channel attention.\\ngiven the same c × h × w feature tensor F from our back-\\nbone, we obtain a new tensor F′ with channels reduced to\\nc′, using a 1 × 1 convolution. We then extract local spatial\\ncontextual information using convolutional ﬁlters of kernel\\nsize 3 × 3, 5 × 5, and 7 × 7, which are efﬁciently imple-\\nmented by 3 × 3 dilated convolutions [7, 57] with dilation\\nparameter 1, 2, and 3 respectively. The resulting features,\\nalong with one obtained by 1 × 1 convolution on F′, are\\nconcatenated into a 4c′ × h × w tensor. Finally, we obtain\\nthe 1 × h × w local spatial attention map Al\\ns by a 1 × 1\\nconvolution that reduces the channel dimension to 1.\\nThe middle column of Figure 6 shows heat maps of local\\nspatial attention, localizing target objects in images.\\nLocal attention feature map\\nWe use the local channel\\nattention map Al\\nc to weigh F in the channel dimension\\nFl\\nc := F ⊙Al\\nc + F.\\n(1)\\nWe then use local spatial attention map Al\\ns to weigh Fl\\nc\\nin the spatial dimensions, resulting in the c × h × w local\\nattention feature map\\nFl = Fl\\nc ⊙Al\\ns + Fl\\nc.\\n(2)\\nHere, A⊙B denotes an element-wise multiplication of ten-\\nsors A and B, with broadcasting when one tensor is smaller.\\nWe adopt the choice of applying channel followed by spa-\\ntial attention from convolutional block attention module\\nCBAM [54]. However, apart from computing Al\\ns at differ-\\nent scales, both attention maps are obtained from the orig-\\ninal tensor F rather than sequentially. In addition, both (1)\\nand (2) include residual connections, while CBAM includes\\na single residual connection over both steps.\\n3.2. Global attention\\nWe extract two matrices capturing global pairwise chan-\\nnel and spatial interaction to weigh the feature map.\\nfeature map\\nconv 1 × 1\\nconv 1 × 1\\nconv 1 × 1\\n×\\n×\\nsoftmax\\nconv 1 × 1\\nattention feature map\\nc′ × hw\\nQs\\nhw × hw\\nc × h × w\\nc′ × hw\\nVs\\nc′ × h × w\\nAg\\ns\\nc × h × w\\nc′ × hw\\nKc\\nF\\nGs\\nFigure 5: Global spatial attention.\\nGlobal channel attention\\nWe introduce a global channel\\nattention mechanism that captures global channel interac-\\ntion. This mechanism is based on the non-local neural net-\\nwork [52], but with the idea of 1D convolution from ECA-\\nNet [51]. As shown in Figure 4, we are given the c × h × w\\nfeature tensor F from our backbone. We apply GAP and\\nsqueeze spatial dimensions, followed by a 1D convolution\\nof kernel size k and a sigmoid function, to obtain 1×c query\\nQc and key Kc tensors. The value tensor Vc is obtained by\\nmere reshaping of F to hw×c, without GAP. Next, we form\\nthe outer product of Kc and Qc, followed by softmax over\\nchannels to obtain a c × c global channel attention map\\nAg\\nc = softmax(Kc\\n⊤Qc).\\n(3)\\nFinally, this attention map is multiplied with Vc and the ma-\\ntrix product VcAg\\nc is reshaped back to c×h×w to give the\\nglobal channel attention feature map Gc. In GSoP [14] and\\nA2-Net [9], a c×c global channel attention map is obtained\\nby multiplication of hw × c matrices; (3) is more efﬁcient,\\nusing only an outer product of 1 × c vectors.\\nGlobal spatial attention\\nSince ordinary convolution ap-\\nplies only a local neighborhood at a time, it cannot capture\\nglobal contextual information. Thus, we apply non-local ﬁl-\\ntering [52], which is a form of self-attention [49] in the spa-\\ntial dimensions. As shown in Figure 5, we are given the\\nsame c × h × w feature tensor F from our backbone. By\\nusing three 1×1 convolutions, which reduce channels to c′,\\nand ﬂattening spatial dimensions to hw, we obtain c′ × hw\\nquery Qs, key Ks, and value Vs tensors, where each col-\\numn is a feature vector corresponding to a particular spatial\\nlocation. We capture pairwise similarities of these vectors\\nby matrix multiplication of Ks and Qs, followed by soft-\\nmax over locations to obtain a hw × hw global spatial at-\\ntention map:\\nAg\\ns = softmax(K⊤\\ns Qs).\\n(4)\\n4\\nThis attention map is multiplied with Vs and the matrix\\nproduct VsAg\\ns is reshaped back to c′ ×h×w by expanding\\nthe spatial dimensions. Finally, using a 1 × 1 convolution,\\nwhich increases channels back to c, we obtain the c×h×w\\nglobal spatial attention feature map Gs.\\nThe right column of Figure 6 shows heat maps for global\\nspatial attention, localizing target objects in images.\\nGlobal attention feature map\\nWe use the global channel\\nattention feature map Fc to weigh F element-wise\\nFg\\nc = F ⊙Gc.\\n(5)\\nWe then use global spatial attention feature map Gs to\\nweigh Fg\\nc element-wise, resulting in the c × h × w global\\nattention feature map\\nFg = Fg\\nc ⊙Gs + Fg\\nc.\\n(6)\\nSimilarly to Fl in (1) and (2), we apply channel attention\\nﬁrst, followed by spatial attention. However, unlike (1),\\nthere is no residual connection in (5). This choice is sup-\\nported by early experiments.\\n3.3. Global-local attention\\nFeature fusion\\nAs shown in Figure 1, we combine the\\nlocal and global attention feature maps, Fl and Fg, with\\nthe original feature F. While concatenation and summation\\nare common operations for feature combination, we use a\\nweighted average with weights wl, wg, w respectively, ob-\\ntained by softmax over three learnable scalar parameters, to\\nobtain a c × h × w global-local attention feature map\\nFgl = wlFl + wgFl + wF.\\n(7)\\nEfﬁcientDet [44] has shown that this is the most effective,\\namong a number of choices, for fusion of features across\\ndifferent scales.\\nPooling\\nWe apply GeM [37], a learnable spatial pooling\\nmechanism, to feature map Fgl (7), followed by a fully-\\nconnected (FC) layer with dropout and batch normalization.\\nThe ﬁnal embedding is obtained by ℓ2-normalization.\\n4. Experiments\\n4.1. Datasets\\nTraining set\\nThere are a number of open landmark\\ndatasets commonly used for training in image retrieval stud-\\nies, including neural code (NC) [3], neural code clean (NC-\\nclean) [16], as well as Google Landmarks v1 (GLDv1) [29]\\nand v2 (GLDv2) [53]. Table 2 shows relevant statistics.\\nThese datasets can be categorized into noisy and clean. The\\nclean sets were obtained from the original noisy sets for\\nmore effective training [16, 53]. The original noisy datasets\\nare much larger, but they have high intra-class variability.\\n(a) input\\n(b) local\\n(c) global\\nFigure 6: Local and global spatial attention. Left: input\\nimages. Middle: local spatial attention heat maps. Right:\\nglobal spatial attention heat maps. Red (blue) means higher\\n(lower) attention weight.\\nEach class can include visually dissimilar images such as\\nexterior and interior views of a building or landmark, in-\\ncluding ﬂoor plans and paintings inside. The clean datasets\\nfocus on views directly relevant to landmark recognition but\\nhave a much smaller number of images.\\nEvaluation set and metrics\\nWe use four common eval-\\nuation datasets for landmark image retrieval: Oxford5k\\n(Ox5k) [32], Paris6k (Par6k) [33], as well as Revisited Ox-\\nford (ROxford or ROxf) and Paris (RParis or RPar) [35].\\nROxford and RParis are used with and without one million\\ndistractors (R1M) [28] and evaluated using the Medium and\\nHard protocols [35]. We evaluate using mean Average Pre-\\ncision (mAP) and mean precision at 10 (mP@10).\\n4.2. Implementation details\\nWe train on 8 TITAN RTX 2080Ti GPUs. All models are\\npre-trained on ImageNet [39] and implemented in PyTorch\\n[31]. For fair comparisons, we set a training environment\\n5\\nFigure 7: Examples of our ranking results. In each row, the ﬁrst image on the left (pink dotted outline) is a query image with a\\ntarget object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images\\nfor the query; red solid outline: negative.\\nsimilar to the those of compared studies [56, 53, 28, 35]. We\\nemploy ResNet101 [18] as a backbone model. The kernel\\nsize k of ECANet in subsection 3.1 is set to 3. The param-\\neter p of GeM in subsection 3.3 is set to 3 and the dimen-\\nsion d of ﬁnal embeddings to 512. We adopt ArcFace [10],\\na cosine-softmax based loss, with a margin of 0.3. We use\\nstochastic gradient descent with initial learning rate 10−3,\\nmomentum 0.9 and weight decay 10−5.\\nWe adopt the batch sampling of Yokoo et al. [56] where\\nmini-batch samples with similar aspect ratios are resized to\\na particular size. Here, we use a batch size of 64. For image\\naugmentation, we apply scaling, random cropping, and var-\\nied illumination. At inference, we apply a multi-resolution\\nrepresentation [16] to query and database images.\\nOur method is denoted as GLAM (global-local atten-\\ntion module). Using the backbone model alone is referred\\nto as baseline. It is compatible with recent models based\\non ResNet101-GeM trained with ArcFace [53, 28]. Adding\\nour local attention (subsection 3.1) to the baseline model is\\ndenoted +local, while adding our global attention (subsec-\\ntion 3.2) is denoted +global. Since we focus on representa-\\ntion learning, we do not consider post-processing methods\\nlike geometry-based re-ranking [29, 40, 53] or graph-based\\nre-ranking [11, 21, 55].\\n4.3. Benchmarking\\nNoisy vs. clean training sets\\nWe begin by training our\\nbest model (baseline+local+global) on all training sets of\\nTable 2, except NC-noisy because some images are cur-\\nrently unavailable. As shown in Table 3, even though\\nTRAIN SET\\n#IMAGES\\n#CLASSES\\nNC-noisy\\n213,678\\n672\\nNC-clean\\n27,965\\n581\\nSfM-120k\\n117,369\\n713\\nGLDv1-noisy\\n1,225,029\\n14, 951\\nGLDv2-noisy\\n4,132,914\\n203,094\\nGLDv2-clean\\n1,580,470\\n81,313\\nTable 2: Statistics of different training sets.\\nMETHOD\\nTRAIN SET\\nDIM OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGeM-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7\\n77.2\\n38.5\\n56.3\\nSOLAR [28]\\nGLDv1-noisy 2048\\n–\\n–\\n69.9\\n81.6\\n47.9\\n64.5\\nGLDv2 [53]\\nGLDv2-clean 2048\\n–\\n–\\n74.2\\n84.9\\n51.6\\n70.3\\nGLAM (Ours)\\nNC-clean\\n512\\n77.8\\n85.8\\n51.6\\n68.1\\n20.9\\n44.7\\nGLDv1-noisy 512\\n92.8\\n95.0\\n73.7\\n83.5\\n49.8\\n69.4\\nGLDv2-noisy 512\\n93.3\\n95.3\\n75.7\\n86.0\\n53.1\\n73.8\\nGLDv2-clean 512\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 3: mAP comparison of our best model (base-\\nline+local+global) trained on different training sets against\\n[53, 28]. All models use ResNet101-GeM. Red: best results.\\nBlue: GLAM higher than SOLAR [28] on GLDv1-noisy.\\nGLDv2-noisy has 2.6 times more images than GLDv2-\\nclean, the latter is superior by a large margin. This shows\\nthat, in training, a cleaner dataset can be more important\\nthan a larger one. By contrast, NC-clean has the worst\\nperformance despite being clean, aparently because it is\\n6\\nMETHOD\\nTRAIN SET\\nDIM\\nBASE\\nMEDIUM\\nHARD\\nOx5k Par6k\\nROxf\\n+R1M\\nRPar\\n+R1M\\nROxf\\n+R1M\\nRPar\\n+R1M\\nmAP\\nmAP mAP mP mAP mP mAP mP mAP mP\\nmAP mP mAP mP mAP mP mAP mP\\nSPoC-V16 [2, 35]\\n[O]\\n512\\n53.1∗\\n–\\n38.0 54.6 17.1 33.3 59.8 93.0 30.3 83.0 11.4 20.9\\n0.9\\n2.9\\n32.4 69.7\\n7.6\\n30.6\\nSPoC-R101 [35]\\n[O]\\n2048\\n–\\n–\\n39.8 61.0 21.5 40.4 69.2 96.7 41.6 92.0 12.4 23.8\\n2.8\\n5.6\\n44.7 78.0 15.3 54.4\\nCroW-V16 [24, 35]\\n[O]\\n512\\n70.8\\n79.7\\n41.4 58.8 22.5 40.5 62.9 94.4 34.1 87.1 13.9 25.7\\n3.0\\n6.6\\n36.9 77.9 10.3 45.1\\nCroW-R101 [35]\\n[O]\\n2048\\n–\\n–\\n42.4 61.9 21.2 39.4 70.4 97.1 42.7 92.9 13.3 27.7\\n3.3\\n9.3\\n47.2 83.6 16.3 61.6\\nMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.0\\n82.9\\n37.8 57.8 21.8 39.7 59.2 93.3 33.6 87.1 14.6 27.0\\n7.4\\n11.9 35.9 78.4 13.2 54.7\\nMAC-R101-Siamese [35]\\n[O]\\n2048\\n–\\n–\\n41.7 65.0 24.2 43.7 66.2 96.4 40.8 93.0 18.0 32.9\\n5.7\\n14.4 44.1 86.3 18.2 67.7\\nRMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.1\\n85.0\\n42.5 62.8 21.7 40.3 66.2 95.4 39.9 88.9 12.0 26.1\\n1.7\\n5.8\\n40.9 77.1 14.8 54.0\\nRMAC-R101-Siamese [35]\\n[O]\\n2048\\n–\\n–\\n49.8 68.9 29.2 48.9 74.0 97.7 49.3 93.7 18.5 32.2\\n4.5\\n13.0 52.1 87.1 21.3 67.4\\nRMAC-R101-Triplet [16, 35]\\nNC-clean\\n2048\\n86.1\\n94.5\\n60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0\\nGeM-R101-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3\\nAGeM-R101-Siamese [17]\\nSfM-120k\\n2048\\n–\\n–\\n67.0\\n–\\n–\\n–\\n78.1\\n–\\n–\\n–\\n40.7\\n–\\n–\\n–\\n57.3\\n–\\n–\\n–\\nSOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048\\n–\\n–\\n69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6\\nDELG-GeM-R101-ArcFace [5]\\nGLDv1-noisy 2048\\n–\\n–\\n73.2\\n–\\n54.8\\n–\\n82.4\\n–\\n61.8\\n–\\n51.2\\n–\\n30.3\\n–\\n64.7\\n–\\n35.5\\n–\\nGeM-R101-ArcFace [53]\\nGLDv2-clean 2048\\n–\\n–\\n74.2\\n–\\n–\\n–\\n84.9\\n–\\n–\\n–\\n51.6\\n–\\n–\\n–\\n70.3\\n–\\n–\\n–\\nGLAM-GeM-R101-ArcFace baseline\\nGLDv2-clean\\n512\\n91.9\\n94.5\\n72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7\\n+local\\nGLDv2-clean\\n512\\n91.2\\n95.4\\n73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1\\n+global\\nGLDv2-clean\\n512\\n92.3\\n95.3\\n77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0\\n+global+local\\nGLDv2-clean\\n512\\n94.2\\n95.6\\n78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0\\nTable 4: mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16:\\nVGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet). ∗: dimension d = 256 [2]. mP: mP@10. Red:\\nbest results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. [53] is the only\\nmodel other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3.\\ntoo small. To achieve best possible performance, we use\\nGLDv2-clean as a training set in the remaining experiments.\\nComparisons on same training set\\nIt is common to com-\\npare methods regardless of training sets as more become\\navailable, e.g., [35, 28]. Since GLDv2-clean is relatively\\nnew, Weyand et al. [53], which introduced the dataset, is the\\nonly study that has trained the same backbone with the same\\nsettings (ResNet101-GeM with ArcFace) on GLDv2-clean.\\nOur baseline is lower than [53], because our dimensinality is\\n512, while other models based on ResNet101 use 2048. Yet,\\nTable 3 shows that our best model trained on GLDv2-clean\\noutperforms [53] by a large margin. But the most impor-\\ntant comparison is with SOLAR [28], also based on self-\\nattention, which has trained ResNet101-GeM on GLDv1-\\nnoisy. On this training set, our best model clearly outper-\\nforms [28] despite lower dimensionality.\\nComparison with state of the art\\nTable 4 shows the\\nperformance of four variants of our model, i.e. baseline\\nwith or without local/global attention, and compares them\\nagainst state-of-the-art (SOTA) methods based on global de-\\nscriptors without re-ranking on the complete set of bench-\\nmarks, including distractors. Both local and global atten-\\ntion bring signiﬁcant gain over the baseline. The effect\\nof global is stronger, while the gain of the two is addi-\\ntive in the combination. The best results are achieved by\\nthe global-local attention network (baseline+global+local).\\nWith this model, we outperform previous best methods\\non most benchmarks except mP@10 on RParis (medium)\\nand RParis+R1M (medium), where we are outperformed\\nby [37, 35]. These results demonstrate that our approach is\\neffective for landmark image retrieval. Figure 7 shows some\\nMETHOD\\nOXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGLAM baseline\\n91.9\\n94.5\\n72.8\\n84.2\\n49.9\\n69.7\\n+local-channel\\n91.3\\n95.3\\n72.2\\n85.8\\n48.3\\n73.1\\n+local-spatial\\n91.0\\n95.1\\n72.1\\n85.3\\n48.3\\n71.9\\n+local\\n91.2\\n95.4\\n73.7\\n86.5\\n52.6\\n75.0\\n+global-channel\\n92.5\\n94.4\\n73.3\\n84.4\\n49.8\\n70.1\\n+global-spatial\\n92.4\\n95.1\\n73.2\\n86.3\\n50.0\\n72.7\\n+global\\n92.3\\n95.3\\n77.2\\n86.7\\n57.4\\n75.0\\n+global+local\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 5: mAP comparison of spatial and channel variants\\nof our local (+local, subsection 3.1) and global (+global,\\nsubsection 3.1) attention modules to the baseline.\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nCBAM style\\n93.8\\n95.7\\n75.6\\n88.4\\n53.3\\n76.8\\nGLAM (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 6: mAP comparison between CBAM style and our\\nlocal spatial attention.\\nexamples of our ranking results.\\n4.4. Ablation study\\nOur ablation study uses the Google Landmark v2 clean\\ndataset (GLDv2-clean) [53] for training, which is shown to\\nbe the most effective in Table 3.\\n7\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf\\nRPar\\nROxf\\nRPar\\nConcatenate\\n89.5\\n95.1\\n73.6\\n86.5\\n54.0\\n73.7\\nSum (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 7: mAP comparison between weighted concatenation\\nand weighted average for feature fusion.\\nMETHOD\\nOXF5K PAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nFixed-size\\n76.1\\n82.6\\n55.7\\n68.4\\n29.2\\n47.5\\nGroup-size (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 8: mAP comparison between ﬁxed-size (224 × 224)\\nand group-size sampling methods.\\nQUERY DATABASE OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nSingle\\nSingle\\n93.3\\n95.2\\n76.9\\n87.1\\n58.6\\n74.7\\nMulti\\nSingle\\n93.9\\n95.4\\n78.0\\n87.7\\n59.0\\n75.5\\nSingle\\nMulti\\n93.6\\n95.6\\n77.0\\n87.8\\n57.1\\n76.0\\nMulti\\nMulti\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 9: mAP comparison of using multiresolution repre-\\nsentation (Multi) or not (Single) on query or database.\\nEffect of attention modules\\nWe ablate the effect of our\\nlocal and global attention networks as well as their com-\\nbination. Table 5 shows the results, which are more ﬁne-\\ngrained than those of Table 4. In particular, it shows the ef-\\nfect of the channel and spatial variants of both local and\\nglobal attention. We observe that, when used alone, the\\nchannel and spatial variants of local attention are harmful\\nin most cases. Even the combination, baseline+local, is not\\nalways effective. By contrast, when used alone, the channel\\nand spatial variants of global attention are mostly beneﬁcial,\\nespecially the latter. Their combination, baseline+global, is\\nimpressive, bringing gain of up to 7.5%. Importantly, the\\ncombination baseline+global+local improves further by up\\nto another 2.8%. This result shows the necessity of local\\nattention in the ﬁnal model.\\nCBAM vs. our local spatial attention\\nWe experiment\\nwith the local spatial attention of CBAM [54]. CBAM ap-\\nplies average and max-pooling to input features and con-\\ncatenates the two for spatial attention. We apply this vari-\\nant to our local spatial attention module for comparison.\\nFor the CBAM style module, we keep the overall design\\nof our module as shown in Figure 3, but apply average and\\nmax-pooling to each of the four convolutional layer outputs\\nbefore concatenation. Table 6 shows that the CBAM style\\nmodule is considerably worse than ours on all benchmarks\\nexcept Paris6k, where it is only slightly better.\\nConcatenation vs. sum for feature fusion\\nWe use a\\nsoftmax-based weighted average of local and global atten-\\ntion feature maps with the original feature map (7). Here,\\nwe compare this weighted average with weighted concate-\\nnation, where concatenation replaces the sum operation\\nin (7). As shown in Table 7, the weighted average outper-\\nforms the weighted concatenation.\\nFixed-size vs. group-size sampling\\nNumerous studies\\nhave proposed methods for constructing batches according\\nto image size for efﬁcient training. For instance, Gordo et\\nal. [16], DELF [29], and Yokoo et al. [56] employed dif-\\nferent image sizes per batch for training instead of a single\\nﬁxed size. We adopt the method of Yokoo et al., which con-\\nstructs a batch with images of similar aspect ratio, so that\\nthe images can be resized to a size with an aspect ratio that\\nis similar to their own. We call this method group-size sam-\\npling. Table 8 compares ﬁxed-size (224 × 224) with group-\\nsize sampling. We observe that maintaining aspect ratios by\\nusing dynamic input sizes is much more effective.\\nMulti-resolution\\nWe use the multi-resolution representa-\\ntion [16] for the ﬁnal feature of an image at inference time.\\nThis method: (1) resizes an image into multiple scales; (2)\\nextracts features from the resized images; and (3) averages\\nthe features to obtain the ﬁnal feature of the image. The\\nmethod is applied to both query and database images to en-\\nhance ranking results, especially for small target objects.\\nTable 9 compares the four cases of applying this method or\\nnot to query or database images.\\n5. Conclusion\\nWe have introduced a novel approach that extracts global\\nand local contextual information using attention mecha-\\nnisms for instance-level image retrieval. It is manifested as\\na network architecture consisting of global and local atten-\\ntion components, each operating on both spatial and chan-\\nnel dimensions. This constitutes a comprehensive study and\\nempirical evaluation of all four forms of attention that have\\npreviously been studied only in isolation. Our ﬁndings indi-\\ncate that the gain (or loss) brought by one form of attention\\nalone strongly depends on the presence of the others, with\\nthe maximum gain appearing when all forms are present.\\nThe output is a modiﬁed feature tensor that can be used in\\nany way, for instance with local feature detection instead of\\nspatial pooling for image retrieval.\\nWith the advent of vision transformers [12, 58] and their\\nrecent application to image retrieval [13], attention is ex-\\npected to play a more and more signiﬁcant role in vi-\\nsion. According to our classiﬁcation, transformers perform\\nglobal spatial attention alone. It is of great interest to in-\\nvestigate the role of the other forms of attention, where our\\n8\\napproach may yield a basic building block of such archi-\\ntectures. One may even envision an extension to language\\nmodels, where transformers originate from [50].\\nReferences\\n[1] Relja Arandjelovi´c, Petr Gronat, Akihiko Torii, Tomas Pa-\\njdla, and Josef Sivic.\\nNetVLAD: CNN architecture for\\nweakly supervised place recognition. In CVPR, 2016. 2\\n[2] Artem Babenko and Victor Lempitsky. Aggregating Local\\nDeep Features for Image Retrieval. In ICCV, 2015. 1, 2, 7\\n[3] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and\\nVictor Lempitsky.\\nNeural Codes for Image Retrieval.\\nIn\\nECCV, 2014. 1, 2, 5\\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\\nand Quoc V. Le.\\nAttention augmented convolutional net-\\nworks. In ICCV, 2019. 2, 3\\n[5] Bingyi Cao, Andr´e Araujo, and Jack Sim. Unifying deep\\nlocal and global features for image search. In ECCV, 2020.\\n2, 3, 7\\n[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.\\nGCNet: Non-Local Networks Meet Squeeze-Excitation Net-\\nworks and Beyond. In ICCV, 2019. 2\\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\nHartwig Adam. Rethinking atrous convolution for seman-\\ntic image segmentation. arXiv preprint arXiv:1706.05587,\\n2017. 4\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In ICML, 2020. 1\\n[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\\nYan, and Jiashi Feng. Aˆ2-nets: Double attention networks.\\nIn NeurIPS, 2018. 1, 2, 3, 4\\n[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou. ArcFace: Additive Angular Margin Loss for Deep\\nFace Recognition. In CVPR, 2019. 6\\n[11] Michael Donoser and Horst Bischof. Diffusion Processes for\\nRetrieval Revisited. In CVPR, 2013. 1, 2, 6\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale.\\narXiv preprint\\narXiv:2010.11929, 2020. 8\\n[13] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\\nHerv´e J´egou.\\nTraining vision transformers for image re-\\ntrieval. Technical report, 2021. 8\\n[14] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global\\nsecond-order pooling convolutional networks.\\nIn CVPR,\\n2019. 2, 3, 4\\n[15] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. Deep image retrieval: Learning global representations\\nfor image search. In ECCV, 2016. 2\\n[16] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. End-to-end learning of deep visual representations for\\nimage retrieval. IJCV, 2017. 1, 2, 5, 6, 7, 8\\n[17] Yinzheng Gu, Chuanpeng Li, and Jinbin Xie.\\nAttention-\\naware generalized mean pooling for image retrieval. arXiv\\npreprint arXiv:1811.00202, 2018. 2, 3, 7\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition.\\nIn CVPR,\\n2016. 6\\n[19] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\\nVedaldi. Gather-excite: Exploiting feature context in con-\\nvolutional neural networks. In NeurIPS, 2018. 2\\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.\\nSqueeze-and-Excitation Networks. In CVPR, 2018. 1, 2\\n[21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,\\nand Ondrej Chum. Efﬁcient diffusion on region manifolds:\\nRecovering small objects with compact cnn representations.\\nIn CVPR, 2017. 2, 6\\n[22] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\\nC. Schmid. Aggregating local image descriptors into com-\\npact codes. PAMI, (99):1–1, 2011. 2\\n[23] Albert Jimenez, Jose M. Alvarez, and Xavier Gir´o-i-Nieto.\\nClass weighted convolutional features for visual instance\\nsearch. In BMVC, 2017. 2, 3\\n[24] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.\\nCrossdimensional weighting for aggregated deep convolu-\\ntional features. In ECCV, 2016. 1, 2, 3, 7\\n[25] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.\\nLearned Contextual Feature Reweighting for Image Geo-\\nLocalization. In CVPR, 2017. 1, 2, 3\\n[26] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\\nProxy anchor loss for deep metric learning. In CVPR, 2020.\\n1\\n[27] David G. Lowe.\\nDistinctive image features from scale-\\ninvariant keypoints. In IJCV, 2004. 1, 2\\n[28] Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian\\nMikolajczyk. SOLAR: Second-Order Loss and Attention for\\nImage Retrieval. In ECCV, 2020. 1, 2, 3, 5, 6, 7\\n[29] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,\\nand Bohyung Han. Large Scale Image Retrieval with Atten-\\ntive Deep Local Features. In ICCV, 2017. 1, 2, 3, 5, 6, 8\\n[30] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\\nSavarese. Deep metric learning via lifted structured feature\\nembedding. In CVPR, 2016. 1\\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\\nAndreas K¨opf, Edward Yang, Zach DeVito, Martin Raison,\\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An im-\\nperative style, high-performance deep learning. In NeurIPS,\\n2019. 5\\n[32] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Object retrieval with large vocabularies\\nand fast spatial matching. In CVPR, 2007. 2, 5\\n[33] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Lost in quantization:Improving particu-\\nlar object retrieval in large scale image databases. In CVPR,\\n2008. 5\\n9\\n[34] Tobias Pl¨otz and Stefan Roth. Neural nearest neighbors net-\\nworks. In NeurIPS, 2018. 2, 3\\n[35] Filip Radenovi´c, Ahmet Iscen, Giorgos Tolias, Yannis\\nAvrithis, and Ondˇrej Chum. Revisiting Oxford and Paris:\\nLarge-Scale Image Retrieval Benchmarking. In CVPR, 2018.\\n5, 6, 7\\n[36] Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum. CNN\\nimage retrieval learns from BoW: Unsupervised ﬁne-tuning\\nwith hard examples. In ECCV, 2016. 2, 7\\n[37] Filip Radenovi´c, Giorgos Tolias, and Ondˇrej Chum. Fine-\\nTuning CNN Image Retrieval with No Human Annotation.\\nIn TPAMI, 2019. 1, 2, 5, 6, 7\\n[38] Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson,\\nand Atsuto Maki. Visual Instance Retrieval with Deep Con-\\nvolutional Networks. In CoRR, 2015. 2\\n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\\nlenge. In International booktitle of Computer Vision, 2015.\\n5\\n[40] Oriane Sim´eoni, Yannis Avrithis, and Ondrej Chum. Local\\nfeatures and visual words emerge in activations. In CVPR,\\n2019. 2, 6\\n[41] O. Sim´eoni, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum.\\nGraph-based particular object discovery. Machine Vision and\\nApplications, 30(2):243–254, 3 2019. 3\\n[42] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\\nical networks for few-shot learning. In NeurIPS, 2017. 1\\n[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich.\\nGoing deeper with\\nconvolutions. In CVPR, 2015. 3\\n[44] Mingxing Tan, Ruoming Pang, and Quoc V. Le. EfﬁcientDet:\\nScalable and Efﬁcient Object Detection. In CVPR, 2020. 5\\n[45] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack\\nSim.\\nDetect-to-retrieve: Efﬁcient regional aggregation for\\nimage search. In CVPR, 2019. 2\\n[46] Giorgios Tolias, Yannis Avrithis, and Herv´e J´egou. To aggre-\\ngate or not to aggregate: Selective match kernels for image\\nsearch. In ICCV, 2013. 2\\n[47] Giorgos Tolias, Tomas Jenicek, and Ondˇrej Chum. Learn-\\ning and aggregating deep local descriptors for instance-level\\nrecognition. In ECCV, 2020. 2, 3\\n[48] Giorgos Tolias, Ronan Sicre, and Herv´e J´egou. Particular ob-\\nject retrieval with integral max-pooling of CNN activations.\\nIn ICLR, 2016. 2\\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 4\\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 9\\n[51] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-\\nmeng Zuo, and Qinghua Hu.\\nECA-Net: Efﬁcient Chan-\\nnel Attention for Deep Convolutional Neural Networks. In\\nCVPR, 2020. 2, 3, 4\\n[52] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\\ning He. Non-local Neural Networks. In CVPR, 2018. 1, 2,\\n3, 4\\n[53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\\nGoogle Landmarks Dataset v2 - A Large-Scale Benchmark\\nfor Instance-Level Recognition and Retrieval.\\nIn CVPR,\\n2020. 1, 2, 5, 6, 7\\n[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\\nKweon. CBAM: Convolutional Block Attention Module. In\\nECCV, 2018. 1, 2, 4, 8\\n[55] Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and\\nShin’ichi Satoh. Efﬁcient image retrieval via decoupling dif-\\nfusion into online and ofﬂine processing. In AAAI, 2019. 2,\\n6\\n[56] Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi\\nIizuka. Two-stage Discriminative Re-ranking for Large-scale\\nLandmark Retrieval. In arXiv:2003.11211, 2020. 6, 8\\n[57] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\\nresidual networks. In CVPR, 2017. 4\\n[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\\nto-token vit: Training vision transformers from scratch on\\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 8\\n[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\\nself-attention for image recognition. In CVPR, 2020. 2, 3\\n10\\n'),\n",
       " Document(metadata={'Published': '2023-06-02', 'Title': 'RITA: Group Attention is All You Need for Timeseries Analytics', 'Authors': 'Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li', 'Summary': \"Timeseries analytics is of great importance in many real-world applications.\\nRecently, the Transformer model, popular in natural language processing, has\\nbeen leveraged to learn high quality feature embeddings from timeseries, core\\nto the performance of various timeseries analytics tasks. However, the\\nquadratic time and space complexities limit Transformers' scalability,\\nespecially for long timeseries. To address these issues, we develop a\\ntimeseries analytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dynamically\\nclusters the objects based on their similarity into a small number of groups\\nand approximately computes the attention at the coarse group granularity. It\\nthus significantly reduces the time and space complexity, yet provides a\\ntheoretical guarantee on the quality of the computed attention. The dynamic\\nscheduler of RITA continuously adapts the number of groups and the batch size\\nin the training process, ensuring group attention always uses the fewest groups\\nneeded to meet the approximation quality requirement. Extensive experiments on\\nvarious timeseries datasets and analytics tasks demonstrate that RITA\\noutperforms the state-of-the-art in accuracy and is significantly faster --\\nwith speedups of up to 63X.\"}, page_content='RITA: Group Attention is All You Need for Timeseries Analytics\\nJiaming Liang\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nliangjm@seas.upenn.edu\\nLei Cao∗\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nlcao@csail.mit.edu\\nSamuel Madden\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nmadden@csail.mit.edu\\nZachary Ives\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nzives@cis.upenn.edu\\nGuoliang Li\\nTsinghua University\\nBeijing, China\\nliguoliang@tsinghua.edu.cn\\nABSTRACT\\nTimeseries analytics is of great importance in many real-world\\napplications. Recently, the Transformer model, popular in natu-\\nral language processing, has been leveraged to learn high quality\\nfeature embeddings from timeseries, core to the performance of\\nvarious timeseries analytics tasks. However, the quadratic time and\\nspace complexities limit Transformers’ scalability, especially for\\nlong timeseries. To address these issues, we develop a timeseries an-\\nalytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dy-\\nnamically clusters the objects based on their similarity into a small\\nnumber of groups and approximately computes the attention at\\nthe coarse group granularity. It thus significantly reduces the time\\nand space complexity, yet provides a theoretical guarantee on the\\nquality of the computed attention. The dynamic scheduler of RITA\\ncontinuously adapts the number of groups and the batch size in the\\ntraining process, ensuring group attention always uses the fewest\\ngroups needed to meet the approximation quality requirement. Ex-\\ntensive experiments on various timeseries datasets and analytics\\ntasks demonstrate that RITA outperforms the state-of-the-art in\\naccuracy and is significantly faster — with speedups of up to 63X.\\n1\\nINTRODUCTION\\nMotivation. Many data driven applications involve processing\\nmassive timeseries data, including IoT [11], medical AI [14], stock\\nmarket [27], and so on. As such, there is a great need for timeseries\\nanalytics, such as forecasting [8], classification [20], clustering [31],\\nsimilarity search [39], and anomaly detection [50], with applications\\nranging from automatically diagnosing diseases [5], recognizing\\nhuman activities [29], to stopping financial fraud [59].\\nEffective feature extraction [40] lies at the core of almost all\\nthese timeseries analytics tasks. Recently researchers [61] have\\nstarted leveraging the self-supervised pre-training methodology of\\nTransformers [4, 16, 52], which have proven remarkably successful\\nin natural language processing (NLP), to automatically learn high\\nquality feature embeddings from timeseries. In NLP, self-supervised\\npre-training exploits the sequential patterns (correlations) among\\nthe words in sentences to produce contextualized feature embed-\\ndings. Timeseries bear similarity to natural language, because in\\ntimeseries data the sequential order among the values (stock price,\\nvolume, etc.) over time matters. That is, each value is highly cor-\\nrelated with other values observed before or after it. Therefore,\\n∗Corresponding Author\\npre-training a Transformer model which takes the correlations\\namong different observations into account is a natural idea to learn\\nfeature embeddings from timeseries. Indeed, the experiments in [61]\\nconfirm that Transformer-based methods outperform traditional\\ntimeseries analytics techniques.\\nHowever, existing work [61] that directly applies Transformers\\nto learn features from timeseries data have been shown not to be\\nscalable to long timeseries [30]. The idea of self-attention [52] is\\ncentral to pre-training methods in NLP: It computes pairwise cor-\\nrelations among different semantic units in a sequence (in NLP, a\\nsentence); as such, it has quadratic time and space complexity in\\nthe length of the input sequence. Such an approach places limits on\\nthe model’s scalability, especially when handling large sequences,\\nwhich are common in real-world timeseries applications such as\\nIoT, medical AI, and finance [6, 34, 62]. Predictions about timeseries\\nmay need to look at months or years of historical data to make ac-\\ncurate predictions, spanning hundreds of thousands of samples. As\\nan example, in collaboration with a research hospital we have been\\ndeveloping a seizure classifier that automatically detects seizures\\nbased on EEG signals (timeseries) collected during the clinical ob-\\nservation of patients. As seizures last only a few seconds, we chunk\\nlong EEG data into many 2 second segments and detect seizures at\\na segment level. However, the classification of a particular segment\\ndepends on up to 12 hours of prior signal to determine if one 2\\nsecond segment indicates seizure or not, because seizure diagnosis\\nneeds to consider long-term trends in the EEG data [6]. The number\\nof segments in 12 hours is more than 21k. This is far larger than\\nthe number of semantic units the typical NLP tasks expect. For\\nexample, BERT [16] limits the number of units to 512 and even\\nmassive models like GPT-3 [4] limit the number of units to 2048.\\nAlthough in NLP some lower-complexity methods have been\\nproposed to approximately compute self-attention [10, 26, 54], their\\nperformance degrades dramatically when used on timeseries, due\\nto the gap between natural language and timeseries, as we will\\nshow in our experiments.\\nProposed Approach. To tackle the aforementioned problem, we\\ndevelop RITA, a Transformer-based timeseries analytics tool, which\\nuses a novel attention mechanism, called group attention, to scale\\nto long timeseries.\\nLeveraging the periodicity of timeseries, RITA chunks the input\\ntimeseries into segments and dynamically clusters the segments\\ninto a small number (denoted as 𝑁) of groups. Segments in the\\nsame group possess similar feature embeddings during the current\\ntraining iteration, thus enabling them to approximately share the\\n1\\narXiv:2306.01926v1  [cs.LG]  2 Jun 2023\\ncomputation of attention. As the timeseries increases in length,\\nmore sharing opportunities become available. RITA then computes\\nthe self-attention at a group level and produces a compressed group\\nattention matrix. In this way, group attention eliminates both com-\\nputation and memory bottlenecks in Transformer-style models and\\nthus more scalable to long timeseries.\\nHowever, making this idea effective and efficient in Transformer\\narchitectures is challenging for several reasons:\\n• Efficiently Producing High Quality Feature Embeddings.\\nAlthough RITA computes the attention matrix at a group level, to\\npreserve the quality of the feature embeddings, it still has to pro-\\nduce different embeddings for different segments. This is because\\neven if some segments share the attention score temporally, it does\\nnot mean they should have the same feature embedding. However,\\nusing the group attention matrix, the existing self-attention mech-\\nanism will only produce a single feature vector for each group. A\\nnaive solution would be to restore the original attention matrix\\nfrom the group attention matrix. However, in this case we again\\nget an attention matrix with quadratic space complexity. Because\\nGPUs have limited memory, GPU memory will remain a bottleneck\\nin group attention.\\n• The Number of Groups N. In RITA, the number of groups\\n𝑁is a crucial factor that balances the speed up and the quality of\\nattention approximation. A small 𝑁will lead to a large speedup,\\nbut the approximation errors can also be significant. On the other\\nhand, although a large 𝑁tends to produce high-quality approxima-\\ntions, it inevitably slows down the training process. Therefore, an\\nappropriate 𝑁is essential to the performance of group attention.\\nHowever, 𝑁depends on the distributional properties of the dataset.\\nFurthermore, like the classical transformer models, RITA stacks\\nmultiple attention layers to produce better embeddings. Ideally,\\ndifferent layers should also use different values of 𝑁. In addition,\\nduring the model training phrase, group attention should use dif-\\nferent values of 𝑁at different iterations to adapt to the varying\\nfeature embeddings. This makes manually setting appropriate 𝑁\\nalmost impossible.\\n• Batch Size. Moreover, as we want to dynamically adjust 𝑁\\nduring training, a fixed batch size is sub-optimal: as 𝑁decreases,\\nthe memory usage of a single sample decreases. This allows a larger\\nbatch size which is beneficial, because: (1) it makes full use of GPU\\nmemory; (2) high-parallelism across the samples in a big batch\\nbrings better performance. Our experimental study shows that\\ndoubling the batch size reduces the training time by 30%, while still\\npreserving the quality of the model. Thus, RITA should dynamically\\nadjust batch size as 𝑁changes.\\nTo address the above problems, we first propose an embedding\\naggregation strategy and a customized group softmax function to\\nreplace the classical softmax function [52]. Together they ensure\\nRITA is able to directly use the compressed attention matrix to\\nproduce different feature embeddings for different segments. We\\ntheoretically show the embeddings RITA produces in this way are\\nidentical to those produced by first re-storing the original large\\nattention matrix. Thus RITA is able to produce high quality embed-\\ndings without introducing extra overhead. Further, we design a GPU\\nfriendly algorithm to group the segments in parallel, effectively\\nminimizing the grouping cost.\\nP0\\nPosition\\nEmbedding\\nW1\\n+\\n+\\n+\\nWindow \\nEmbedding\\n+\\nE0\\nRaw\\nTimeseries\\nTime-aware \\nConvolution\\nW[CLS]\\nW2\\n⊗\\n.....\\nWn\\nP1\\nP2\\n.....\\nPn\\n.....\\nE1\\nE2\\nEn\\n.....\\nO0\\nO1\\nO2\\nOn\\n.....\\nRITA Encoder\\nScale & Input\\nFigure 1: RITA Architecture\\nSecond, we design an adaptive scheduler which dynamically de-\\ncides an appropriate 𝑁for each group attention layer during the\\ntraining process. It starts with a large 𝑁and iteratively merges\\ngroups that are similar to each other. Guided by an error bound on\\nthe approximated self-attention that users can tolerate, it automati-\\ncally determines if two groups are mergeable, performing merging\\nefficiently in a GPU-friendly way.\\nMoreover, we propose a learning-based method to model the\\ncorrelation between the number of groups 𝑁and the batch size 𝐵.\\nThis model is used to predict 𝐵for a given 𝑁when training RITA.\\nSpecifically, we first sample some 𝑁values in a reasonable range.\\nFor each sampled 𝑁, we find a batch size that consumes up to a\\ncertain percentage of GPU memory in a cost-efficient way. Using a\\nsmall set of mathematical functions as a prior, RITA learns a model\\nwith only a few <N, B> pairs as ground truth labels.\\nOur experiments on public timeseries benchmarks and the MGH\\nEEG data [6] confirm that RITA outperforms state-of-the-art meth-\\nods in accuracy on various timeseries analytics tasks, while our\\ngroup attention mechanism achieves a 63X speedup with much\\nless memory required, compared to existing self-attention mecha-\\nnisms [10, 52, 54].\\nContributions. The key contributions of this work include:\\n• Our group attention mechanism leverages the periodicity of\\ntimeseries, reducing the time and space complexity of the self-\\nattention mechanism with accuracy guarantees, allowing RITA to\\nscale to long timeseries data.\\n• Guided by an approximation error bound, our adaptive sched-\\nuler dynamically adapts the number of groups and the batch size\\nto the distribution properties of the evolving feature embeddings,\\nmaking group attention efficient and easily tunable.\\n• We conduct experiments on various datasets and different ana-\\nlytics tasks, demonstrating that RITA is 4 to 63 times faster than\\nthe state-of-the-art while achieving better accuracy when handling\\nlong timeseries (length ≥2000).\\n2\\n2\\nBACKGROUND\\nWe provide some background on the canonical self-attention mod-\\nule in the Transformer[52]. A self-attention module takes 𝑛hidden\\nembedding vectors 𝐻∈R𝑛∗𝑑ℎas input, then projects them to\\nqueries (𝑄), keys (𝐾) and values (𝑉) and performs Scaled-dot Prod-\\nuct Attention, which given input hidden state 𝐻, is computed by:\\n𝑄= 𝐻𝑊𝑄, 𝐾= 𝐻𝑊𝐾,𝑉= 𝐻𝑊𝑉\\n𝑂= 𝐴𝑉= 𝑆𝑜𝑓𝑡𝑀𝑎𝑥( 𝑄𝐾𝑇\\n√︁\\n𝑑𝑘\\n)𝑉\\n(1)\\nWhere 𝑊𝑄∈R𝑑ℎ∗𝑑𝑘,𝑊𝐾∈R𝑑ℎ∗𝑑𝑘,𝑊𝑉∈R𝑑ℎ∗𝑑𝑣are projection\\nmatrices for generating 𝑄, 𝐾,𝑉. 𝑄∈R𝑛∗𝑑𝑘is also regarded as the\\npacking of 𝑛query vectors {𝑞1, ...,𝑞𝑛} with dimension 𝑑𝑘into a\\nmatrix. 𝐾∈R𝑛∗𝑑𝑘,𝑉∈R𝑛∗𝑑𝑣are regarded as the packing of key\\nvectors {𝑘1, ...,𝑘𝑛} and value vectors {𝑣1, ..., 𝑣𝑛} in the same way.\\nGiven a matrix 𝑀∈R𝐿∗𝑛, the softmax function normalizes 𝑀\\nto ensure the sum of each row equals to 1, as shown below.\\n𝑆𝑜𝑓𝑡𝑀𝑎𝑥(𝑀𝑖,𝑗) =\\n𝑒𝑥𝑝(𝑀𝑖,𝑗)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑀𝑖,𝑘)\\n(2)\\nNote the attention matrix A is an 𝑛×𝑛matrix, where 𝑛represents\\nthe number of elements in the input sequence (e.g. words in NLP).\\n3\\nRITA OVERVIEW\\nGiven a collection of unlabeled timeseries, RITA first pre-trains\\na Transformer-style model to produce high quality feature em-\\nbeddings for timeseries data. This pre-trained model is then used\\nto support various downstream tasks, similar to BERT [16]. Next,\\nwe overview the model architecture of RITA. We show how RITA\\nsupports various downstream tasks in Appendix A.7.\\nAs shown in Fig. 1, RITA is consist of two components: (1) Time-\\naware Convolution Layer (2) RITA Encoder.\\nTime-aware Convolution Layer fills the gap between timeseries\\nand natural language. Despite their high-level similarity, there is a\\nbig gap between timeseries and natural language. First, in natural\\nlanguage each word, as a discrete semantic unit, has an indepen-\\ndent meaning, while each element in a timeseries is a continuous,\\nnumerical value and does not necessarily constitute an independent\\nevent. Furthermore, the input sequences are single-channeled in\\nNLP, but often multi-channeled in timeseries (i.e., sensor data often\\nconsists of several related channels).\\nRITA leverages the classical convolution [28] strategy to solve\\nthis problem. Convolution is widely used to capture the local struc-\\ntures of an image. We use convolution to chunk one input timeseries\\ninto a sequence of windows and learn the local structure of each\\nwindow, similar to the discrete semantic units in natural language.\\nIt also discovers the correlations across different channels, thus\\nnaturally solving the multi-channel problem.\\nMore specifically, treating a multi-variate timeseries of length 𝑛\\nand with𝑚variables as an n × m matrix𝑇, RITA uses𝑑convolution\\nkernels to chunk𝑇into n windows and produce one d-dimensional\\nembedding per window using the convolution operation [28]. Each\\nconvolution kernel corresponds to a w × m matrix, where 𝑤defines\\nthe number of timestamps that each convolution kernel covers,\\nidentical to the window size in sliding window.\\nRITA Encoder functions as Transformer Encoder as described in\\nthe original Transformer work[52]. It takes the embeddings of 𝑛\\nsemantic units 𝑋1,𝑋2, ...,𝑋𝑛(𝑋𝑖∈𝑅𝑑) as input (e.g. embeddings of\\n𝑛windows for a timeseries), then models the correlations between\\nthe semantic units and outputs 𝑌1, ...,𝑌𝑛(𝑌𝑖∈𝑅𝑑) as the context-\\naware embedding of each unit.\\nWhat makes RITA Encoder different from Transformer Encoder\\nis that: at the core of Transformer Encoder lies self-attention mech-\\nanism which incurs a 𝑂(𝑛2) time complexity and memory usage.\\nThis quadratic cost becomes prohibitive for long timeseries and\\nlimits the scalablity of Transformer-based models. To make the\\nattention computation efficient yet high-quality, we replace the\\ncanonical self-attention with our proposed group attention.\\nSelf-supervised Pretraining. Inspired by the “cloze text” pre-\\ntraining task in NLP, we designed a mask-and-predict task as the\\npretraining task for our model. The timeseries is randomly masked\\nand the model should recover the masked values based on corre-\\nsponding contextual information.\\nTo be specific, we generate masks on time-stamps, with a mask\\nrate 𝑝. The timeseries is scaled to be non-negative and the values\\nacross all the channels on the masked timestamps are set to be -1,\\nan impossible value on normal timestamps. Then the masked time-\\nseries is fed into RITA and the output representation is translated\\nto the recovered timeseries by a Transpose Convolution layer.\\n4\\nGROUP ATTENTION MECHANISM\\nGroup attention, a novel and efficient approximate attention mecha-\\nnism, addresses the performance bottleneck of self-attention in the\\nvanilla Transformer. In this section, we first introduce the frame-\\nwork of group attention and then theoretically establish the bound\\nof its approximation error.\\n4.1\\nThe Idea of Group Attention\\nAs periodicity is a natural property of timeseries [56], similar\\nwindows frequently occur. Similar windows result in similar\\nqueries/keys for attention computation, bringing opportunities for\\nsaving computation.\\nAs discussed in Sec. 2, 𝐴𝑖𝑗, the attention score of window 𝑖onto\\nwindow 𝑗, is determined by the inner product between the query\\nvector of window 𝑖and the key vector of window 𝑗, that is, 𝑞𝑖· 𝑘𝑗.\\nGiven another window 𝑥, if window 𝑥has the similar key vector\\nto window 𝑗, that is, 𝑘𝑗≈𝑘𝑥, then 𝑞𝑖· 𝑘𝑗≈𝑞𝑖· 𝑘𝑥. In other words,\\n𝐴𝑖𝑗≈𝐴𝑖𝑥when 𝑘𝑗≈𝑘𝑥.\\nThis observation inspires our group attention mechanism. That\\nis, we group the windows by their similarity in keys. Assuming\\nall windows in the same group have the same attention score onto\\nanother window 𝑘, we then only compute the attention once by\\nusing one single key to represent this group, for example the centroid\\nof the group of keys. This thus saves significant computation cost.\\nBetter yet, after grouping 𝑛windows into 𝑁groups, group atten-\\ntion compresses the attention matrix from an𝑛×𝑛matrix to an𝑛×𝑁\\nmatrix. Because 𝑁(number of groups) tends to be much smaller\\nthan 𝑛(number of windows) due to the periodicity of timeseries,\\ngroup attention consumes much less memory than the original\\nself-attention mechanism, successfully eliminating the memory\\nbottleneck. Note that it also doesn’t hurt quality all that much, as\\nconfirmed in our experiments (Sec. 6.2).\\n3\\nGrouping\\nAverage\\nK\\nQ\\nMatMul\\nAttention Matrix\\nWeighted\\nSoftMax\\nV\\nSum\\nAggregate\\nTranspose\\nMatMul\\nOutput\\nQ \\nK \\nV\\nFigure 2: Group Attention\\n4.2\\nComputing the Output Feature Embedding\\nWe now discuss how to efficiently compute the output feature\\nembeddings using the small compressed group attention matrix.\\n4.2.1\\nProblem: Producing Embeddings w/ Group Attention Matrix\\nAs described in the Background, once we have acquired the at-\\ntention matrix 𝐴, canonical self-attention computes the output\\nembedding 𝑂as O = AV. Because 𝐴is an 𝑛× 𝑛matrix and 𝑉is an\\n𝑛×𝑑𝑣matrix, the matrix product operation still produces an 𝑛×𝑑𝑣\\nmatrix 𝑂. That is, it produces a 𝑑𝑣dimensional feature vector for\\neach window. However, our group attention will produce an 𝑛× 𝑁\\nattention matrix e\\n𝐴, where 𝑁corresponds to the number of groups.\\nIn this case the matrix product will produce a 𝑁×𝑑𝑣matrix e\\n𝑂. That\\nis, it produces a feature vector for each group. However, our goal\\nis to produce different embeddings for different windows, because\\neven if some windows share the attention score temporally, it does\\nnot mean they should have the same feature embedding.\\nA Naive Solution. A naive solution would be to restore the full\\nattention matrix 𝐴from the group attention matrix e\\n𝐴. For example,\\ngiven one group composed of 𝑤𝑖𝑛𝑖and 𝑤𝑖𝑛𝑗, we map its group\\nattention vector in e\\n𝐴into two rows that correspond to 𝑤𝑖𝑛𝑖and\\n𝑤𝑖𝑛𝑗in 𝐴. However, in this case we again get a 𝑛× 𝑛attention\\nmatrix; and GPU memory remains a bottleneck in group attention.\\n4.2.2\\nSolution: Embedding Aggregation and Group SoftMax\\nUsing an embedding aggregation operation and a group softmax\\nfunction, RITA produces 𝑛embeddings without restoring the full\\nattention matrix. Fig. 2 shows the workflow of group attention.\\nEmbedding Aggregation. The idea is inspired by the observation\\non the matrix product operation O = AV conducted on the fully\\nrestored attention matrix 𝐴.\\nGiven an element𝑂𝑖,𝑗of𝑂corresponding to the 𝑗𝑡ℎdimension of\\n𝑤𝑖𝑛𝑖’s feature vector,𝑂𝑖,𝑗= 𝑎𝑖·𝑣𝑗, where vector ai ∈Rn denotes the\\n𝑖𝑡ℎrow of the attention matrix 𝐴and vector vj ∈Rn denotes the 𝑗𝑡ℎ\\ndimension of all the 𝑛feature vectors. Given ai =< a1\\ni , a2\\ni , · · · , an\\ni >\\nand vj =< v1\\nj , v2\\nj , · · · , vn\\nj >, 𝑂𝑖,𝑗= Ín\\nk=1 ak\\ni vk\\nj .\\nAs an example, assume 𝑤𝑖𝑛1 and 𝑤𝑖𝑛2 belong to the same group\\n𝐺1. Then 𝑎1\\n𝑖= 𝑎2\\n𝑖= e𝑎1\\n𝑖, where e𝑎1\\n𝑖∈e\\n𝐴corresponds to the attention\\nof group 𝐺1 onto 𝑤𝑖𝑛𝑖. Therefore, 𝑎1\\n𝑖𝑣1\\n𝑗+ 𝑎2\\n𝑖𝑣2\\n𝑗= e𝑎1\\n𝑖(𝑣1\\n𝑗+ 𝑣2\\n𝑗).\\nAs an immediate generalization of the above analysis, if we ag-\\ngregate up the windows that belong to the same group and convert\\nthe n-dimensional feature vector 𝑣𝑗into a 𝑁-dimensional group fea-\\nture vectore𝑣𝑗beforehand, we could directly use the group attention\\nvector e𝑎𝑖and the group feature vector e𝑣𝑗to compute 𝑂𝑖,𝑗.\\nUsing embedding aggregation, RITA is able to produce the fea-\\nture embedding e\\n𝑂that is identical to the embedding 𝑂produced\\nby using the full attention matrix 𝐴and the embedding matrix 𝑉.\\nGroup Softmax Function. In canonical self-attention the atten-\\ntion matrix 𝐴is computed as 𝐴= SoftMax( QKT\\n√\\ndk ). To compute 𝐴,\\nwe have to first compute 𝑄𝐾𝑇(denoted as 𝑃) which is an 𝑛× 𝑛\\nmatrix. Then normalizing the 𝑃matrix with softmax produces the\\nattention matrix 𝐴.\\nGroup attention follows the same procedure. But after grouping\\nkeys into e𝐾, 𝑄e𝐾𝑇produces an 𝑛× 𝑁matrix e𝑃. Due to the non-\\nlinearity of the softmax function, applying softmax directly on e𝑃\\nwill result in a group attention matrix e\\n𝐴from which we are not able\\nto recover a full attention matrix that is identical to first restoring\\ne𝑃to 𝑃and then applying softmax on 𝑃. The 𝐴matrix produced\\nby the latter is desirable, as we want to approximate the original\\nattention matrix as accurately as possible. However, restoring the\\nsmall 𝑛× 𝑁e𝑃matrix is not memory efficient, as it will end up with\\na full 𝑛× 𝑛matrix 𝑃.\\nTo solve the above problems, we introduce a new group softmax\\nfunction to replace the original softmax function (Eq. 2).\\n𝐺𝑟𝑜𝑢𝑝𝑆𝑜𝑓𝑡𝑀𝑎𝑥(g\\n𝑃𝑖,𝑗) =\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑁−1\\n𝑘=0 𝑐𝑜𝑢𝑛𝑡𝑘𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n(3)\\nIn Eq. 3, 𝑐𝑜𝑢𝑛𝑡𝑘represents the number of windows that Group\\n𝐺𝑘contains. Compared to the original softmax, our group softmax\\nconsiders each group 𝐺𝑘as 𝑐𝑜𝑢𝑛𝑡𝑘elements and counts it 𝑐𝑜𝑢𝑛𝑡𝑘\\ntimes when summing up the exponential of each group’s 𝑃𝑖,𝑘. In\\nthis way, the group softmax function operating on the small e𝑃\\nmatrix will produce exactly the same result to the softmax function\\noperating on the full 𝑃matrix.\\nTheoretical Guarantee. In Appendix A.4, we prove that the group\\nsoftmax function and the embedding aggregation operation produce\\nthe same output feature embedding with the naive method that has\\nto first restore the big full attention matrix.\\nWe show an efficient implementation of the embedding aggrega-\\ntion operation and group softmax function in Appendix A.2, Alg. 1.\\nTime Complexity. The time complexity of Alg. 1 is 𝑂(𝑛𝑁𝑑) and\\nthe space complexity is𝑂(𝑛𝑁), while the time and space complexity\\nof the original self-attention mechanism are 𝑂(𝑛2𝑑) and 𝑂(𝑛2).\\n4.3\\nError Bound\\nGroup attention produces a group attention matrix e\\n𝐴which approxi-\\nmates the attention matrix𝐴produced by the classical self-attention\\nwith a bounded error, as shown in Lemma 1.\\nLemma 1. Let 𝑅be the radius of the ball where all key vectors\\nlive; e𝑘𝑖be the representative of the group that contains key 𝑘𝑖. Let 𝐴\\ndenote the full attention matrix restored from e\\n𝐴. Suppose the distance\\nbetween e𝑘𝑖and 𝑘𝑖(||ek𝑖−k𝑖||) satisfies: ||ek𝑖−k𝑖|| ≤d.\\nThen ∀𝜖> 1, if d ≤ln(𝜖)\\n2R , 1\\n𝜖≤Ai,j\\nAi,j ≤𝜖\\nLemma 1 shows that the error bound 𝜖of the group attention is\\ndetermined by the distance 𝑑. As discussed in Sec. 5.1, it inspires\\nus to design a strategy to dynamically determine the number of\\ngroups 𝑁– the most critical parameter of group attention. Please\\nrefer to Appendix A.5 for the proof.\\n4\\n4.4\\nGPU Friendly Grouping Method\\nIn this section, we discuss the implementation of a grouping method.\\nTo make group attention efficient and effective, the grouping\\nmethod has to satisfy the following requirements:\\n(1) Tight distance bound: to ensure the approximation quality,\\nthe distance between each key and its group representative should\\nbe minimized according to Lemma 1.\\n(2) Lightweight: to ensure the performance gain, the grouping\\nmethod must be lightweight, at worst not exceeding the complexity\\nof group attention itself (𝑂(𝑁𝑛)).\\n(3) GPU friendly: to take advantage of GPUs, we prefer a group-\\ning method that mainly consists of matrix operations, which can\\nbe efficiently executed on a GPU.\\nTo satisfy the above requirements, after thorough investigation\\non various clustering algorithms, we design a GPU friendly K-\\nmeans [35] as the grouping method.\\nFirst, K-means minimizes the overall distance between any object\\nand its cluster center, hence naturally satisfying Requirement 1.\\nSecond, given 𝑁centers, in each iteration the time and space\\ncomplexity of K-means is 𝑂(𝑛𝑁). Usually, the iteration goes until\\nconvergence. However, we observe that rather than seeking a per-\\nfect K-means clustering, training a few iterations is sufficient to\\nget a good grouping for group attention, because typically the later\\niterations only slightly update the clustering and group attention\\nis robust to such imperfection.\\nThird, we design a GPU-friendly implementation of K-means.\\nThe performance bottleneck of K-means comes from the dis-\\ntance computation between each vector and its center, that is,\\n|vi −cj| =\\n√︃\\n(vi −cj)2, i ∈[1, n], j ∈[1, N]. The performance bot-\\ntleneck is 𝑣𝑖−𝑐𝑗. We instead use a different formulation: |𝑣𝑖−\\n𝑐𝑗| = |vi −cj| =\\n√︃\\n|vi|2 + |cj|2 −2vi · cj, i ∈[1, n], j ∈[1, N]. This is\\nbecause in this formulation, the performance bottleneck is 𝑣𝑖· 𝑐𝑗,\\nwhich could be implemented as a matrix product operation. Al-\\nthough the complexity of the two formulations is the same, in GPUs\\nmatrix product is much more efficient than pairwise difference.\\n5\\nADAPTIVE SCHEDULER\\nNext, we present the adaptive scheduler of RITA which addresses\\nthe challenges of determining an appropriate number of groups\\n𝑁and accordingly the batch size 𝐵, as described in Introduction.\\nUsing a dynamic scheduling method we propose, the scheduler\\nautomatically determines and adjusts 𝑁and 𝐵based on the distri-\\nbutional properties of the feature embeddings produced over the\\niterative training process, while guaranteed to produce high quality\\nattention approximation that meets the requirement of users.\\nIn Sec. 5.1 we show how RITA automatically determines 𝑁. Then\\nwe introduce in Sec. 5.2 the learning-based method which given an\\n𝑁, immediately predicts a good batch size.\\n5.1\\nDynamically Determining the Number of\\nGroups N\\nWithout loss of generality, we use one group attention module as\\nan example to show how RITA automatically gets an appropriate 𝑁.\\nThe adaptive scheduler of RITA starts with a large 𝑁and decreases\\nit dynamically. This is because in the training process of RITA, the\\nfeature embeddings produced epoch by epoch tend to get stabler\\nand stabler and gradually converge, thus no need to increase 𝑁.\\nRITA reduces the number of groups by merging similar groups.\\nIntuitively, given two groups, we could measure their similarity\\nbased on the distance of their centers. If the distance between\\ntheir centers is smaller than a distance threshold, then the two\\ngroups could be merged. However, setting an appropriate distance\\nthreshold seems hard – as difficult as setting an appropriate 𝑁.\\nTo solve this problem, RITA leverages the error bound of group\\nattention introduced in Sec. 4.3. It only requires users to set an\\nerror bound 𝜖, and then uses Lemma 1 to translate 𝜖to a distance\\nthreshold 𝑑. RITA then uses Lemma 2 to determine if merging some\\ngiven clusters still meets the error bound threshold 𝜖.\\nLemma 2. Denote 𝑐𝑘to be the cluster center of 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘. Assume\\nthe existing grouping satisfies ∀k,\\nmax\\nx∈clusterk\\n|ck −x| ≤d , thus satis-\\nfying an error bound 𝜖by Lemma 1. If there exist 𝑚clusters, namely,\\n𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘1,𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘2, ...,𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘𝑚, satisfying that:\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘𝑖\\n|𝑐𝑘𝑖−𝑐𝑘𝑗| + |𝑥−𝑐𝑘𝑖| ≤𝑑,𝑖, 𝑗∈[1,𝑚]\\n(4)\\nmerging them into one cluster still meets the error bound 𝜖.\\nPlease refer to Appendix A.6 for the proof.\\nFinding the Mergable Clusters. We formulate the problem of\\nfinding mergeable clusters using graph theory:\\n(1) each cluster is a node in the graph;\\n(2) if 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖and 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗satisfy:\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖\\n|𝑐𝑖−𝑐𝑗|+|𝑥−𝑐𝑖| ≤𝑑, and\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗\\n|𝑐𝑗−𝑐𝑖|+|𝑥−𝑐𝑗| ≤𝑑\\nthere is an undirected edge between 𝑛𝑜𝑑𝑒𝑖and 𝑛𝑜𝑑𝑒𝑗;\\nIn this scenario, finding the maximum number of mergeable\\nclusters is equivalent to finding the minimal clique cover in the\\ncorresponding graph, which is an NP-hard problem [24]. Such\\nheavy computation overhead is not acceptable for RITA. We thus\\noffer a simplified solution:\\n(1) Halve the clusters into two sets 𝑆1,𝑆2;\\n(2) If 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖∈𝑆1 and 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗∈𝑆2 satisfy:\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖\\n|𝑐𝑖−𝑐𝑗| + |𝑥−𝑐𝑖| ≤𝑑,\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗\\n|𝑐𝑗−𝑐𝑖| + |𝑥−𝑐𝑗| ≤𝑑\\n2\\n(5)\\n𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗is marked.\\n(3) Decrease the number of clusters by counting the masks in 𝑆2.\\nIn this solution, clusters in 𝑆1 can be regarded as transfer nodes.\\nIf (5) holds for (𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖∈𝑆1,𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗1 ∈𝑆2) and (𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑖∈\\n𝑆1,𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗2 ∈𝑆2), respectively, we have,\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗1\\n|𝑐𝑗1 −𝑐𝑗2 | + |𝑥−𝑐𝑗1 |\\n≤\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗1\\n|𝑐𝑗1 −𝑐𝑖| + |𝑐𝑖−𝑐𝑗2 | + |𝑥−𝑐𝑗1 |\\n≤\\n𝑚𝑎𝑥\\n𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑗1\\n|𝑐𝑗1 −𝑐𝑖| + |𝑐𝑖−𝑐𝑗2 | + |𝑥−𝑐𝑗1 | + |𝑥−𝑐𝑗2 | ≤𝑑\\n(6)\\nThus (4) holds when merging several clusters in 𝑆2 with one\\ncluster in 𝑆1. As a result, we can greedily merge clusters in 𝑆2, as\\nillustrated in step(3).\\nAssume the number of clusters decreases by 𝐷after merging,\\nwe apply a momentum update [42] on the number of clusters 𝑁, as\\nis commonly used in machine learning to smooth the changing of\\n𝑁and avoid sample selection bias. To be specific: 𝑁𝑛𝑒𝑤= 𝛼(𝑁−\\n𝐷) + (1 −𝛼)𝑁, where 𝛼is a hyper-parameter for momentum.\\n5\\n5.2\\nDynamically Determining the Batch Size\\nBecause of the dynamic grouping operation, the computational\\ngraph in deep learning training [1] varies from sample to sample. As\\na result, it is impossible to precisely compute a batch’s GPU memory\\nusage without indeed feeding it into the model. To overcome this\\nproblem, RITA learns a batch size prediction function offline; then\\nat the RITA training time, given a number of groups 𝑁, RITA uses\\nthis function to predict a proper batch size.\\nWhen the model architecture and hardware are fixed, the batch\\nsize depends on the length of the timeseries 𝐿and the average\\ngroup number among all attention module 𝑁. So RITA samples\\nseveral (𝐿𝑖, 𝑁𝑖) pairs and estimate a proper batch size for each pair.\\nMore specifically, given a user-defined timeseries maximal length\\n𝐿𝑚𝑎𝑥, we randomly sample integral points (𝐿𝑖, 𝑁𝑖) from plane\\n{1 ≤𝐿≤𝐿𝑚𝑎𝑥, 1 ≤𝑁≤𝐿}. Then we use a binary search based\\nalgorithm to find the maximal batch size 𝐵𝑖that consumes less than\\n90% available GPU memory, aiming to avoid wasting GPU memory\\nand the risks of out of memory (OOM).\\nTreating these pairs as ground truth labels, we use function\\nfitting [18] to learn the batch size predicting function B = f (L, N),\\nwhere B is a function of two variables 𝐿and 𝑁.\\nLearning the Prediction Function. We apply curve fit from\\nSciPy [53] as the function fitting tool to fit the two-variable function\\n𝐵𝑖= 𝑓(𝐿𝑖, 𝑁𝑖) on plane {1 ≤𝐿≤𝐿𝑚𝑎𝑥, 1 ≤𝑁≤𝐿}.\\nWe observe that applying one function to the whole plane incurs\\na huge estimation error. So we develop a dynamic-programming\\n(DP) method to divide the plane into several sub-planes and apply\\na distinct function to each sub-plane respectively. It is optimal in\\nminimizing the total estimation error on all sub-planes\\nWith the learned prediction function 𝑓, we can estimate a proper\\nbatch size for any (𝐿, 𝑁) during training, even if it is not seen in\\nthe sampled (𝐿𝑖, 𝑁𝑖) pairs.\\nThe Algorithms and Optimality Proof. Please refer to Appen-\\ndix A.3 for the pseudo code of the binary search-based algorithm\\nand the description of the DP method for plane-division and the\\nproof for its optimality.\\n6\\nEVALUATION\\nOur experimental study focuses on the following questions:\\n1. Effectiveness and efficiency of RITA: How does RITA com-\\npare with other Transformer-based methods and traditional time-\\nseries representation learning methods in accuracy and efficiency?\\n2. Ablation Study: How do the key techniques of RITA work?\\n6.1\\nExperimental Setup\\nDatasets. We evaluate RITA on classification and imputation tasks\\nusing 5 multi-variate and 3 uni-variate timeseries datasets.\\n• WISDM [55] is a popular multivariate timeseries dataset gen-\\nerated from the accelerometer in the mobile phone. The subjects\\nperformed 18 daily activities (e.g. walking, jogging). The dataset\\nwas collected from 51 subjects and the sampling rate is 20 Hz.\\n• HHAR dataset [46] contains sensing data of accelerometer col-\\nlected from 9 users performing 5 activities with 12 different smart-\\nphones (varying in sampling rate). This increases the complexity\\nof the task and thus can test the model’s robustness.\\n• RWHAR RealWorld HAR dataset [48] covers 15 subjects per-\\nforming 8 locomotion-style activities. Each subject wears the sen-\\nsors for approximately ten minutes. The sampling rate is 50 Hz.\\n• ECG dataset [34] consists of 10,000 EEG recordings for arrhyth-\\nmia classification. Each recording has an uncertain length ranging\\nfrom 6 to 60 seconds sampled at 500 Hz. The ECG recordings corre-\\nspond to 9 types of heart problems such as atrial fibrillation (AF)\\nand premature atrial contraction (PAC), etc.\\n• MGH [6] is a EEG dataset collected by Mass. General Hospital.\\nEach timeseries corresponds to the EEG data observed from one\\npatient during their stay in ICU for a couple of days. The EEG\\nmonitoring produced data with 20 channels. The sampling rate is\\n200 HZ. So it produces very long timeseries.\\n• WISDM*/HHAR*/RWHAR* are three uni-variate datasets de-\\nrived by picking one channel from WISDM/HHAR/RWHAR.\\nTraining/Validation Data Generation. We apply a sliding win-\\ndow on the raw timeseries to get training/validation samples. The\\nsize of the sliding window is set as 200 on small datasets (WISDM,\\nHHAR, RWHAR), 2000 on medium size dataset (ECG), and 10,000\\non the large dataset (MGH). Table 1 shows the statics of the gen-\\nerated datasets. They are randomly split into training/validation\\nset in a proportion of 0.9/0.1. In “pretraining + few-label finetun-\\ning” scenario, we use 100 labeled data per class for finetuning. We\\nguarantee that training set does not overlap with the validation set.\\nDataset\\nTrain. Size\\nValid. Size\\nLength\\nChannel\\nClasses\\nWISDM\\n28,280\\n3,112\\n200\\n3\\n18\\nHHAR\\n20,484\\n2,296\\n200\\n3\\n5\\nRWHAR\\n27,253\\n3,059\\n200\\n3\\n8\\nECG\\n31,091\\n3,551\\n2000\\n12\\n9\\nMGH\\n8,550\\n950\\n10000\\n21\\nN/A\\nTable 1: The statistics of the datasets\\nAlternative Methods. We compare RITA against the SOTA Trans-\\nformer based timeseries representation learning method TST [61].\\nTo evaluate our group attention (referred to as Group Attn.), we\\ndevelop three baselines by replacing the group attention compo-\\nnent in RITA with the classic vanilla Self-Attention [52](referred\\nto as Vanilla) and two SOTA methods that reduce the complexity\\nof self-attention by approximation in NLP, namely, Performer [10]\\n(referred to as Performer) and Linformer [54] (referred to as Lin-\\nformer). Similar to our proposed Group Attn., Vanilla, Performer,\\nLinformer all use RITA’s time-aware convolution operation (Sec. 3)\\nto turn timeseries segments into input feature vectors.\\nWe also compare Group Attn. against GRAIL [40], which is\\nthe SOTA of the non-deep learning methods for timeseries repre-\\nsentation learning. GRAIL supports classification tasks by feeding\\nthe learned representations into a Support-Vector Machine [12]\\nor K-Nearest Neighbor [17] classifier. Note GRAIL only targets\\nuni-variate timeseries and cannot support imputation tasks.\\nMethodology. We mainly focus on two downstream tasks:\\n(1) Classification. First, we train Group Attn. and the base-\\nlines with full labels from scratch to test the effectiveness of RITA\\nframework and the approximation quality of our group attention.\\nSecond, to measure the effectiveness of self-supervised pretrain-\\ning, we evaluate the accuracy of training on few labeled timeseries\\nwith/without pretraining on large scales of unlabeled timeseries. To\\nbe specific, we split the training set into a pretraining set and a fine-\\ntuning set, with very few data in the latter (100 labeled samples per\\n6\\n(a) Effectiveness \\n(b) Efficiency\\nTraining Time/sec\\nFigure 3: Full-label classification results (multi-variate data).\\nclass in our experiment). We train the model on the cloze pretrain-\\ning task with a mask rate 𝑝= 0.2. Then we train two classification\\nmodels using the finetuning set, either based on the pretrained\\nversion or from scratch. We repeat the experiment 5 times with\\nrandom data splits and report the median accuracy.\\n(2) Imputation. We run the imputation task on the datasets used\\nin classification as well as the large unlabeled MGH dataset, and\\nmeasure the mean square error and absolute imputation error. To\\nget timeseries with missing values, we randomly mask the values\\nwith an expected mask rate of 𝑝= 0.2. The masked values are\\nreplaced with a special value.\\nFinally, to evaluate Group Attn.’s benefit on efficiency, the total\\ntime of forward computation, backward propagation, and grouping\\nare measured for all methods in all the experiments.\\nTo save space, we only report the average training time per epoch\\nhere and refer readers to Appendix A.8 for the inference time.\\nWe first compare against the Transformer-based methods on\\nmulti-variate datasets (sec. 6.2, 6.3), then compare against the non-\\ndeep learning method GRAIL on uni-variate datasets (sec. 6.4).\\nConfiguration. Please refer to Appendix A.1 for the experiment\\nconfiguration and hyper-parameter settings.\\n6.2\\nEffectiveness: Transformer-Based Methods\\nWe first evaluate the quality of the models trained with full labels\\nfrom scratch. We then show how the pretraining of RITA increases\\nthe accuracy of the downstream tasks.\\n6.2.1\\nfull-label training (Multi-variate classification)\\nResults shown in Figure 3(a) get us the following observations:\\n(1) RITA’s advantage over TST. On all four datasets for the clas-\\nsification tasks, Group Attn. and the other three baselines that use\\nRITA architecture (Vanilla, Performer, and Linformer) outperform\\nTST. In particular, Group Attn. outperforms TST by 49 percentage\\npoints on the ECG dataset (88.48% vs 39.93%) with long timeseries.\\nTwo deficiencies in TST may cause its poor performance on the long\\ntimeseries. Firstly, TST concatenates the output embedding vector\\nof each time stamp, then uses a linear classifier to do classification\\non the concatenated vector. When the timeseries is long, the linear\\nclassifier has so many parameters that it tends to overfit easily.\\nSecondly, TST replaces Layer Normalization in vanilla Transformer\\nwith Batch Normalization. When the timeseries is long, it can only\\naccommodate a small number of timeseries in each batch, leading\\nto bias in Batch Normalization.\\n(2) Group-attention’s advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on\\n3 out of 4 datasets for classification. Although Linformer works\\nslightly better than Group Attn. on the ECG dataset (90.37% vs\\n88.84%), its performance is the worst in all other cases compared\\nto any other RITA-based methods. Vanilla computes the attention\\nscores precisely. Thus it is expected to work well. However, Group\\nAttn. outperforms Vanilla on WISDM (87.50% vs 86.95%) and is very\\nclose to it on other 3 datasets. This suggests that group attention’s\\napproximation quality is good.\\n6.2.2\\npretraining + few label finetune (Multi-variate classification)\\nThe results shown in Table 3 get us the following observation:\\n(1) Pretraining is effective. Pretraining always leads to better\\naccuracy than training with a few labels from scratch. In particular,\\non WISDM data all the methods using RITA architecture increase\\nthe accuracy by at least 10%. This is impressive considering we do\\nnot have a very large unlabeled pre-training set to use.\\n(2) RITA’s advantage over TST. our Group Attn. and other\\nthree baselines using RITA architecture (Vanilla, Performer, and\\nLinformer) significantly outperform TST on all four classification\\ndatasets by 25 percentage points.\\n(3) Group Attention’s advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on 3\\nout of 4 datasets. When compared to Vanilla, Group Attn. is better\\non HHAR and ECG, and comparable on the other two, further con-\\nfirming its high quality on approximation. Further, we notice that\\nLinformer struggles in this setting: in average its accuracy is worse\\nthan Vanilla by 8.22% and Group Attn. by 8.01%. This is because the\\nlow-rank projection operation introduces extra model parameters,\\nmaking Linformer more easily overfit, while overfitting is especially\\nharmful when there are only a few labeled training samples.\\n6.2.3\\nfull-dataset training (Multi-variate imputation)\\nSimilar to classification tasks, the results of imputation tasks\\n(Table.2) show that Group Attn. consistently outperforms the base-\\nlines in training time while achieving comparable/better MSE. Again,\\non the large dataset MGH (length = 10,000), TST and Vanilla fail due\\nto out of memory (OOM) errors. Methods using RITA framework\\n(Group Attn., Performer, Linformer) all achieve very low MSE (are\\nhighly accurate). Among them Linformer is the worst.\\n6.3\\nEfficiency: Transformer-based Methods\\nWe measure the efficiency by the average training time per epoch\\nincluding the cost of the forward computation + backward propaga-\\ntion and the grouping overhead. We first show the results on all the\\n5 datasets in Sec. 6.3.1. We then vary the length of the timeseries\\non the MGH dataset to show group attention’s scalability on long\\ntimeseries in Sec. 6.3.2.\\n6.3.1\\nTraining Time: All Multi-variate Datasets\\nThe results in Fig. 3(b) and Table 2 lead to the below observations:\\n(1) Vanilla Self-Attention is not scalable. In average, it takes\\n2-3 minutes to train one epoch when the length of the timeseries is\\nonly 200 (WISDM, HHAR, RWHAR), takes over 15 minutes when\\nthe length increases to 2,000 (ECG), and fails on the long MGH data\\nwhen the length reaches 10,000 due to out of GPU memory.\\n(2) Group Attn.’s advantage over all other attention mecha-\\nnisms. As we have shown in Sec. 6.2, Group Attn. is more accurate\\n7\\nDataset\\nLength\\nTST [61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nWISDM\\n200\\n13.30\\n150.3\\n3.240\\n178.1\\n3.449\\n162.6\\n3.852\\n141.9\\n3.277\\n136.7\\nHHAR\\n200\\n1.085\\n78.2\\n0.2968\\n97.4\\n0.2980\\n82.6\\n0.3198\\n81.1\\n0.2974\\n73.3\\nRWHAR\\n200\\n0.0882\\n83.9\\n0.0478\\n108.1\\n0.0489\\n89.1\\n0.0572\\n98.4\\n0.0478\\n81.3\\nECG\\n2000\\n0.0905\\n696.3\\n0.0037\\n857.9\\n0.0033\\n270.2\\n0.0035\\n291.38\\n0.0038\\n164.36\\nMGH\\n10000\\nN/A\\nN/A\\nN/A\\nN/A\\n0.00014\\n356.2\\n0.00088\\n404.9\\n0.00042\\n54.4\\nTable 2: Imputation results (multi-variate data). The best results are marked with bold.\\nDataset\\nPretrain Size\\nTST [61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nWISDM\\n62,231\\n49.13%\\n50.03%\\n66.16%\\n75.89%\\n66.09%\\n73.97%\\n50.12%\\n67.44%\\n62.56%\\n75.06%\\nHHAR\\n68,294\\n72.56%\\n75.30%\\n75.60%\\n81.35%\\n76.52%\\n80.70%\\n65.94%\\n76.52%\\n76.17%\\n82.62%\\nRWHAR\\n63,599\\n69.46%\\n80.41%\\n85.68%\\n91.14%\\n87.54%\\n91.33%\\n81.03%\\n86.33%\\n86.13%\\n89.63%\\nECG\\n561,358\\n20.98%\\n27.99%\\n42.05%\\n46.16%\\n43.34%\\n45.58%\\n27.19%\\n31.34%\\n42.58%\\n46.39%\\nTable 3: Pretrain + few-label finetuning results. The best results are marked with bold.\\nTraining Time/sec\\nMSE\\n(a) Effectiveness\\n(b) Efficiency\\nFigure 4: Varying the lengths of timeseries.\\nthan Performer and Linformer in classification and imputation tasks,\\nwhile Group Attn. is always faster than Performer, Linformer, and\\nall other baselines on all 5 multi-variate datasets, thus a win-win.\\n(3) The longer the timeseries, the larger the speedup. On\\nthe medium sized ECG dataset with a length of 2,000, Group Attn.\\nhas a speedup of 3.86/1.36/2.27 compared to Vanilla/Performer/Lin-\\nformer. When the length increases to 10,000, the speedup on the\\nMGH dataset increases to 6.59/7.48 compared to Performer/Lin-\\nformer (Vanilla and TST failed in this case) on imputation task\\n(Table. 2). However, even on the short WISDM, HHAR, RWHAR\\ndatasets, Group Attn. still consistently outperforms other methods,\\nconfirming that it does not introduce much overhead. This is be-\\ncause when the length of the timeseries gets longer, Group Attn.\\ngets more opportunities to find windows with similar properties.\\n6.3.2\\nTraining time: Varying the Length\\nIn this experiment, we truncate the original MGH timseries into\\nsequences with the lengths at 2000/4000/6000/8000/10000, and com-\\npare Group Attn. against Vanilla and other attention mechanisms.\\nVanilla cannot handle sequences longer than 8000.\\nThe results in Fig. 4 again show that the longer the timeseries, the\\nlarger the speed up. With comparable MSE, Group Attn. outperforms\\nVanilla by 63X. Moreover, as the length increases from 2000 to 10000,\\nthe training time of Group Attn. only increases from 31.2 seconds\\nto 54.4 seconds per epoch. The reason is that as the timeseires\\nbecomes longer, there are more grouping opportunities because of\\nthe similarity of the timeseries segments.\\nAccuracy\\nTraining Time/sec\\n(a)\\n(b)\\nFigure 5: Comparison to non-deep learning method (uni-\\nvariate data).\\n6.4\\nComparison to Non-deep Learning Methods\\nWe compare against GRAIL, the SOTA of non-deep learning time-\\nseries representation learning. We use the three uni-variate datasets,\\nbecause GRAIL only targets uni-variate timeseries.\\nResults in Fig. 5 show that on all 3 datasets RITA significantly\\noutperforms GRAIL in accuracy by 45, 16, and 21 percentage points\\nbecause of the expressive power of Transformer. Moreover, thanks\\nto the GPU-friendly design of RITA, it is at least 2× faster than\\nGRAIL in training time.\\n6.5\\nAblation Study\\n6.5.1\\nAdaptive Scheduler\\nTo evaluate the effectiveness of RITA’s adaptive scheduler (Sec. 5),\\nwe compare it against a baseline using a fixed group number 𝑁. We\\nvary 𝑁and the error bound threshold 𝜖used by RITA.\\nFrom the results in Table 4 we get the following observations:\\n(1) Adaptive Scheduler is better than fixed 𝑁. Training with\\nAdaptive Scheduler already achieves better or comparable perfor-\\nmance compared to the best performing 𝑁. More specifically, on\\nthe MGH dataset, dynamic scheduler always achieves better accu-\\nracy and is much faster compared to fixed 𝑁. On the ECG dataset,\\nalthough fixed 𝑁is slightly better than adaptive scheduler in accu-\\nracy when setting the N as 512, it runs much slower than adaptive\\nscheduler. Of course, finding the best 𝑁that balances the accuracy\\nand running time requires careful tuning.\\n(2) Adaptive Scheduler is tuning free. It is robust on both\\naccuracy and running time when 𝜖varies, while the results of\\nfixed 𝑁vary significantly when the value of 𝑁changes. Therefore,\\nAdaptive Scheduler frees the users from tuning the 𝜖threshold,\\nwhile it is hard to find an appropriate 𝑁for a given dataset.\\n8\\nDataset\\nTask\\nScheduler\\nParameter\\nMetric\\nTime\\nECG\\nClass.\\nDynamic\\n1.5\\n88.34%\\n292.5\\n2\\n88.48%\\n236.8\\n3\\n87.83%\\n216.8\\nFixed\\n64\\n87.50%\\n255.2\\n128\\n88.96%\\n297.2\\n256\\n88.82%\\n414.1\\n512\\n90.03%\\n662.6\\n1024\\n88.65%\\n873.7\\nMGH\\nImput.\\nDynamic\\n1.5\\n0.00041\\n60.7\\n2\\n0.00040\\n57.9\\n3\\n0.00042\\n54.4\\nFixed\\n128\\n0.00054\\n128.6\\n256\\n0.00053\\n190.2\\n512\\n0.00049\\n240.8\\n1024\\n0.00046\\n323.3\\nTable 4: Adaptive Scheduling VS Fixed N.\\nPretrain Data size\\nFew-label Accuracy\\nN/A\\n62.56%\\n12,446\\n72.94%\\n24,892\\n72.78%\\n37,338\\n74.10%\\n49,784\\n74.22%\\n62,231\\n75.06%\\nTable 5: RITA Pretraining: increasing sizes of pretrain set.\\n6.5.2\\nThe Sizes of the Pretraining Data\\nNext, we evaluate how the number of unlabeled data influences the\\neffectiveness of pretraining. To get empirical results, we pretrain\\nRITA on WISDM dataset with 20%/40%/60%/80% of the pretraining\\ndata and finetune each pretrained model with 100 labels per class.\\nThe results in Table 5 show that: (1) The more pretraining data,\\nthe larger the improvement. The accuracy increases with the\\nsizes of the pretraining data; (2) Marginal utility diminishing.\\nThe first 20% pretraining data gives a 10.38% improvement in accu-\\nracy (72.94% vs 62.56%), while the remaining 80% pretraining data\\nonly gives an additional improvement of 2.12% (75.06% vs 72.94%).\\n7\\nRELATED WORK\\n7.1\\nTimeseries Analytics\\nThere is a great deal of prior work on timeseries analytics methods.\\nThis work can be divided into three categories: (1) non-deep learn-\\ning methods; (2) CNN/RNN-based deep learning methods; and (3)\\nTransformer-based deep learning methods.\\nTraditional Methods. These methods, such as TS-CHIEF [45],\\nHIVE-COTE [33], ROCKET [15] have achieved notable performance\\non public datasets. Despite that, traditional methods suffer from\\none or more issues: they (1) rely on expert knowledge for feature\\nextraction; (2) incur heavy computation cost and are inappropriate\\nfor GPU devices; (3) support only uni-variate timeseries; (4) perform\\nclassification solely. Some work [61] shows that the transformed-\\nbased methods outperform these traditional methods especially on\\nmulti-variate timeseries.\\nIn particular, as the SOTA of timeseries representation learn-\\ning, GRAIL [40] extracts landmarks from data and computes the\\nrepresentations with the combination of the landmarks. However,\\nGRAIL only supports uni-variate timeseries. Our experiments (Sec. 6.4)\\nshow that RITA significantly outperforms GRAIL in both effective-\\nness and efficiency on uni-variate timeseries.\\nCNN/RNN-based Deep Learning Methods. CNN-based methods,\\nsuch as InceptionTime [21] and Resnet [19], are good at classifica-\\ntion tasks, but can not handle generative tasks such as forecasting\\nbecause of the inductive bias of convolution networks. RNN-based\\nmethods, such as Brit [7] and deepAR [44], are capable for classifi-\\ncation, regression and generation. However, the recurrent structure\\nbrings a lot of problems: (1) limiting the model’s ability in captur-\\ning long-range correlation; (2) notoriously difficult to train [41]\\nbecause of gradient vanishing and exploding problem. As a result,\\nsuch methods can hardly scale to very long timeseries.\\nTransformer-based Deep Learning Methods. Given that Trans-\\nformer is the best choice for backbone in almost all sequence mod-\\neling tasks, some effort has been made to apply Transformer to\\ntimeseries analytics. Targeting forecasting of uni-variate timeseries,\\nLogTrans [30] introduced a log sparsity assumption to attention\\ncomputation. Informer [62] pushes LogTrans a step further and\\nscales forecasting to multi-variate timeseries. Autoformer [57] per-\\nforms forecasting by decomposing timeseries into two parts, i.e.\\nthe trend part and the seasonal part.\\nFor imputation tasks, CDSA [37] outperforms statistical meth-\\nods and the SOTA of RNN-based method Brit [7] on 3 public and\\n2 competition datasets. For timeseries classification, AutoTrans-\\nformer [43] performs architecture search to adapt to the tasks\\nin different domains. For timeseries anomaly detection, Anomaly\\nTransformer [58] outperforms many widely-used methods such\\nas OmniAnomaly [47], assuming the attention score maps show\\nGaussian distribution.\\nAll of these works are designed for specific tasks, rather than\\nfunctioning as a representation learning framework to serve\\ndifferent downstream tasks. To fill this gap, some researchers pro-\\nposed a Transformer-based architecture, called TST [61]. Like RITA,\\nTST supports regression, classification, and unsupervised learning\\nthrough the “cloze test” pretraining task on timeseries. However,\\nTST directly uses the classical Vanilla self-attention, thus not scal-\\nable to long timeseries as shown in our experiments (Sec. 6.3.2).\\n7.2\\nEfficient Transformers\\nThe need of improving the scalability of Transformers has led to\\nmore efficient variations of Transformers, especially for accommo-\\ndating long text data in NLP [49].\\nIntroducing fixed/random patterns to self-attention mechanism\\nis an intuitive idea. Sparse Transformer [9] and Longformer [3] only\\ncompute attention at fixed intervals. ETC [2] and BigBird [60] use\\nglobal-local attention: the attention computation is limited within\\na fixed radius, while some auxiliary tokens are added to attend/get\\nattended globally. The deficiencies of fixed attention patterns are\\nobvious: it heavily depends on users to give an optimal setting.\\nTo decrease the reliance on human labor, some works seek to\\nintroduce learnable/adaptive attention patterns instead of fixed\\npatterns. Reformer [26] proposed only computing the dominant\\nattention terms based on their observation of sparsity in atten-\\ntion matrix from language/image data. Such sparsity is intuitive\\nin language data, in which a word’s attention mainly focuses on\\nthe nearby sentences. However, attention in timeseries data shows\\nstrong seasonal patterns rather than sparse patterns, mainly as\\n9\\nresult of the periodicity of timeseries data. Therefore, such works\\ndo not work well for timeseries.\\nApart from introducing attention patterns, some works seek\\nto solve this problem with applied mathematics techniques. Lin-\\nformer [54] performs a projection to decrease the size of query,\\nkey and value matrices before attention computation, because the\\nattention matrix tends to be low-ranked. Performer [10] uses linear\\nfunctions to approximate the kernel function softmax, making at-\\ntention computation commutative. When the sequence length is far\\ngreater than the dimension of embedding vectors, Performer ben-\\nefits from changing the order of matrix multiplication. Linformer\\nand Performer do not depend on the unique properties of language\\ndata, thus potentially fitting timeseries better than other techniques,\\nwhich is why we compared against them in our experiments. How-\\never as shown in Sec. 6, our group attention significantly outper-\\nforms them in both accuracy and efficiency (training time), because\\ngroup attention fully leverages the periodicity of timeseries.\\n8\\nCONCLUSION\\nIn this work, we presented RITA, an automatic, self-supervised, and\\nscalable timeseries analytics tool. RITA effectively adapts Trans-\\nformer, popular in NLP, into timeseries analytics. As the key com-\\nponent of RITA, group attention eliminates the performance bottle-\\nneck of the classical self-attention mechanisms, thus successfully\\nscaling RITA to highly complex, long timeseries data. Our experi-\\nments confirm that RITA significantly speeds up the state-of-the-art\\nby 63X with a better accuracy.\\nREFERENCES\\n[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\\nCraig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.\\n2016. Tensorflow: Large-scale machine learning on heterogeneous distributed\\nsystems. arXiv preprint arXiv:1603.04467 (2016).\\n[2] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher,\\nPhilip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020.\\nETC: Encoding long and structured inputs in transformers.\\narXiv preprint\\narXiv:2004.08483 (2020).\\n[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\\ndocument transformer. arXiv preprint arXiv:2004.05150 (2020).\\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 1877–1901.\\n[5] C Bui, N Pham, A Vo, A Tran, A Nguyen, and T Le. 2017. Time series forecasting\\nfor healthcare diagnosis and prognostics with the focus on cardiovascular dis-\\neases. In International conference on the development of biomedical engineering in\\nVietnam. Springer, 809–818.\\n[6] Lei Cao, Wenbo Tao, Sungtae An, Jing Jin, Yizhou Yan, Xiaoyu Liu, Wendong\\nGe, Adam Sah, Leilani Battle, Jimeng Sun, Remco Chang, M. Brandon Westover,\\nSamuel Madden, and Michael Stonebraker. 2019. Smile: A System to Support\\nMachine Learning on EEG Data at Scale. Proc. VLDB Endow. 12, 12 (2019), 2230–\\n2241.\\n[7] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018.\\nBrits:\\nBidirectional recurrent imputation for time series. Advances in neural information\\nprocessing systems 31 (2018).\\n[8] Chris Chatfield. 2000. Time-series forecasting. Chapman and Hall/CRC.\\n[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating\\nlong sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019).\\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\\nAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\\nLukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint\\narXiv:2009.14794 (2020).\\n[11] Andrew A Cook, Göksel Mısırlı, and Zhong Fan. 2019. Anomaly detection for IoT\\ntime-series data: A survey. IEEE Internet of Things Journal 7, 7 (2019), 6481–6494.\\n[12] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine\\nlearning 20, 3 (1995), 273–297.\\n[13] David R Cox. 1958. The regression analysis of binary sequences. Journal of the\\nRoyal Statistical Society: Series B (Methodological) 20, 2 (1958), 215–232.\\n[14] Benjamin F Crabtree, Subhash C Ray, Priscilla M Schmidt, Patrick T O’Connor,\\nand David D Schmidt. 1990. The individual over time: time series applications in\\nhealth care research. Journal of clinical epidemiology 43, 3 (1990), 241–260.\\n[15] Angus Dempster, François Petitjean, and Geoffrey I. Webb. 2020. ROCKET: excep-\\ntionally fast and accurate time series classification using random convolutional\\nkernels. Data Min. Knowl. Discov. 34, 5 (2020), 1454–1495.\\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\\nProceedings of the 2019 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). 4171–\\n4186.\\n[17] Evelyn Fix and Joseph Lawson Hodges. 1989. Discriminatory analysis. Nonpara-\\nmetric discrimination: Consistency properties. International Statistical Review/Re-\\nvue Internationale de Statistique 57, 3 (1989), 238–247.\\n[18] Philip George Guest and Philip George Guest. 2012. Numerical methods of curve\\nfitting. Cambridge University Press.\\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\\nlearning for image recognition. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition. 770–778.\\n[20] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar,\\nand Pierre-Alain Muller. 2019. Deep learning for time series classification: a\\nreview. Data mining and knowledge discovery 33, 4 (2019), 917–963.\\n[21] Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier,\\nDaniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane Idoumghar, Pierre-\\nAlain Muller, and François Petitjean. 2020. Inceptiontime: Finding alexnet for\\ntime series classification. Data Mining and Knowledge Discovery 34, 6 (2020),\\n1936–1962.\\n[22] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization\\nfor nearest neighbor search. IEEE transactions on pattern analysis and machine\\nintelligence 33, 1 (2010), 117–128.\\n[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019. Billion-scale similarity\\nsearch with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535–547.\\n[24] Richard M Karp. 1972. Reducibility among combinatorial problems. In Complexity\\nof computer computations. Springer, 85–103.\\n[25] Eamonn Keogh, Kaushik Chakrabarti, Michael Pazzani, and Sharad Mehrotra.\\n2001. Dimensionality reduction for fast similarity search in large time series\\ndatabases. Knowledge and information Systems 3, 3 (2001), 263–286.\\n[26] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient\\ntransformer. arXiv preprint arXiv:2001.04451 (2020).\\n[27] John Kraft and Arthur Kraft. 1977. Determinants of common stock prices: A time\\nseries analysis. The journal of finance 32, 2 (1977), 417–425.\\n[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-\\nsification with Deep Convolutional Neural Networks. In Advances in Neural\\nInformation Processing Systems, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-\\nberger (Eds.), Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/\\npaper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\\n[29] Oscar D Lara and Miguel A Labrador. 2012. A survey on human activity recog-\\nnition using wearable sensors. IEEE communications surveys & tutorials 15, 3\\n(2012), 1192–1209.\\n[30] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,\\nand Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-\\nneck of transformer on time series forecasting. Advances in Neural Information\\nProcessing Systems 32 (2019).\\n[31] T Warren Liao. 2005. Clustering of time series data—a survey. Pattern recognition\\n38, 11 (2005), 1857–1874.\\n[32] Rake& Agrawal King-lp Lin and Harpreet S Sawhney Kyuseok Shim. 1995. Fast\\nsimilarity search in the presence of noise, scaling, and translation in time-series\\ndatabases. In Proceeding of the 21th International Conference on Very Large Data\\nBases. 490–501.\\n[33] Jason Lines, Sarah Taylor, and Anthony Bagnall. 2018. Time Series Classification\\nwith HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based\\nEnsembles. ACM Trans. Knowl. Discov. Data 12, 5, Article 52 (jul 2018), 35 pages.\\n[34] Feifei Liu, Chengyu Liu, Lina Zhao, Xiangyu Zhang, Xiaoling Wu, Xiaoyan\\nXu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, et al. 2018. An open\\naccess database for evaluating the algorithms of electrocardiogram rhythm and\\nmorphology abnormality detection. Journal of Medical Imaging and Health\\nInformatics 8, 7 (2018), 1368–1373.\\n[35] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on\\ninformation theory 28, 2 (1982), 129–137.\\n[36] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.\\narXiv preprint arXiv:1711.05101 (2017).\\n[37] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, and\\nShih-Fu Chang. 2019. CDSA: cross-dimensional self-attention for multivariate,\\ngeo-tagged time series imputation. arXiv preprint arXiv:1905.09904 (2019).\\n10\\n[38] Yu A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximate\\nnearest neighbor search using hierarchical navigable small world graphs. IEEE\\ntransactions on pattern analysis and machine intelligence 42, 4 (2018), 824–836.\\n[39] Tripti Negi and Veena Bansal. 2005. Time series: Similarity search and its appli-\\ncations. In Proceedings of the International Conference on Systemics, Cybernetics\\nand Informatics: ICSCI-04, Hyderabad, India. 528–533.\\n[40] John Paparrizos and Michael J Franklin. 2019. Grail: efficient time-series repre-\\nsentation learning. Proceedings of the VLDB Endowment 12, 11 (2019), 1762–1777.\\n[41] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty\\nof training recurrent neural networks. In International conference on machine\\nlearning. PMLR, 1310–1318.\\n[42] Ning Qian. 1999. On the momentum term in gradient descent learning algorithms.\\nNeural networks 12, 1 (1999), 145–151.\\n[43] Yankun Ren, Longfei Li, Xinxing Yang, and Jun Zhou. 2022. AutoTransformer:\\nAutomatic Transformer Architecture Design for Time Series Classification. In\\nPacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 143–\\n155.\\n[44] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.\\nDeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-\\nnational Journal of Forecasting 36, 3 (2020), 1181–1191.\\n[45] Ahmed Shifaz, Charlotte Pelletier, François Petitjean, and Geoffrey I. Webb. 2020.\\nTS-CHIEF: a scalable and accurate forest algorithm for time series classification.\\nData Mining and Knowledge Discovery 34 (2020), 742–775.\\n[46] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,\\nMikkel Baun Kjærgaard, Anind Dey, Tobias Sonne, and Mads Møller Jensen.\\n2015. Smart devices are different: Assessing and mitigatingmobile sensing het-\\nerogeneities for activity recognition. In Proceedings of the 13th ACM conference\\non embedded networked sensor systems. 127–140.\\n[47] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust\\nanomaly detection for multivariate time series through stochastic recurrent\\nneural network. In Proceedings of the 25th ACM SIGKDD international conference\\non knowledge discovery & data mining. 2828–2837.\\n[48] Timo Sztyler and Heiner Stuckenschmidt. 2016. On-body localization of wearable\\ndevices: An investigation of position-aware activity recognition. In 2016 IEEE\\nInternational Conference on Pervasive Computing and Communications (PerCom).\\nIEEE, 1–9.\\n[49] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient\\ntransformers: A survey. ACM Computing Surveys (CSUR) (2020).\\n[50] Mingyan Teng. 2010. Anomaly detection on time series. In 2010 IEEE International\\nConference on Progress in Informatics and Computing, Vol. 1. IEEE, 603–608.\\n[51] Patrick A Thompson. 1990. An MSE statistic for comparing forecast accuracy\\nacross series. International Journal of Forecasting 6, 2 (1990), 219–227.\\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In Advances in Neural Information Processing Systems 30: Annual Con-\\nference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA. 5998–6008.\\n[53] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\\nReddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser,\\nJonathan Bright, Stéfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jar-\\nrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern,\\nEric Larson, C J Carey, İlhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas,\\nDenis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\\nCharles R. Harris, Anne M. Archibald, Antônio H. Ribeiro, Fabian Pedregosa,\\nPaul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Al-\\ngorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261–272.\\nhttps://doi.org/10.1038/s41592-019-0686-2\\n[54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Lin-\\nformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768\\n(2020).\\n[55] Gary M Weiss, Kenichi Yoneda, and Thaier Hayajneh. 2019. Smartphone and\\nsmartwatch-based biometrics using activities of daily living. IEEE Access 7 (2019),\\n133190–133202.\\n[56] Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke, and Huan Xu. 2021.\\nRobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity Detection.\\nIn Proceedings of the 2021 International Conference on Management of Data (Virtual\\nEvent, China) (SIGMOD ’21). Association for Computing Machinery, New York,\\nNY, USA, 2328–2337. https://doi.org/10.1145/3448016.3452779\\n[57] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-\\ncomposition transformers with auto-correlation for long-term series forecasting.\\nAdvances in Neural Information Processing Systems 34 (2021), 22419–22430.\\n[58] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2021. Anomaly\\nTransformer: Time Series Anomaly Detection with Association Discrepancy.\\narXiv preprint arXiv:2110.02642 (2021).\\n[59] Dianmin Yue, Xiaodan Wu, Yunfeng Wang, Yue Li, and Chao-Hsien Chu. 2007. A\\nreview of data mining-based financial fraud detection research. In 2007 Interna-\\ntional Conference on Wireless Communications, Networking and Mobile Computing.\\nIeee, 5519–5522.\\n[60] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\\nAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\\net al. 2020. Big bird: Transformers for longer sequences. Advances in Neural\\nInformation Processing Systems 33 (2020), 17283–17297.\\n[61] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and\\nCarsten Eickhoff. 2021. A Transformer-based Framework for Multivariate Time\\nSeries Representation Learning. In KDD ’21: The 27th ACM SIGKDD Conference\\non Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18,\\n2021. 2114–2124.\\n[62] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\\nand Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-\\nquence time-series forecasting. In Proceedings of AAAI.\\nA\\nAPPENDIX: SUPPLEMENTARY MATERIAL\\nA.1\\nExperiment Configuration and\\nHyper-parameter Settings\\nConfiguration. All models were trained on an NVIDIA Tesla V100\\n16GB GPU. All the methods are optimized with AdamW [36] of\\nwhich the starting learning rate and weight decay parameter are\\nboth 1𝑒−4. In full-label training scenario, we train the models for\\n100 epochs. In “pretraining + few-label finetuning scenario”, as the\\npretrained models require fewer epochs to converge [61], we train\\nthe model for 50 epochs. For a fair comparison, the baselines use a\\nmaximal batch size within GPU’s capacity during training.\\nAs for model hyper-parameter setting, RITA and the baselines\\nuse a Transformer structure balancing Vanilla ’s accuracy and\\nefficiency: 8-layer stack of 2-head attention with hidden vectors\\nin dimension of 64. Convolution kernel size is set to 5 by default.\\nWe set the error bound threshold (𝜖, Sec. 5.1) of Group Attention\\nto 2, as it balances the accuracy and the efficiency in general on\\nall datasets. Because Linformer requires the users to set the sizes\\nof projection matrix, in different settings we choose an accuracy-\\nefficiency balancing one among {64,128,256,512}.\\nA.2\\nEfficient Computation of Group Attention\\nAlgorithm 1 Efficient Computation of Group Attention\\nRequire: 𝑄,𝑉, 𝑅,𝐶𝑂𝑈𝑁𝑇, 𝐵𝐸𝐿𝑂𝑁𝐺\\nEnsure: 𝑄,𝑉∈R𝑛∗𝑑,𝑅∈R𝑁∗𝑑,𝐶𝑂𝑈𝑁𝑇∈N𝑁,𝐵𝐸𝐿𝑂𝑁𝐺∈N𝑛\\n1: function group_attention(𝑄,𝑉, 𝑅)\\n2:\\nfor 𝑖= 0 →𝑁−1 do\\n3:\\ne𝑣𝑖←Í𝑛−1\\n𝑗=0 (𝐵𝐸𝐿𝑂𝑁𝐺𝑗== 𝑖)𝑣𝑗\\n4:\\ne𝑃←𝑄𝑅𝑇\\n5:\\nfor 𝑖= 0 →𝑛−1 do\\n6:\\nfor 𝑗= 0 →𝑁−1 do\\n7:\\n𝑤𝑖,𝑗←𝑒𝑥𝑝(e𝑃𝑖,𝑗)𝐶𝑂𝑈𝑁𝑇𝑗\\n8:\\nfor 𝑖= 0 →𝑛−1 do\\n9:\\n𝑠𝑖←Í𝑁−1\\n𝑗=0 𝑤𝑖,𝑗\\n10:\\nfor 𝑖= 0 →𝑛−1 do\\n11:\\n𝑜𝑖←Í𝑁−1\\n𝑗=0\\n𝑒𝑥𝑝( e𝑃𝑖,𝑗)\\n𝑠𝑖\\ne𝑣𝑗\\n12:\\nreturn 𝑂\\nIn Alg. 1, we denote𝐶𝑂𝑈𝑁𝑇𝑖to be the size of the 𝑖𝑡ℎgroup, 𝑁to\\nbe the number of groups, r𝑖to be the representative key of the 𝑖𝑡ℎ\\ngroup and R to be the matrix consisting of all r𝑖, 𝐵𝐸𝐿𝑂𝑁𝐺𝑖to be\\nthe group that k𝑖belongs to. 𝑄,𝑉are the packing matrices of query\\nvectors and value vectors as described in Sec.2. Alg. 1 outputs the\\n11\\npacking matrix 𝑂for new feature emebddings {𝑜1, ...,𝑜𝑛}, where 𝑜𝑖\\ncorresponds to the feature embedding of 𝑤𝑖𝑛𝑖. Lines 2-3 implement\\nthe embedding aggregation operation, while Lines 8-11 implement\\nthe group softmax function.\\nA.3\\nThe Algorithms and Optimality Proof for\\nDynamically Determing Batch Size\\nAlgorithm 2 Binary Search for Batch Size\\nRequire: 𝐿, 𝑁\\nEnsure: 1 ≤𝐿≤𝐿𝑚𝑎𝑥, 1 ≤𝑁≤𝐿\\n1: function binary_search(𝐿, 𝑁)\\n2:\\n𝐿←1\\n3:\\n𝑅←𝑀𝑎𝑥𝐵𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒\\n4:\\n𝑑𝑎𝑡𝑎←𝑅𝑎𝑛𝑑𝑜𝑚𝑇𝑖𝑚𝑒𝑆𝑒𝑟𝑖𝑒𝑠𝑖𝑛𝑙𝑒𝑛𝑔𝑡ℎ𝐿\\n5:\\n𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙\\n6:\\nwhile 𝐿≤𝑅do\\n7:\\n𝐼𝑛𝑝𝑢𝑡←𝑑𝑎𝑡𝑎× 𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙\\n8:\\n𝑀𝑜𝑑𝑒𝑙𝐹𝑜𝑟𝑤𝑎𝑟𝑑(𝐼𝑛𝑝𝑢𝑡)\\n9:\\n𝑀𝑜𝑑𝑒𝑙𝐵𝑎𝑐𝑘𝑤𝑎𝑟𝑑\\n10:\\n𝑢←𝑃𝑒𝑎𝑘𝑀𝑒𝑚𝑜𝑟𝑦𝑈𝑠𝑎𝑔𝑒\\n𝑇𝑜𝑡𝑎𝑙𝑀𝑒𝑚𝑜𝑟𝑦\\n11:\\nif 0.9 > 𝑢then\\n12:\\n𝐿←𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙+ 1\\n13:\\n𝐵←𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙\\n14:\\nelse\\n15:\\n𝑅←𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙−1\\n16:\\n𝐵𝑡𝑒𝑚𝑝𝑜𝑟𝑎𝑙←⌊𝐿+𝑅⌋\\n2\\n17:\\nreturn 𝐵\\nAlgorithm 3 Dynamic Programming for Plane Division\\nRequire: 𝐿𝑖, 𝑁𝑖, 𝐵𝑖, 𝐿𝑚𝑎𝑥\\nEnsure: 1 ≤𝐿𝑖≤𝐿𝑚𝑎𝑥, 1 ≤𝑁𝑖≤𝐿𝑖\\n1: function cost(S)\\n2:\\nif |𝑆| < 𝑀then return +∞\\n3:\\n𝐿, 𝑁, 𝐵←𝑝𝑜𝑖𝑛𝑡𝑠𝑖𝑛𝑆\\n4:\\n𝑓←𝑓𝑢𝑛𝑐𝑡𝑖𝑜𝑛𝑓𝑖𝑡𝑡𝑖𝑛𝑔(𝐵|𝐿, 𝑁)\\nreturn 𝐸(𝐵, 𝐿, 𝑁|𝑓)\\n5: function dynamic_programming(𝐿𝑖, 𝑁𝑖, 𝐿𝑚𝑎𝑥)\\n6:\\nfor 𝑙1 = 1 →𝐿𝑚𝑎𝑥do\\n7:\\nfor 𝑙2 = 1 →𝑙1 do\\n8:\\nfor 𝑛= 1 →𝑙1 do\\n9:\\n𝑆←𝑝𝑜𝑖𝑛𝑡𝑠𝑠𝑒𝑡𝑖𝑛{𝑙2 ≤𝐿≤𝑙1, 𝑁≤𝑛}\\n10:\\n𝑔(𝑛) ←𝐶𝑂𝑆𝑇(𝑆)\\n11:\\nfor 𝑖= 1 →𝑛do\\n12:\\n𝑆←𝑝𝑜𝑖𝑛𝑡𝑠𝑠𝑒𝑡𝑖𝑛{𝑙2 ≤𝐿≤𝑙1,𝑖≤𝑁≤𝑛}\\n13:\\n𝑔(𝑛) ←𝑚𝑖𝑛(𝑔(𝑛),𝑔(𝑖) + 𝐶𝑂𝑆𝑇(𝑆))\\n14:\\n𝑓𝑙2,𝑙1 ←𝑔(𝑙1)\\n15:\\n16:\\nfor 𝑙= 1 →𝐿𝑚𝑎𝑥do\\n17:\\n𝑑𝑝(𝑙) ←𝑓(1,𝑙)\\n18:\\nfor 𝑖= 1 →𝑙do\\n19:\\n𝑑𝑝(𝑙) ←𝑚𝑖𝑛(𝑑𝑝(𝑙),𝑑𝑝(𝑖) + 𝑓(𝑖,𝑙))\\nreturn 𝑑𝑝(𝐿𝑚𝑎𝑥)\\nWe describe Alg. 3 and intuitively show its optimality. We assume\\nthat Scipy [53] learns an optimal function in Line 4 so that function\\nCOST gives the optimal estimation error when fitting the points in\\nset 𝑆. When fitting very few points, we assign an infinite cost to\\nprevent a biased fitting function (Line 2). 𝑔(𝑛) denotes the minimal\\nestimation error for points in sub-plane {𝑙2 ≤𝐿≤𝑙1, 𝑁≤𝑛}. In\\nLines 11-13, we enumerate all possible ways of cutting {𝑙2 ≤𝐿≤\\n𝑙1, 𝑁≤𝑛} horizontally into two sub-plane {𝑙2 ≤𝐿≤𝑙1, 𝑁≤𝑖} and\\n{𝑙2 ≤𝐿≤𝑙1,𝑖≤𝑁≤𝑛} by iterating 𝑖from 1 to n. Choosing the\\ncutting strategy that minimizes estimation error gets us a𝑔(𝑙1) with\\nminimal estimation error for sub-plane {𝑙2 ≤𝐿≤𝑙1, 𝑁≤𝑙1}, which\\nis recorded as 𝑓𝑙1,𝑙2 in Line 14. 𝑑𝑝(𝑙) denotes the minimal estimation\\nerror for sub-plane {𝐿≤𝑙}. We enumerate all the possible ways\\nof cutting {𝐿≤𝑙} vertically into two sub-plane {𝐿≤𝑖} and {𝑖≤\\n𝐿≤𝑙} by iterating 𝑖from 1 to 𝑙(Line 17-19). Finally, we have the\\nminimal estimation error for the whole plane as 𝑑𝑝(𝐿𝑚𝑎𝑥). Based\\non the above discussion, this algorithm guarantees to not miss any\\nbetter solution, hence optimal.\\nA.4\\nThe Correctness of Group Attention\\nLemma 3. Assuming the windows belonging to the same group 𝐺𝑖\\nhave the same key vector, i.e. 𝑘𝑗= 𝑟𝑖(𝑤𝑖𝑛𝑗∈𝐺𝑖), then the feature\\nembedding 𝑂produced by the original self-attention mechanism is\\nidentical to the output of our group attention mechanism implemented\\nin Algorithm 1.\\nProof. Denote e\\n𝑘𝑗to be the representative vectors of 𝑘𝑗, i.e. e\\n𝑘𝑗=\\n𝑟𝑖= 𝑘𝑗(𝑤𝑖𝑛𝑗∈𝐺𝑖). Algorithm 1 gives that\\ne𝑣𝑖=\\n𝑛−1\\n∑︁\\n𝑗=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑗== 𝑖)v𝑗, e𝑃𝑖,𝑗= q𝑖· r𝑗\\n𝑠𝑖=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(e𝑃𝑖,𝑗)𝐶𝑂𝑈𝑁𝑇𝑗, e𝑜𝑖=\\n𝑁−1\\n∑︁\\n𝑗=0\\ne𝑃𝑖,𝑗\\n𝑠𝑖\\ne𝑣𝑗\\n(7)\\nBy the canonical self-attention mechanism introduced in Sec. 2,\\nwe get:\\n𝑃𝑖,𝑗= q𝑖· kj, 𝐴𝑖,𝑗=\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n, o𝑖=\\n𝑛−1\\n∑︁\\n𝑗=0\\n𝐴𝑖,𝑗v𝑗\\n(8)\\nWith 7 and 8, we have\\n𝑛−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(𝑃𝑖,𝑗) =\\n𝑛−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(q𝑖· k𝑗)\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)𝑒𝑥𝑝(q𝑖· k𝑥)\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(q𝑖· r𝑗)\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(q𝑖· r𝑗)𝐶𝑂𝑈𝑁𝑇𝑗\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(e𝑃𝑖,𝑗)𝐶𝑂𝑈𝑁𝑇𝑗\\n= 𝑠𝑖\\n(9)\\n12\\nFurther,\\no𝑖=\\n𝑛−1\\n∑︁\\n𝑗=0\\n𝐴𝑖,𝑗vj\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)𝐴𝑖,𝑥v𝑥\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)\\n𝑒𝑥𝑝(𝑃𝑖,𝑥)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\nv𝑥\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)\\n𝑒𝑥𝑝(q𝑖· k𝑥)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\nv𝑥\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)\\n𝑒𝑥𝑝(q𝑖· rj)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\nv𝑥\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(q𝑖· rj)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n𝑛−1\\n∑︁\\n𝑥=0\\n(𝐵𝐸𝐿𝑂𝑁𝐺𝑥== 𝑗)v𝑥\\n=\\n𝑁−1\\n∑︁\\n𝑗=0\\n𝑒𝑥𝑝(q𝑖· rj)\\nÍ𝑛−1\\n𝑘=0 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\ne𝑣𝑗\\n(10)\\nCombining (7), (9) (10), we have oi = ÍN −1\\nj=0\\nePi,j\\nsi evj = eoi.\\nThis concludes that the output of our group attention is identical\\nto vanilla self-attention’s.\\n□\\nA.5\\nThe Proof of Error Bound (Lemma 1)\\nProof. We have\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\n𝑒𝑥𝑝(𝑃𝑖,𝑗) = 𝑒𝑥𝑝(q𝑖· ek𝑗)\\n𝑒𝑥𝑝(q𝑖· k𝑗) = 𝑒𝑥𝑝(q𝑖· (ek𝑗−k𝑗))\\n= 𝑒𝑥𝑝(||q𝑖|| · ||ek𝑗−k𝑗|| · 𝑐𝑜𝑠(q𝑖,ek𝑗−k𝑗))\\n(11)\\nSo\\n𝑒𝑥𝑝(−𝑑𝑅) ≤𝑒𝑥𝑝(𝑃𝑖,𝑗)\\n𝑒𝑥𝑝(𝑃𝑖,𝑗) ≤𝑒𝑥𝑝(𝑑𝑅)\\n(12)\\nThen we have:\\n𝐴𝑖,𝑗\\n𝐴𝑖,𝑗\\n=\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑛\\n𝑘=1 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n/\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑛\\n𝑘=1 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n= 𝑒𝑥𝑝(𝑃𝑖,𝑗)\\n𝑒𝑥𝑝(𝑃𝑖,𝑗)\\nÍ𝑛\\n𝑘=1 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\nÍ𝑛\\n𝑘=1 𝑒𝑥𝑝(𝑃𝑖,𝑘)\\n(13)\\nCombining (12) (13), the error is bounded by\\n𝑒𝑥𝑝(−2𝑑𝑅) ≤𝐴𝑖,𝑗\\n𝐴𝑖,𝑗\\n≤𝑒𝑥𝑝(2𝑑𝑅)\\n(14)\\nThus, if d ≤ln(𝜖)\\n2R , 1\\n𝜖≤Ai,j\\nAi,j ≤𝜖. This proves Lemma 1.\\nA.6\\nThe Proof of Merge Operation (Lemma 2)\\nProof. Denote the cluster size of 𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘to be 𝑛𝑘.After merge-\\ning, the new center will be:\\n𝑐′ =\\nÍ𝑚\\n𝑖=1 𝑛𝑘𝑖𝑐𝑘𝑖\\nÍ𝑚\\n𝑖=1 𝑛𝑘𝑖\\nFor ∀𝑖∈[1,𝑚], ∀𝑥∈𝑐𝑙𝑢𝑠𝑡𝑒𝑟𝑘𝑖, it holds that:\\n|𝑥−𝑐′| ≤|𝑥−𝑐𝑘𝑖| + |𝑐𝑘𝑖−𝑐′| (𝑇𝑟𝑖𝑎𝑛𝑔𝑙𝑒𝑖𝑛𝑒𝑞𝑢𝑎𝑙𝑖𝑡𝑦)\\n= |𝑥−𝑐𝑘𝑖| + |\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n𝑐𝑘𝑖−\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗𝑐𝑘𝑗\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n|\\n= |𝑥−𝑐𝑘𝑖| + |\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗(𝑐𝑘𝑖−𝑐𝑘𝑗)\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n|\\n= |𝑥−𝑐𝑘𝑖| +\\n| Í𝑚\\n𝑗=1 𝑛𝑘𝑗(𝑐𝑘𝑖−𝑐𝑘𝑗) |\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n≤|𝑥−𝑐𝑘𝑖| +\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗|𝑐𝑘𝑖−𝑐𝑘𝑗|\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n=\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗(|𝑐𝑘𝑖−𝑐𝑘𝑗| + |𝑥−𝑐𝑘𝑖|)\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n≤\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗𝑑\\nÍ𝑚\\n𝑗=1 𝑛𝑘𝑗\\n= 𝑑\\n(15)\\nA.7\\nDownstream Tasks\\nRITA supports a variety of downstream tasks. In this section, we\\nshow that with minimal modification RITA can effectively support\\nclassification, imputation and forecasting tasks. Other unsupervised\\ntasks such as similarity search or clustering are naturally supported\\nby extracting feature embeddings from RITA.\\nA.7.1\\nClassification\\nTo classify timeseries, we input timeseries to the model as described\\nin Sec. 3 and attach a special token [CLS] as the first input em-\\nbedding. [CLS]’s embedding acts as the embedding for the entire\\ntimeseries, and the output representation of [CLS] is fed into a\\nclassifier: y = Softmax(WclsZ[CLS] + Bcls), where 𝑍[𝐶𝐿𝑆] ∈R𝑑is\\nthe output representation of [CLS], C is the number of classes, and\\nWcls ∈RC×d, Bcls ∈RC are learnable parameters for classification\\ntask. The result vector 𝑦∈R𝐶represents the possibility that the\\ninput timeseries belongs to each class.\\nWe apply Cross Entropy Loss as the loss function of the classi-\\nfication task [13]: L = 1\\nC\\nÍC\\ni=1 −ˆy(i)log(y(i)), where ˆ𝑦is a binary\\nindicator for ground truth label:\\nˆ𝑦(𝑖) =\\n(\\n1\\n𝑖is ground truth label\\n0\\n𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒\\n(16)\\nA.7.2\\nImputation\\nTimeseries are mainly generated by sensors, a common problem\\nof which is missing values. This becomes a challenge when many\\ndownstream analytics require the missing values to be recovered.\\nThe recovering task is imputation.\\nDenote the real timeseries as𝑇𝑟∈R𝑡×𝑚, the observed timeseries\\nwith missing values as 𝑇𝑜∈R𝑡×𝑚, and the set of missing values’\\npositions as 𝑀. We scale the values of all timeseries to non-negative\\nand use a special value (-1) to indicate missing values:\\n𝑇𝑜(𝑖, 𝑗) =\\n(\\n−1\\n(𝑖, 𝑗) ∈𝑀\\n𝑇𝑟(𝑖, 𝑗)\\n(𝑖, 𝑗) ∉𝑀\\n(17)\\n𝑇𝑜is fed into the RITA as input, and the output representa-\\ntions are concatenated and fed into a Transpose Convolution layer\\nwhich decodes the output embedding vectors from hidden space to\\ntimeseries values, corresponding to the convolution operation in\\n13\\nthe input stage, i.e., Y = TransposeCNN (Z1 +○Z2 +○... +○Zn), where\\n𝑌∈R𝑡×𝑚is the recovered timeseries, and 𝑍𝑖∈R𝑑is the output of\\neach position.\\nHere Mean Square Error is chosen as the loss function [51]:\\n𝐿=\\n1\\n|𝑀|\\nÍ\\n(𝑖,𝑗)∈𝑀(𝑌(𝑖, 𝑗) −𝑇𝑟(𝑖, 𝑗))2.\\nA.7.3\\nForecasting\\nForecasting can be regarded as a special case of imputation, in\\nwhich all missing values are at the end of timeseries.\\nSo like in imputation task, we scale the timeseries to non-\\nnegative and use a special value (-1) to indicate the values to be\\npredicted:\\n𝑇𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑(𝑖, 𝑗) =\\n(\\n𝑇𝑟𝑒𝑎𝑙(𝑖, 𝑗)\\n𝑖≤𝑡𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑\\n−1\\n𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒\\n(18)\\nWhere 𝑡𝑜𝑏𝑠𝑒𝑟𝑣𝑒𝑑is the observed timestamp. Then the output\\nrepresentations are fed into a Transpose Convolution layer using\\nMean Squared Error as loss function, as described above.\\nA.7.4\\nOther Unsupervised Tasks\\nRITA naturally supports other unsupervised tasks, such as similar-\\nity search and clustering [25, 31, 32], by producing the embedding\\nof one timeseries (output representation of the special token [CLS]).\\nClustering can be performed on the embeddings with flexible choice\\nof distance metrics. Similarly, a high dimensional similarity search\\nsystem [22, 23, 38] can be built on the embeddings.\\nA.8\\nInference Time\\nDataset\\nLength\\nTST[61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nWISDM\\n200\\n2.18\\n2.26\\n2.35\\n2.22\\n2.17\\nHHAR\\n200\\n1.19\\n1.23\\n1.28\\n1.21\\n1.18\\nRWHAR\\n200\\n1.32\\n1.37\\n1.42\\n1.34\\n1.31\\nECG\\n2000\\n18.44\\n15.26\\n5.80\\n6.08\\n5.16\\nTable 6: Inference time: Classification on multi-variate data\\n(seconds).\\nDataset\\nLength\\nTST[61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nWISDM\\n200\\n2.03\\n2.11\\n2.19\\n2.07\\n2.02\\nHHAR\\n200\\n1.11\\n1.14\\n1.19\\n1.12\\n1.10\\nRWHAR\\n200\\n1.23\\n1.27\\n1.32\\n1.25\\n1.22\\nECG\\n2000\\n17.22\\n14.32\\n4.73\\n4.99\\n4.11\\nMGH\\n10000\\nN/A\\nN/A\\n6.58\\n6.88\\n1.35\\nTable 7: Inference time: Imputation on multi-variate data\\n(seconds).\\nIn this section, we present the average inference time on valida-\\ntion sets. The results in Table. 6 and 7 correspond to the average\\ninference time on validation sets of classification and imputation\\ntasks, respectively. Consistent with the results in Section. 6.3, our\\nmethod Group Attn. outperforms the baselines on both classifica-\\ntion and imputation tasks, particularly on the datasets comprising\\nlong timeseries (ECG and MGH).\\n14\\n')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.character.RecursiveCharacterTextSplitter at 0x22c423307d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=800)\n",
    "\n",
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_documents = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "\n",
    "vector_store = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "  input_variables=['context', 'question'],\n",
    "\n",
    "  template=\"\"\"Answer the following question based on the given context:\n",
    "  <context>\n",
    "  {context}\n",
    "  </context>\n",
    "\n",
    "  Question: {question},\n",
    "  Answer:\"\"\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "chain = create_stuff_documents_chain(\n",
    "  llm=llm,\n",
    "  prompt=prompt,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prati\\AppData\\Local\\Temp\\ipykernel_4312\\3824356140.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  context = retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the Transformer model, the **attention mechanism**—specifically **self-attention**—is a core component that allows the model to weigh the importance of different parts of the input data dynamically. Here's a breakdown of how it works:\n",
      "\n",
      "1. **Input Embeddings**: The model starts with a sequence of input embeddings represented as a matrix **H** of shape \\( n \\times d_h \\), where \\( n \\) is the number of elements in the sequence (e.g., words in a sentence) and \\( d_h \\) is the dimensionality of each embedding.\n",
      "\n",
      "2. **Linear Projections**:\n",
      "   - **Queries (Q)**: \\( Q = H W_Q \\)\n",
      "   - **Keys (K)**: \\( K = H W_K \\)\n",
      "   - **Values (V)**: \\( V = H W_V \\)\n",
      "   \n",
      "   Here, \\( W_Q \\), \\( W_K \\), and \\( W_V \\) are learned projection matrices that transform the input embeddings into query, key, and value vectors, respectively.\n",
      "\n",
      "3. **Scaled Dot-Product Attention**:\n",
      "   - **Attention Scores**: Compute the raw attention scores by taking the dot product of queries and keys: \\( Q K^T \\).\n",
      "   - **Scaling**: To maintain stable gradients, scale the attention scores by the square root of the key dimension: \\( \\frac{Q K^T}{\\sqrt{d_k}} \\).\n",
      "   - **Softmax Normalization**: Apply the softmax function to each row of the scaled scores to obtain the attention weights: \\( \\text{SoftMax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) \\). This ensures that the weights sum to 1 for each query.\n",
      "   - **Weighted Sum**: Multiply the attention weights by the values to get the output: \\( \\text{Attention}(Q, K, V) = \\text{SoftMax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V \\).\n",
      "\n",
      "4. **Output**: The result is a matrix **O** of shape \\( n \\times d_v \\), where \\( d_v \\) is the dimensionality of the value vectors. This matrix **O** contains contextually rich representations of each element in the input sequence, effectively capturing relationships and dependencies between different parts of the sequence.\n",
      "\n",
      "**Key Characteristics of the Attention Mechanism**:\n",
      "- **Dynamic Weighting**: Unlike fixed representations, self-attention allows each element in the sequence to dynamically focus on different parts of the input, enabling the model to capture complex dependencies.\n",
      "- **Parallelization**: The computations for queries, keys, and values can be performed in parallel, making the Transformer highly efficient for training on large datasets.\n",
      "- **Scalability Issues**: However, the self-attention mechanism has quadratic time and space complexity with respect to the sequence length \\( n \\), which can limit scalability for very long sequences. This is a challenge that models like RITA address with novel attention mechanisms such as **group attention**.\n",
      "\n",
      "In summary, the attention mechanism in the Transformer model enables the network to focus on relevant parts of the input sequence dynamically, facilitating the capture of intricate patterns and relationships essential for tasks like translation, summarization, and time-series analysis.\n"
     ]
    }
   ],
   "source": [
    "question = 'What is the attention mechanism in the transformer model?'\n",
    "\n",
    "context = retriever.get_relevant_documents(question)\n",
    "\n",
    "response = chain.invoke(\n",
    "  {\n",
    "    'context': context,\n",
    "    'question': question\n",
    "  }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
